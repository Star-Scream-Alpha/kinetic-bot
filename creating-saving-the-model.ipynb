{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1131ba0-86e4-4c07-a5d4-403a20478d4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m r2_score, mean_squared_error\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgc\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import optuna\n",
    "import joblib\n",
    "import gc\n",
    "from scipy.optimize import brentq\n",
    "from bisect import bisect_right\n",
    "from scipy.stats import norm\n",
    "from functools import partial\n",
    "import dask\n",
    "from dask import delayed, compute\n",
    "import dask.multiprocessing\n",
    "from dask.diagnostics import ProgressBar \n",
    "\n",
    "from scipy.optimize import root_scalar\n",
    "\n",
    "import boto3\n",
    "import pyarrow.dataset as ds\n",
    "from datetime import datetime\n",
    "\n",
    "import re\n",
    "import glob\n",
    "import pytz\n",
    "from datetime import datetime, timedelta\n",
    "from bisect import bisect_right\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import brentq\n",
    "from io import BytesIO  \n",
    "from pyarrow import parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "import shutil \n",
    "import os\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac3a966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.16.5-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /Users/harshitgajjar/Library/Python/3.9/lib/python/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/harshitgajjar/Library/Python/3.9/lib/python/site-packages (from optuna) (25.0)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Downloading sqlalchemy-2.0.44-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: tqdm in /Users/harshitgajjar/Library/Python/3.9/lib/python/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /Users/harshitgajjar/Library/Python/3.9/lib/python/site-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Using cached mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /Users/harshitgajjar/Library/Python/3.9/lib/python/site-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
      "Collecting tomli (from alembic>=1.5.0->optuna)\n",
      "  Downloading tomli-2.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/harshitgajjar/Library/Python/3.9/lib/python/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
      "Downloading alembic-1.16.5-py3-none-any.whl (247 kB)\n",
      "Downloading sqlalchemy-2.0.44-cp39-cp39-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Using cached mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Downloading tomli-2.3.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: tomli, sqlalchemy, Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.10 alembic-1.16.5 colorlog-6.10.1 optuna-4.6.0 sqlalchemy-2.0.44 tomli-2.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c7b2339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2025-09-01 | Symbol: NIFTY25SEPFUT... Found 33433 rows.\n",
      "Processing 2025-09-02 | Symbol: NIFTY25SEPFUT... Found 35933 rows.\n",
      "Processing 2025-09-03 | Symbol: NIFTY25SEPFUT... Found 25346 rows.\n",
      "Processing 2025-09-04 | Symbol: NIFTY25SEPFUT... Found 36497 rows.\n",
      "Processing 2025-09-05 | Symbol: NIFTY25SEPFUT... Found 35720 rows.\n",
      "Processing 2025-09-06 | Symbol: NIFTY25SEPFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-09-07 | Symbol: NIFTY25SEPFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-09-08 | Symbol: NIFTY25SEPFUT... Found 35001 rows.\n",
      "Processing 2025-09-09 | Symbol: NIFTY25SEPFUT... Found 30184 rows.\n",
      "Processing 2025-09-10 | Symbol: NIFTY25SEPFUT... Found 38016 rows.\n",
      "Processing 2025-09-11 | Symbol: NIFTY25SEPFUT... Found 30489 rows.\n",
      "Processing 2025-09-12 | Symbol: NIFTY25SEPFUT... Found 33590 rows.\n",
      "Processing 2025-09-13 | Symbol: NIFTY25SEPFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-09-14 | Symbol: NIFTY25SEPFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-09-15 | Symbol: NIFTY25SEPFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-09-16 | Symbol: NIFTY25SEPFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-09-17 | Symbol: NIFTY25SEPFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-09-18 | Symbol: NIFTY25SEPFUT... Found 30193 rows.\n",
      "Processing 2025-09-19 | Symbol: NIFTY25SEPFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-09-20 | Symbol: NIFTY25SEPFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-09-21 | Symbol: NIFTY25SEPFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-09-22 | Symbol: NIFTY25SEPFUT... Found 15642 rows.\n",
      "Processing 2025-09-23 | Symbol: NIFTY25SEPFUT... Found 26223 rows.\n",
      "Processing 2025-09-24 | Symbol: NIFTY25SEPFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-09-25 | Symbol: NIFTY25SEPFUT... Found 28750 rows.\n",
      "Processing 2025-09-26 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-09-27 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-09-28 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-09-29 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-09-30 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-10-01 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-10-02 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-10-03 | Symbol: NIFTY25OCTFUT... Found 19390 rows.\n",
      "Processing 2025-10-04 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-10-05 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-10-06 | Symbol: NIFTY25OCTFUT... Found 35960 rows.\n",
      "Processing 2025-10-07 | Symbol: NIFTY25OCTFUT... Found 36850 rows.\n",
      "Processing 2025-10-08 | Symbol: NIFTY25OCTFUT... Found 31370 rows.\n",
      "Processing 2025-10-09 | Symbol: NIFTY25OCTFUT... Found 36628 rows.\n",
      "Processing 2025-10-10 | Symbol: NIFTY25OCTFUT... Found 35731 rows.\n",
      "Processing 2025-10-11 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-10-12 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-10-13 | Symbol: NIFTY25OCTFUT... Found 30941 rows.\n",
      "Processing 2025-10-14 | Symbol: NIFTY25OCTFUT... Found 35792 rows.\n",
      "Processing 2025-10-15 | Symbol: NIFTY25OCTFUT... Found 35270 rows.\n",
      "Processing 2025-10-16 | Symbol: NIFTY25OCTFUT... Found 36618 rows.\n",
      "Processing 2025-10-17 | Symbol: NIFTY25OCTFUT... Found 33774 rows.\n",
      "Processing 2025-10-18 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-10-19 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-10-20 | Symbol: NIFTY25OCTFUT... Found 31212 rows.\n",
      "Processing 2025-10-21 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-10-22 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-10-23 | Symbol: NIFTY25OCTFUT... Found 32374 rows.\n",
      "Processing 2025-10-24 | Symbol: NIFTY25OCTFUT... Found 38976 rows.\n",
      "Processing 2025-10-25 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-10-26 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-10-27 | Symbol: NIFTY25OCTFUT... Found 37186 rows.\n",
      "Processing 2025-10-28 | Symbol: NIFTY25OCTFUT... Found 39246 rows.\n",
      "Processing 2025-10-29 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-10-30 | Symbol: NIFTY25OCTFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-10-31 | Symbol: NIFTY25NOVFUT... Found 26477 rows.\n",
      "Processing 2025-11-01 | Symbol: NIFTY25NOVFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-11-02 | Symbol: NIFTY25NOVFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-11-03 | Symbol: NIFTY25NOVFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-11-04 | Symbol: NIFTY25NOVFUT... Found 34540 rows.\n",
      "Processing 2025-11-05 | Symbol: NIFTY25NOVFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-11-06 | Symbol: NIFTY25NOVFUT... Found 34108 rows.\n",
      "Processing 2025-11-07 | Symbol: NIFTY25NOVFUT... Found 35018 rows.\n",
      "Processing 2025-11-08 | Symbol: NIFTY25NOVFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-11-09 | Symbol: NIFTY25NOVFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-11-10 | Symbol: NIFTY25NOVFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-11-11 | Symbol: NIFTY25NOVFUT... Found 36282 rows.\n",
      "Processing 2025-11-12 | Symbol: NIFTY25NOVFUT... Found 31790 rows.\n",
      "Processing 2025-11-13 | Symbol: NIFTY25NOVFUT... Found 32998 rows.\n",
      "Processing 2025-11-14 | Symbol: NIFTY25NOVFUT... Data not found (Holiday/Weekend).\n",
      "Processing 2025-11-15 | Symbol: NIFTY25NOVFUT... Data not found (Holiday/Weekend).\n",
      "\n",
      "Concatenating master dataframe...\n",
      "Saving 1213548 rows to master_fut_df.csv...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from io import BytesIO\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "BUCKET = \"live-market-data\"\n",
    "SYMBOL = \"NIFTY\"\n",
    "START_DATE = datetime.date(2025, 9, 1)\n",
    "END_DATE = datetime.date(2025, 11, 15)\n",
    "\n",
    "# Expiry Dates (Thursdays)\n",
    "EXPIRY_SEP = datetime.date(2025, 9, 25)\n",
    "EXPIRY_OCT = datetime.date(2025, 10, 30)\n",
    "EXPIRY_NOV = datetime.date(2025, 11, 27)\n",
    "\n",
    "OUTPUT_FILE = \"master_fut_df.csv\"\n",
    "\n",
    "# ==========================================\n",
    "# UTILITIES\n",
    "# ==========================================\n",
    "def get_trading_symbol(current_date):\n",
    "    \"\"\"Determines the active futures contract based on date.\"\"\"\n",
    "    if current_date <= EXPIRY_SEP:\n",
    "        return \"NIFTY25SEPFUT\"\n",
    "    elif current_date <= EXPIRY_OCT:\n",
    "        return \"NIFTY25OCTFUT\"\n",
    "    else:\n",
    "        return \"NIFTY25NOVFUT\"\n",
    "\n",
    "def download_and_process_day(date_obj):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    year = date_obj.year\n",
    "    month = date_obj.month\n",
    "    day = date_obj.day\n",
    "    \n",
    "    fut_ts = get_trading_symbol(date_obj)\n",
    "    key = f\"year={year}/month={month:02d}/day={day:02d}/Futures/{SYMBOL}/{fut_ts}.parquet\"\n",
    "    \n",
    "    print(f\"Processing {date_obj} | Symbol: {fut_ts}...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=BUCKET, Key=key)\n",
    "        df = pd.read_parquet(BytesIO(obj[\"Body\"].read()))\n",
    "        \n",
    "        # 1. Standardize Date/Time\n",
    "        # Ensure Date and Time columns exist for easy filtering later\n",
    "        if 'Date' not in df.columns or 'Time' not in df.columns:\n",
    "            # Fallback if DateTime is a single column\n",
    "            if 'DateTime' in df.columns:\n",
    "                dt = pd.to_datetime(df['DateTime'])\n",
    "                df['Date'] = dt.dt.date\n",
    "                df['Time'] = dt.dt.time\n",
    "        \n",
    "        # 2. Add Ticker\n",
    "        df['Ticker'] = fut_ts\n",
    "        \n",
    "        # 3. Return As-Is (No renaming)\n",
    "        print(f\"Found {len(df)} rows.\")\n",
    "        return df\n",
    "        \n",
    "    except s3.exceptions.NoSuchKey:\n",
    "        print(\"Data not found (Holiday/Weekend).\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==========================================\n",
    "# MAIN LOOP\n",
    "# ==========================================\n",
    "def main():\n",
    "    all_dfs = []\n",
    "    current_date = START_DATE\n",
    "    \n",
    "    while current_date <= END_DATE:\n",
    "        df = download_and_process_day(current_date)\n",
    "        if df is not None and not df.empty:\n",
    "            all_dfs.append(df)\n",
    "        \n",
    "        current_date += datetime.timedelta(days=1)\n",
    "        \n",
    "    if all_dfs:\n",
    "        print(\"\\nConcatenating master dataframe...\")\n",
    "        master_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        \n",
    "        # Final Format Check\n",
    "        print(f\"Saving {len(master_df)} rows to {OUTPUT_FILE}...\")\n",
    "        master_df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(\"Done.\")\n",
    "    else:\n",
    "        print(\"No data downloaded.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94637d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Trading_Symbol</th>\n",
       "      <th>Instrument_Token</th>\n",
       "      <th>LTP</th>\n",
       "      <th>LTQ</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open_Interest</th>\n",
       "      <th>BestBid</th>\n",
       "      <th>BestAsk</th>\n",
       "      <th>BidSize</th>\n",
       "      <th>AskSize</th>\n",
       "      <th>Ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/09/2025</td>\n",
       "      <td>08:46:42.278</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "      <td>13568258</td>\n",
       "      <td>24568.5</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>16610100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/09/2025</td>\n",
       "      <td>09:10:01.994</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "      <td>13568258</td>\n",
       "      <td>24568.5</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>16610100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/09/2025</td>\n",
       "      <td>09:15:01.722</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "      <td>13568258</td>\n",
       "      <td>24590.0</td>\n",
       "      <td>300</td>\n",
       "      <td>1050</td>\n",
       "      <td>16610100</td>\n",
       "      <td>24586.6</td>\n",
       "      <td>24597.3</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/09/2025</td>\n",
       "      <td>09:15:02.471</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "      <td>13568258</td>\n",
       "      <td>24602.8</td>\n",
       "      <td>75</td>\n",
       "      <td>1050</td>\n",
       "      <td>16610100</td>\n",
       "      <td>24586.6</td>\n",
       "      <td>24597.3</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/09/2025</td>\n",
       "      <td>09:15:02.975</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "      <td>13568258</td>\n",
       "      <td>24595.7</td>\n",
       "      <td>75</td>\n",
       "      <td>5700</td>\n",
       "      <td>16610100</td>\n",
       "      <td>24595.7</td>\n",
       "      <td>24605.5</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213543</th>\n",
       "      <td>13/11/2025</td>\n",
       "      <td>15:29:33.077</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "      <td>9485826</td>\n",
       "      <td>25957.3</td>\n",
       "      <td>75</td>\n",
       "      <td>5163450</td>\n",
       "      <td>17647575</td>\n",
       "      <td>25951.3</td>\n",
       "      <td>25957.3</td>\n",
       "      <td>75</td>\n",
       "      <td>150</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213544</th>\n",
       "      <td>13/11/2025</td>\n",
       "      <td>15:29:34.659</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "      <td>9485826</td>\n",
       "      <td>25957.5</td>\n",
       "      <td>75</td>\n",
       "      <td>5163450</td>\n",
       "      <td>17647575</td>\n",
       "      <td>25951.3</td>\n",
       "      <td>25957.3</td>\n",
       "      <td>75</td>\n",
       "      <td>150</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213545</th>\n",
       "      <td>13/11/2025</td>\n",
       "      <td>15:29:35.142</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "      <td>9485826</td>\n",
       "      <td>25950.0</td>\n",
       "      <td>75</td>\n",
       "      <td>5164350</td>\n",
       "      <td>17647575</td>\n",
       "      <td>25950.0</td>\n",
       "      <td>25951.0</td>\n",
       "      <td>1725</td>\n",
       "      <td>825</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213546</th>\n",
       "      <td>13/11/2025</td>\n",
       "      <td>15:29:35.834</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "      <td>9485826</td>\n",
       "      <td>25950.0</td>\n",
       "      <td>75</td>\n",
       "      <td>5164350</td>\n",
       "      <td>17647575</td>\n",
       "      <td>25950.0</td>\n",
       "      <td>25951.0</td>\n",
       "      <td>1725</td>\n",
       "      <td>825</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213547</th>\n",
       "      <td>13/11/2025</td>\n",
       "      <td>15:29:36.267</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "      <td>9485826</td>\n",
       "      <td>25950.0</td>\n",
       "      <td>75</td>\n",
       "      <td>5164350</td>\n",
       "      <td>17647575</td>\n",
       "      <td>25950.0</td>\n",
       "      <td>25951.0</td>\n",
       "      <td>1725</td>\n",
       "      <td>825</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1213548 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date          Time Trading_Symbol  Instrument_Token      LTP  \\\n",
       "0        01/09/2025  08:46:42.278  NIFTY25SEPFUT          13568258  24568.5   \n",
       "1        01/09/2025  09:10:01.994  NIFTY25SEPFUT          13568258  24568.5   \n",
       "2        01/09/2025  09:15:01.722  NIFTY25SEPFUT          13568258  24590.0   \n",
       "3        01/09/2025  09:15:02.471  NIFTY25SEPFUT          13568258  24602.8   \n",
       "4        01/09/2025  09:15:02.975  NIFTY25SEPFUT          13568258  24595.7   \n",
       "...             ...           ...            ...               ...      ...   \n",
       "1213543  13/11/2025  15:29:33.077  NIFTY25NOVFUT           9485826  25957.3   \n",
       "1213544  13/11/2025  15:29:34.659  NIFTY25NOVFUT           9485826  25957.5   \n",
       "1213545  13/11/2025  15:29:35.142  NIFTY25NOVFUT           9485826  25950.0   \n",
       "1213546  13/11/2025  15:29:35.834  NIFTY25NOVFUT           9485826  25950.0   \n",
       "1213547  13/11/2025  15:29:36.267  NIFTY25NOVFUT           9485826  25950.0   \n",
       "\n",
       "         LTQ   Volume  Open_Interest  BestBid  BestAsk  BidSize  AskSize  \\\n",
       "0         75        0       16610100      0.0      0.0        0        0   \n",
       "1         75        0       16610100      0.0      0.0        0        0   \n",
       "2        300     1050       16610100  24586.6  24597.3      300      300   \n",
       "3         75     1050       16610100  24586.6  24597.3      300      300   \n",
       "4         75     5700       16610100  24595.7  24605.5      300      300   \n",
       "...      ...      ...            ...      ...      ...      ...      ...   \n",
       "1213543   75  5163450       17647575  25951.3  25957.3       75      150   \n",
       "1213544   75  5163450       17647575  25951.3  25957.3       75      150   \n",
       "1213545   75  5164350       17647575  25950.0  25951.0     1725      825   \n",
       "1213546   75  5164350       17647575  25950.0  25951.0     1725      825   \n",
       "1213547   75  5164350       17647575  25950.0  25951.0     1725      825   \n",
       "\n",
       "                Ticker  \n",
       "0        NIFTY25SEPFUT  \n",
       "1        NIFTY25SEPFUT  \n",
       "2        NIFTY25SEPFUT  \n",
       "3        NIFTY25SEPFUT  \n",
       "4        NIFTY25SEPFUT  \n",
       "...                ...  \n",
       "1213543  NIFTY25NOVFUT  \n",
       "1213544  NIFTY25NOVFUT  \n",
       "1213545  NIFTY25NOVFUT  \n",
       "1213546  NIFTY25NOVFUT  \n",
       "1213547  NIFTY25NOVFUT  \n",
       "\n",
       "[1213548 rows x 13 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_start = pd.read_csv('master_fut_df.csv')\n",
    "df_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "987c5c1b-4b2e-43fa-ade2-74070f1ebf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all the functions here\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "def clean_time_format(time_str):\n",
    "    # Check if milliseconds are missing\n",
    "    if '.' not in time_str:\n",
    "        return f\"{time_str}.000000\"  # Append milliseconds\n",
    "    else:\n",
    "        # Ensure milliseconds are in the format `000000`\n",
    "        time_parts = time_str.split('.')\n",
    "        milliseconds = time_parts[1][:6].ljust(6, '0')  # Pad or truncate to 6 digits\n",
    "        return f\"{time_parts[0]}.{milliseconds}\"  \n",
    "\n",
    "def additional_columns(df):\n",
    "    df = df.copy()\n",
    "    df['Session'] = np.where((df['Date'] != df['Date'].shift(1)) & (df['Time'] != pd.to_datetime('00:00:00').time()), 0, np.nan)\n",
    "    df['Session'] = np.where((df['Session'].shift(-1) == 0), 1, df['Session'])  \n",
    "    df.at[df.index[-1], 'Session'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_opening_and_closing_trades(df, indicator_base, trade_indicator_base, num_indicators):\n",
    "    df = df.copy()\n",
    "    \n",
    "    for i in range(num_indicators):\n",
    "        indicator = f'{indicator_base}{i+1}'\n",
    "        trade_indicator = f'{trade_indicator_base}{i+1}'\n",
    "        \n",
    "        df[f'Trade{trade_indicator}'] = np.where(\n",
    "            (df['Session'] == 0) & (~np.isnan(df[f'Trend{trade_indicator}'])), \n",
    "            df['Open'] * df[f'Trend{trade_indicator}'], \n",
    "            df[f'Trade{trade_indicator}']\n",
    "        )\n",
    "        \n",
    "        df[f'Trade{trade_indicator}'] = np.where(\n",
    "            df[f'Trade{trade_indicator}'] == 0, \n",
    "            np.nan, \n",
    "            df[f'Trade{trade_indicator}']\n",
    "        )\n",
    "        \n",
    "        df['Cum_non_NaN'] = df[f'Trade{trade_indicator}'].notna().cumsum()\n",
    "        df['Has_non_NaN'] = np.where(\n",
    "            (df['Session'] == 1) & (df['Cum_non_NaN'] > 0), \n",
    "            True, \n",
    "            False\n",
    "        )\n",
    "        \n",
    "        df[f'Trend{trade_indicator}'] = np.where(\n",
    "            (df['Session'] == 1) & (df['Has_non_NaN']) & \n",
    "            (df[f'Trend{trade_indicator}'] == df[f'Trend{trade_indicator}'].shift(1)), \n",
    "            df[f'Trend{trade_indicator}'] * -1, \n",
    "            df[f'Trend{trade_indicator}']\n",
    "        )\n",
    "        \n",
    "        df[f'Trade{trade_indicator}'] = np.where(\n",
    "            (df['Session'] == 1) & (df['Has_non_NaN']), \n",
    "            df['Close'] * df[f'Trend{trade_indicator}'], \n",
    "            df[f'Trade{trade_indicator}']\n",
    "        )\n",
    "        \n",
    "        df = df.drop(columns=['Cum_non_NaN', 'Has_non_NaN'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def resampled_ohlc(df):\n",
    "    df = df.copy()\n",
    "    df['Time'] = df['Time'].astype(str)\n",
    "    df['Time'] = df['Time'].apply(clean_time_format) # Only when time contains milliseconds\n",
    "    df['datetime'] = pd.to_datetime(df['Date'].astype(str) + ' ' + df['Time'].astype(str), errors='coerce')\n",
    "\n",
    "    df.set_index('datetime', inplace=True)\n",
    "\n",
    "    #intervals = ['1s', '15s', '30s', '45s', '1min', '5min', '15min', '30min', '60min']\n",
    "    intervals = ['15s']\n",
    "    resampled_dfs = {}\n",
    "\n",
    "    for interval in intervals:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        interval_start = df.index.floor(interval)\n",
    "        df['group'] = interval_start\n",
    "        df['GroupTime'] = interval_start.time\n",
    "        df['TimePassed'] = ((pd.to_datetime(df['Time'], format='%H:%M:%S.%f') - pd.to_datetime(df['GroupTime'], format='%H:%M:%S')).dt.total_seconds())\n",
    "\n",
    "        df['Open'] = df.groupby('group')['LTP'].transform('first')\n",
    "        df['High'] = df.groupby('group')['LTP'].transform('cummax')\n",
    "        df['Low'] = df.groupby('group')['LTP'].transform('cummin')\n",
    "        df['Close'] = df['LTP'] \n",
    "        df['BestBid'] = df['BuyPrice']\n",
    "        df['BestAsk'] = df['SellPrice']\n",
    "        df['BidSize'] = df['BuyQty']\n",
    "        df['AskSize'] = df['SellQty']\n",
    "        df['Volume'] = df.groupby('group')['LTQ'].transform('cumsum')\n",
    "                \n",
    "        df['GroupNum'] = (df['group'] != df['group'].shift(1)).cumsum()\n",
    "\n",
    "        df['GroupTimeOnly'] = df['group'].dt.time\n",
    "        intervals_list = df['GroupTimeOnly'].unique()\n",
    "        intervals_list_sorted = sorted(intervals_list)\n",
    "        intervals_list_sorted = sorted(df['GroupTimeOnly'].unique())\n",
    "        time_to_groupnum_mapping = {time: idx + 1 for idx, time in enumerate(intervals_list_sorted)}\n",
    "        df['GroupNumTime'] = df['GroupTimeOnly'].map(time_to_groupnum_mapping)\n",
    "        \n",
    "        ohlc = df.copy()\n",
    "\n",
    "        ohlc.reset_index(inplace=True)\n",
    "\n",
    "        ohlc['Date'] = ohlc['datetime'].dt.strftime('%d/%m/%Y')\n",
    "     #   ohlc['Time'] = ohlc['datetime'].dt.strftime('%H:%M:%S')\n",
    "\n",
    "        ohlc.drop(columns=['datetime'], inplace=True)\n",
    "        \n",
    "        bar_change_mask = ohlc['GroupNum'] != ohlc['GroupNum'].shift(1)\n",
    "        ohlc['PrevBar'] = np.nan\n",
    "        ohlc['PrevPrevBar'] = np.nan\n",
    "\n",
    "        ohlc['PrevBar'] = np.where(bar_change_mask, ohlc.index - 1, np.nan)\n",
    "        ohlc['PrevBar'] = np.where(ohlc['PrevBar'] == -1, np.nan, ohlc['PrevBar'])\n",
    "        ohlc['PrevBar'] = ohlc['PrevBar'].ffill()\n",
    "\n",
    "        ohlc['PrevPrevBar'] = np.where(bar_change_mask, ohlc['PrevBar'].shift(1), np.nan)\n",
    "        ohlc['PrevPrevBar'] = ohlc['PrevPrevBar'].ffill()\n",
    "\n",
    "        ohlc.rename(columns={'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'group': 'Group'}, inplace=True)\n",
    "        ohlc.dropna(subset=['Open', 'High', 'Low', 'Close'], inplace=True)\n",
    "        ohlc = ohlc[['Date', 'Time', 'Open', 'High', 'Low', 'Close', 'BestBid', 'BestAsk', 'BidSize', 'AskSize', 'Volume', 'Group', 'GroupNumTime', 'TimePassed', 'GroupNum', 'PrevBar', 'PrevPrevBar']]\n",
    "        \n",
    "        ohlc = additional_columns(ohlc)\n",
    "        ohlc['SessionBar'] = ohlc.groupby('Group')['Session'].transform(lambda x: (x == 0).any())\n",
    "        \n",
    "        ohlc = ohlc.reset_index(drop=True)\n",
    "        \n",
    "        resampled_dfs[interval] = [ohlc]\n",
    "\n",
    "    return resampled_dfs\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from hurst import compute_Hc\n",
    "from scipy.signal import hilbert\n",
    "\n",
    "def rolling_hilbert(series):\n",
    "    if len(series) < 10:  # Ensure enough data points\n",
    "        return np.nan\n",
    "    return np.angle(hilbert(series)[-1])  # Compute Hilbert phase for last value\n",
    "    \n",
    "def forecast_next(x):\n",
    "        if len(x) < 2:\n",
    "            return x[-1]\n",
    "        X = np.arange(len(x))\n",
    "        coef = np.polyfit(X, x, 1)   \n",
    "        return np.polyval(coef, len(x))  \n",
    "\n",
    "def hurst_exp(x):\n",
    "        if len(x) > 10:\n",
    "            H, _, _ = compute_Hc(x, kind='price', simplified=True)\n",
    "            return H\n",
    "        return np.nan\n",
    "\n",
    "def generate_basic_variables(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    open_dict, high_dict, low_dict, close_dict, volume_dict = df['Open'].to_dict(), df['High'].to_dict(), df['Low'].to_dict(), df['Close'].to_dict(), df['Volume'].to_dict()\n",
    "    df['PrevOpen'] = df['PrevBar'].map(lambda x: open_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevHigh'] = df['PrevBar'].map(lambda x: high_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevLow'] = df['PrevBar'].map(lambda x: low_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevClose'] = df['PrevBar'].map(lambda x: close_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevVolume'] = df['PrevBar'].map(lambda x: volume_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['SecondInterval'] = np.floor(df['TimePassed']).where(df['TimePassed'] < 15, np.nan) + 1\n",
    "\n",
    "    df['Spread'] = (df['BestAsk'] - df['BestBid']).round(2)\n",
    "    df['SpreadSize'] = (df['AskSize'] - df['BidSize']).round(2)\n",
    "\n",
    "    df['LTQ'] = np.where(df['GroupNum'] == df['GroupNum'].shift(1), df['Volume'] - df['Volume'].shift(1), df['Volume'])\n",
    "    LTQ_dict = df['LTQ'].to_dict()\n",
    "    df['PrevLTQ'] = df['PrevBar'].map(lambda x: LTQ_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevPrevLTQ'] = df['PrevPrevBar'].map(lambda x: LTQ_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['MeanLTQ'] = df['LTQ'].expanding().mean().round(2)\n",
    "    df['StdLTQ'] = df['LTQ'].expanding().std().round(2)\n",
    "    df['SumPrevLTQ'] = df['PrevLTQ'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SumPrevLTQ'] = df['SumPrevLTQ'].ffill()\n",
    "    df['PrevMeanLTQ'] = (df['SumPrevLTQ']/(df['GroupNum']-1)).round(2)\n",
    "    df['ZScoreLTQ'] = ((df['LTQ'] - df['MeanLTQ'])/df['StdLTQ']).round(2)\n",
    "    df['SkewLTQVariance'] = ((df['LTQ'] - df['MeanLTQ'])**3).cumsum()\n",
    "    df['SkewLTQ'] = (df['SkewLTQVariance']/((df['StdLTQ']**3)*(df.index))).round(2)\n",
    "    df['LTQ_10th_Percentile'] = df['LTQ'].expanding().quantile(0.10)\n",
    "    df['LTQ_25th_Percentile'] = df['LTQ'].expanding().quantile(0.25)\n",
    "    df['LTQ_50th_Percentile'] = df['LTQ'].expanding().quantile(0.50)\n",
    "    df['LTQ_75th_Percentile'] = df['LTQ'].expanding().quantile(0.75)\n",
    "    df['LTQ_90th_Percentile'] = df['LTQ'].expanding().quantile(0.90)\n",
    "    df['LTQIQR'] = (df['LTQ_75th_Percentile'] - df['LTQ_25th_Percentile']).round(2)\n",
    "    df['PrevAutoCorLTQNumerator'] = ((df['LTQ'] - df['MeanLTQ']) * (df['LTQ'].shift(1) - df['MeanLTQ'])).cumsum()\n",
    "    df['PrevAutoCorLTQDenominator'] = np.square(df['LTQ']- df['MeanLTQ']).cumsum()\n",
    "    df['AutoCorrelationLTQ'] = (df['PrevAutoCorLTQNumerator']/df['PrevAutoCorLTQDenominator']).round(2)\n",
    "    df['DeviationFlag'] = np.where(df['LTQ'] > (df['MeanLTQ'] + df['StdLTQ']), 1, 0)\n",
    "    df['AbsoluteDeviationLTQ'] = df['DeviationFlag'].cumsum()\n",
    "    df['SumAbsoluteDeviationLTQ'] = df['LTQ'].where(df['AbsoluteDeviationLTQ'] != df['AbsoluteDeviationLTQ'].shift(1)).cumsum()\n",
    "    df['SumAbsoluteDeviationLTQ'] = df['SumAbsoluteDeviationLTQ'].ffill()\n",
    "    df['MeanAbsoluteDeviationLTQ'] = np.where(df['AbsoluteDeviationLTQ'] != 0, (df['SumAbsoluteDeviationLTQ']/df['AbsoluteDeviationLTQ']).round(2), np.nan)\n",
    "    \n",
    "    volume_dict = df['Volume'].to_dict()\n",
    "    df['PrevPrevVolume'] = df['PrevPrevBar'].map(lambda x: volume_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['SumPrevVolume'] = df['PrevVolume'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SumPrevVolume'] = df['SumPrevVolume'].ffill()\n",
    "    df['PrevMeanVolume'] = (df['SumPrevVolume']/(df['GroupNum']-1)).round(2)\n",
    "    df['MeanVolume'] = ((df['SumPrevVolume'] + df['Volume'])/df['GroupNum']).round(2)\n",
    "    df['ResidualsVolume'] = np.square(df['PrevVolume'] - df['PrevMeanVolume']).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['ResidualsVolume'] = df['ResidualsVolume'].ffill()\n",
    "    df['PrevStdVolume'] = np.sqrt(df['ResidualsVolume']/df['GroupNum']-1)\n",
    "    df['ResidualsVolume'] = df['ResidualsVolume'] + np.square(df['Volume'] - df['MeanVolume'])\n",
    "    df['StdVolume'] = (np.sqrt(df['ResidualsVolume']/df['GroupNum'])).round(2)\n",
    "    df['PrevZScoreVolume'] = ((df['PrevVolume']-df['PrevMeanVolume'])/df['PrevStdVolume']).round(2)\n",
    "    df['ZScoreVolume'] = ((df['Volume']-df['MeanVolume'])/df['StdVolume']).round(2)\n",
    "    df['SkewNumerator'] = ((df['PrevVolume'] - df['PrevMeanVolume'])**3).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SkewNumerator'] = df['SkewNumerator'].ffill()\n",
    "    df['PrevSkewVariance'] = df['SkewNumerator']/(df['GroupNum']-1)\n",
    "    df['PrevSkewVolume'] = df['PrevSkewVariance']/(df['PrevStdVolume']**3)\n",
    "    df['SkewVariance'] = (df['SkewNumerator'] + (df['Volume'] - df['MeanVolume'])**3)/df['GroupNum']\n",
    "    df['SkewVolume'] = (df['SkewVariance']/(df['StdVolume']**3)).round(2)\n",
    "    df['CVVolume'] = (df['StdVolume']/df['MeanVolume']).round(2)\n",
    "    df['KurtosisNumerator'] = ((df['PrevVolume'] - df['PrevMeanVolume'])**4).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['KurtosisNumerator'] = df['KurtosisNumerator'].ffill()\n",
    "    df['PrevKurtosisVariance'] = df['KurtosisNumerator']/(df['GroupNum']-1)\n",
    "    df['PrevKurtosisVolume'] = df['PrevKurtosisVariance']/(df['PrevStdVolume']**4)-3\n",
    "    df['KurtosisVariance'] = (df['KurtosisNumerator'] + (df['Volume'] - df['MeanVolume'])**4)/df['GroupNum']\n",
    "    df['KurtosisVolume'] = (df['KurtosisVariance']/(df['StdVolume']**4)-3).round(2)\n",
    "    df['Volume_10th_Percentile'] = df['Volume'].expanding().quantile(0.10)\n",
    "    df['Volume_25th_Percentile'] = df['Volume'].expanding().quantile(0.25)\n",
    "    df['Volume_50th_Percentile'] = df['Volume'].expanding().quantile(0.50)\n",
    "    df['Volume_75th_Percentile'] = df['Volume'].expanding().quantile(0.75)\n",
    "    df['Volume_90th_Percentile'] = df['Volume'].expanding().quantile(0.90)\n",
    "    df['VolumeIQR'] = (df['Volume_75th_Percentile'] - df['Volume_25th_Percentile']).round(2)\n",
    "    df['PrevAutoCorVolumeNumerator'] = ((df['PrevVolume'] - df['PrevMeanVolume']) * (df['PrevPrevVolume'] - df['PrevMeanVolume'])).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['PrevAutoCorVolumeDenominator'] = np.square(df['PrevVolume'] - df['PrevMeanVolume']).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['AutoCorrelationVolume'] = (df['PrevAutoCorVolumeNumerator']/df['PrevAutoCorVolumeDenominator']).round(2)\n",
    "    df['AutoCorrelationVolume'] = df['AutoCorrelationVolume'].ffill()\n",
    "    mask = df['GroupNum'] != df['GroupNum'].shift(1)\n",
    "    df['TimeSumPrevVolume'] = (df.groupby('GroupNumTime')['PrevVolume'].apply(lambda group: group.where(mask).cumsum()).reset_index(level=0, drop=True))\n",
    "    df['TimeSumPrevVolume'] = df['TimeSumPrevVolume'].ffill()\n",
    "    df['TimeSumGroupNumTime'] = (df.groupby('GroupNumTime').apply(lambda group: group['GroupNum'].ne(group['GroupNum'].shift()).cumsum()).reset_index(level=0, drop=True))\n",
    "    df['TimeSumPrevVolume'] = np.where(df['GroupNumTime'] == 1, 0, df['TimeSumPrevVolume'])\n",
    "    df['TimeMeanVolume'] = (df['TimeSumPrevVolume']/df['TimeSumGroupNumTime']).round(2)\n",
    "    df['VolumeOutlierSum1'] = df['PrevVolume'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreVolume'].shift(1) > 1)).cumsum()\n",
    "    df['VolumeOutlierSum1'] = df['VolumeOutlierSum1'].ffill()\n",
    "    df['VolumeOutlierSum2'] = df['PrevVolume'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreVolume'].shift(1) > 2)).cumsum()\n",
    "    df['VolumeOutlierSum2'] = df['VolumeOutlierSum2'].ffill()\n",
    "    df['VolumeOutlierSum3'] = df['PrevVolume'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreVolume'].shift(1) > 3)).cumsum()\n",
    "    df['VolumeOutlierSum3'] = df['VolumeOutlierSum3'].ffill()\n",
    "    df['VolumeOutlierImpact1'] = (df['VolumeOutlierSum1']/df['SumPrevVolume']).round(2)\n",
    "    df['VolumeOutlierImpact2'] = (df['VolumeOutlierSum2']/df['SumPrevVolume']).round(2)\n",
    "    df['VolumeOutlierImpact3'] = (df['VolumeOutlierSum3']/df['SumPrevVolume']).round(2)\n",
    "    df['ForwardVolume'] = np.where(df['GroupNum'] != df['GroupNum'].shift(1), df['Volume'].shift(1), np.nan)\n",
    "    df['ForwardVolume'] = df['ForwardVolume'].shift(-1)\n",
    "    df['ForwardVolume'] = df['ForwardVolume'].bfill()\n",
    "    df['ProportionVolume'] = (df['Volume']/df['ForwardVolume']).round(2)\n",
    "    df['SpeedVolume'] = ((df['Volume']*15)/df['TimePassed']).round(2)\n",
    "    df['SumIntervalMeanVolume'] = (df.groupby('SecondInterval')['ProportionVolume'].apply(lambda group: group.where(df['SecondInterval'] != df['SecondInterval'].shift(1)).cumsum()).reset_index(level=0, drop=True))\n",
    "    df['SumIntervalMeanVolume'] = df['SumIntervalMeanVolume'].ffill()\n",
    "    df['IntervalMeanVolume'] = (df['SumIntervalMeanVolume']/df['GroupNum']).round(2)\n",
    "\n",
    "    df['Range'] = df['High'] - df['Low']\n",
    "    df['ShiftedRange'] = df['Range'].shift(1).where(df['GroupNum'] != df['GroupNum'].shift(1))\n",
    "    df['MinRange'] = df['ShiftedRange'].cummin()\n",
    "    df['MinRange'] = df['MinRange'].ffill()\n",
    "    range_dict = df['Range'].to_dict()\n",
    "    df['PrevRange'] = df['PrevBar'].map(lambda x: range_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevPrevRange'] = df['PrevPrevBar'].map(lambda x: range_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['SumPrevRange'] = df['PrevRange'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SumPrevRange'] = df['SumPrevRange'].ffill()\n",
    "    df['PrevMeanRange'] = (df['SumPrevRange']/(df['GroupNum']-1)).round(2)\n",
    "    df['MeanRange'] = ((df['SumPrevRange'] + df['Range'])/df['GroupNum']).round(2)\n",
    "    df['ResidualsRange'] = np.square(df['PrevRange'] - df['PrevMeanRange']).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['ResidualsRange'] = df['ResidualsRange'].ffill()\n",
    "    df['PrevStdRange'] = np.sqrt(df['ResidualsRange']/df['GroupNum']-1)\n",
    "    df['ResidualsRange'] = df['ResidualsRange'] + np.square(df['Range'] - df['MeanRange'])\n",
    "    df['StdRange'] = (np.sqrt(df['ResidualsRange']/df['GroupNum'])).round(2)\n",
    "    df['PrevZScoreRange'] = ((df['PrevRange']-df['PrevMeanRange'])/df['PrevStdRange']).round(2)\n",
    "    df['ZScoreRange'] = ((df['Range']-df['MeanRange'])/df['StdRange']).round(2)\n",
    "    df['MaxRange'] = df['Range'].where(df['ZScoreRange'] <= 3).cummax()\n",
    "    df['MaxRange'] = df['MaxRange'].ffill()\n",
    "    df['SkewNumerator'] = ((df['PrevRange'] - df['PrevMeanRange'])**3).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SkewNumerator'] = df['SkewNumerator'].ffill()\n",
    "    df['PrevSkewVariance'] = df['SkewNumerator']/(df['GroupNum']-1)\n",
    "    df['PrevSkewRange'] = df['PrevSkewVariance']/(df['PrevStdRange']**3)\n",
    "    df['SkewVariance'] = (df['SkewNumerator'] + (df['Range'] - df['MeanRange'])**3)/df['GroupNum']\n",
    "    df['SkewRange'] = (df['SkewVariance']/(df['StdRange']**3)).round(2)\n",
    "    df['CVRange'] = (df['StdRange']/df['MeanRange']).round(2)\n",
    "    df['KurtosisNumerator'] = ((df['PrevRange'] - df['PrevMeanRange'])**4).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['KurtosisNumerator'] = df['KurtosisNumerator'].ffill()\n",
    "    df['PrevKurtosisVariance'] = df['KurtosisNumerator']/(df['GroupNum']-1)\n",
    "    df['PrevKurtosisRange'] = df['PrevKurtosisVariance']/(df['PrevStdRange']**4)-3\n",
    "    df['KurtosisVariance'] = (df['KurtosisNumerator'] + (df['Range'] - df['MeanRange'])**4)/df['GroupNum']\n",
    "    df['KurtosisRange'] = (df['KurtosisVariance']/(df['StdRange']**4)-3).round(2)\n",
    "    df['Range_10th_Percentile'] = df['Range'].expanding().quantile(0.10)\n",
    "    df['Range_25th_Percentile'] = df['Range'].expanding().quantile(0.25)\n",
    "    df['Range_50th_Percentile'] = df['Range'].expanding().quantile(0.50)\n",
    "    df['Range_75th_Percentile'] = df['Range'].expanding().quantile(0.75)\n",
    "    df['Range_90th_Percentile'] = df['Range'].expanding().quantile(0.90)\n",
    "    df['RangeIQR'] = (df['Range_75th_Percentile'] - df['Range_25th_Percentile']).round(2)\n",
    "    df['PrevAutoCorRangeNumerator'] = ((df['PrevRange'] - df['PrevMeanRange']) * (df['PrevPrevRange'] - df['PrevMeanRange'])).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['PrevAutoCorRangeDenominator'] = np.square(df['PrevRange'] - df['PrevMeanRange']).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['AutoCorrelationRange'] = (df['PrevAutoCorRangeNumerator']/df['PrevAutoCorRangeDenominator']).round(2)\n",
    "    df['AutoCorrelationRange'] = df['AutoCorrelationRange'].ffill()\n",
    "    df['NormalizedRange'] = np.minimum(((df['Range'] - df['MinRange'])/(df['MaxRange'] - df['MinRange'])).round(2), 1)\n",
    "    mask = df['GroupNum'] != df['GroupNum'].shift(1)\n",
    "    df['TimeSumPrevRange'] = (df.groupby('GroupNumTime')['PrevRange'].apply(lambda group: group.where(mask).cumsum()).reset_index(level=0, drop=True))\n",
    "    df['TimeSumPrevRange'] = df['TimeSumPrevRange'].ffill()\n",
    "    df['TimeSumGroupNumTime'] = (df.groupby('GroupNumTime').apply(lambda group: group['GroupNum'].ne(group['GroupNum'].shift()).cumsum()).reset_index(level=0, drop=True))\n",
    "    df['TimeSumPrevRange'] = np.where(df['GroupNumTime'] == 1, 0, df['TimeSumPrevRange'])\n",
    "    df['TimeMeanRange'] = (df['TimeSumPrevRange']/df['TimeSumGroupNumTime']).round(2)\n",
    "    df['RangeOutlierSum1'] = df['PrevRange'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreRange'].shift(1) > 1)).cumsum()\n",
    "    df['RangeOutlierSum1'] = df['RangeOutlierSum1'].ffill()\n",
    "    df['RangeOutlierSum2'] = df['PrevRange'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreRange'].shift(1) > 2)).cumsum()\n",
    "    df['RangeOutlierSum2'] = df['RangeOutlierSum2'].ffill()\n",
    "    df['RangeOutlierSum3'] = df['PrevRange'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreRange'].shift(1) > 3)).cumsum()\n",
    "    df['RangeOutlierSum3'] = df['RangeOutlierSum3'].ffill()\n",
    "    df['RangeOutlierImpact1'] = (df['RangeOutlierSum1']/df['SumPrevRange']).round(2)\n",
    "    df['RangeOutlierImpact2'] = (df['RangeOutlierSum2']/df['SumPrevRange']).round(2)\n",
    "    df['RangeOutlierImpact3'] = (df['RangeOutlierSum3']/df['SumPrevRange']).round(2)\n",
    "    df['ForwardRange'] = np.where(df['GroupNum'] != df['GroupNum'].shift(1), df['Range'].shift(1), np.nan)\n",
    "    df['ForwardRange'] = df['ForwardRange'].shift(-1)\n",
    "    df['ForwardRange'] = df['ForwardRange'].bfill()\n",
    "    df['ProportionRange'] = (df['Range']/df['ForwardRange']).round(2)\n",
    "    df['SpeedRange'] = ((df['Range']*15)/df['TimePassed']).round(2)\n",
    "    df['SumIntervalMeanRange'] = (df.groupby('SecondInterval')['ProportionRange'].apply(lambda group: group.where(df['SecondInterval'] != df['SecondInterval'].shift(1)).cumsum()).reset_index(level=0, drop=True))\n",
    "    df['SumIntervalMeanRange'] = df['SumIntervalMeanRange'].ffill()\n",
    "    df['IntervalMeanRange'] = (df['SumIntervalMeanRange']/df['GroupNum']).round(2)\n",
    "\n",
    "    df['Lows'] = (df['Low'] - df['PrevLow']).round(2)\n",
    "    df['Lows'] = np.where(df['SessionBar'] == True, 0, df['Lows'])\n",
    "    df['Lows'] = np.where(df['Lows'] < 0, 0, df['Lows'])\n",
    "    df['LastLows'] = np.where(df['GroupNum'] != df['GroupNum'].shift(1), df['Lows'].shift(1), np.nan)\n",
    "    df['LastLows'] = df['LastLows'].shift(1)\n",
    "    df['LastLows'] = df['LastLows'].ffill()\n",
    "    df['LastLows'] = np.where(df['LastLows'] < 0, 0, df['LastLows'])\n",
    "    lows_dict = df['Lows'].to_dict() \n",
    "    df['PrevLows'] = df['PrevBar'].map(lambda x: lows_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['NumberOfLows'] = ((df['PrevLows'] > 0) & (df['GroupNum'] != df['GroupNum'].shift(1))).cumsum()\n",
    "    df['CurrentNoOfLows'] = np.where(df['Lows'] > 0, df['NumberOfLows']+1, df['NumberOfLows'])\n",
    "    df['SumPrevLows'] = df['PrevLows'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SumPrevLows'] = df['SumPrevLows'].ffill()\n",
    "    df['PrevMeanLows'] = (df['SumPrevLows']/df['NumberOfLows']).round(2)\n",
    "    df['MeanLows'] = ((df['SumPrevLows'] + df['Lows'])/df['CurrentNoOfLows']).round(2)\n",
    "    df['ResidualsLows'] = (np.square(df['PrevLows'] - df['PrevMeanLows']).where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['PrevLows'] > 0)).cumsum()).round(2)\n",
    "    df['ResidualsLows'] = df['ResidualsLows'].ffill()\n",
    "    df['PrevStdLows'] = np.sqrt(df['ResidualsLows']/df['NumberOfLows'])\n",
    "    df['ResidualsLows'] = df['ResidualsLows'] + np.square(df['Lows'] - df['MeanLows'])\n",
    "    df['StdLows'] = (np.sqrt(df['ResidualsLows']/df['CurrentNoOfLows'])).round(2)\n",
    "    df['PrevZScoreLows'] = ((df['PrevLows']-df['PrevMeanLows'])/df['PrevStdLows']).round(2)\n",
    "    df['ZScoreLows'] = ((df['Lows']-df['MeanLows'])/df['StdLows']).round(2)\n",
    "    df['SkewLowsNumerator'] = ((df['PrevLows'] - df['PrevMeanLows'])**3).where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['PrevLows'] > 0)).cumsum()\n",
    "    df['SkewLowsNumerator'] = df['SkewLowsNumerator'].ffill()\n",
    "    df['PrevSkewLowsVariance'] = df['SkewLowsNumerator']/(df['NumberOfLows'])\n",
    "    df['PrevSkewLows'] = df['PrevSkewLowsVariance']/(df['PrevStdLows']**3)\n",
    "    df['SkewLowsVariance'] = (df['SkewLowsNumerator'] + (df['Lows'] - df['MeanLows'])**3)/df['CurrentNoOfLows']\n",
    "    df['SkewLows'] = (df['SkewLowsVariance']/(df['StdLows']**3)).round(2)\n",
    "    df['CVLows'] = (df['StdLows']/df['MeanLows']).round(2)\n",
    "    df['KurtosisLowsNumerator'] = ((df['PrevLows'] - df['PrevMeanLows'])**4).where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['PrevLows'] > 0)).cumsum()\n",
    "    df['KurtosisLowsNumerator'] = df['KurtosisLowsNumerator'].ffill()\n",
    "    df['PrevKurtosisLowsVariance'] = df['KurtosisLowsNumerator']/(df['NumberOfLows'])\n",
    "    df['PrevKurtosisLows'] = df['PrevKurtosisLowsVariance']/(df['PrevStdLows']**4)-3\n",
    "    df['KurtosisLowsVariance'] = (df['KurtosisLowsNumerator'] + (df['Lows'] - df['MeanLows'])**4)/df['CurrentNoOfLows']\n",
    "    df['KurtosisLows'] = (df['KurtosisLowsVariance']/(df['StdLows']**4)-3).round(2)\n",
    "    df['Lows_10th_Percentile'] = df['Lows'].expanding().quantile(0.10)\n",
    "    df['Lows_25th_Percentile'] = df['Lows'].expanding().quantile(0.25)\n",
    "    df['Lows_50th_Percentile'] = df['Lows'].expanding().quantile(0.50)\n",
    "    df['Lows_75th_Percentile'] = df['Lows'].expanding().quantile(0.75)\n",
    "    df['Lows_90th_Percentile'] = df['Lows'].expanding().quantile(0.90)\n",
    "    df['LowsIQR'] = (df['Lows_75th_Percentile'] - df['Lows_25th_Percentile']).round(2)\n",
    "\n",
    "    df['Highs'] = (df['PrevHigh'] - df['High']).round(2)\n",
    "    df['Highs'] = np.where(df['SessionBar'] == True, 0, df['Highs'])\n",
    "    df['Highs'] = np.where(df['Highs'] < 0, 0, df['Highs'])\n",
    "    df['LastHighs'] = np.where(df['GroupNum'] != df['GroupNum'].shift(1), df['Highs'].shift(1), np.nan)\n",
    "    df['LastHighs'] = df['LastHighs'].shift(1)\n",
    "    df['LastHighs'] = df['LastHighs'].ffill()\n",
    "    df['LastHighs'] = np.where(df['LastHighs'] < 0, 0, df['LastHighs'])\n",
    "    highs_dict = df['Highs'].to_dict()\n",
    "    df['PrevHighs'] = df['PrevBar'].map(lambda x: highs_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['NumberOfHighs'] = ((df['PrevHighs'] > 0) & (df['GroupNum'] != df['GroupNum'].shift(1))).cumsum()\n",
    "    df['CurrentNoOfHighs'] = np.where(df['Lows'] > 0, df['NumberOfHighs']+1, df['NumberOfHighs'])\n",
    "    df['SumPrevHighs'] = df['PrevHighs'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SumPrevHighs'] = df['SumPrevHighs'].ffill()\n",
    "    df['PrevMeanHighs'] = (df['SumPrevHighs']/df['NumberOfHighs']).round(2)\n",
    "    df['MeanHighs'] = ((df['SumPrevHighs'] + df['Highs'])/df['CurrentNoOfHighs']).round(2)\n",
    "    df['ResidualsHighs'] = (np.square(df['PrevHighs'] - df['PrevMeanHighs']).where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['PrevHighs'] > 0)).cumsum()).round(2)\n",
    "    df['ResidualsHighs'] = df['ResidualsHighs'].ffill()\n",
    "    df['PrevStdHighs'] = np.sqrt(df['ResidualsHighs']/df['NumberOfHighs'])\n",
    "    df['ResidualsHighs'] = df['ResidualsHighs'] + np.square(df['Highs'] - df['MeanHighs'])\n",
    "    df['StdHighs'] = (np.sqrt(df['ResidualsHighs']/df['CurrentNoOfHighs'])).round(2)\n",
    "    df['PrevZScoreHighs'] = ((df['PrevHighs']-df['PrevMeanHighs'])/df['PrevStdHighs']).round(2)\n",
    "    df['ZScoreHighs'] = ((df['Highs']-df['MeanHighs'])/df['StdHighs']).round(2)\n",
    "    df['SkewHighsNumerator'] = ((df['PrevHighs'] - df['PrevMeanHighs'])**3).where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['PrevHighs'] > 0)).cumsum()\n",
    "    df['SkewHighsNumerator'] = df['SkewHighsNumerator'].ffill()\n",
    "    df['PrevSkewHighsVariance'] = df['SkewHighsNumerator']/(df['NumberOfHighs'])\n",
    "    df['PrevSkewHighs'] = df['PrevSkewHighsVariance']/(df['PrevStdHighs']**3)\n",
    "    df['SkewHighsVariance'] = (df['SkewHighsNumerator'] + (df['Highs'] - df['MeanHighs'])**3)/df['CurrentNoOfHighs']\n",
    "    df['SkewHighs'] = (df['SkewHighsVariance']/(df['StdHighs']**3)).round(2)\n",
    "    df['CVHighs'] = (df['StdHighs']/df['MeanHighs']).round(2)\n",
    "    df['KurtosisHighsNumerator'] = ((df['PrevHighs'] - df['PrevMeanHighs'])**4).where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['PrevHighs'] > 0)).cumsum()\n",
    "    df['KurtosisHighsNumerator'] = df['KurtosisHighsNumerator'].ffill()\n",
    "    df['PrevKurtosisHighsVariance'] = df['KurtosisHighsNumerator']/(df['NumberOfHighs'])\n",
    "    df['PrevKurtosisHighs'] = df['PrevKurtosisHighsVariance']/(df['PrevStdHighs']**4)-3\n",
    "    df['KurtosisHighsVariance'] = (df['KurtosisHighsNumerator'] + (df['Highs'] - df['MeanHighs'])**4)/df['CurrentNoOfHighs']\n",
    "    df['KurtosisHighs'] = (df['KurtosisHighsVariance']/(df['StdHighs']**4)-3).round(2)\n",
    "    df['Highs_10th_Percentile'] = df['Highs'].expanding().quantile(0.10)\n",
    "    df['Highs_25th_Percentile'] = df['Highs'].expanding().quantile(0.25)\n",
    "    df['Highs_50th_Percentile'] = df['Highs'].expanding().quantile(0.50)\n",
    "    df['Highs_75th_Percentile'] = df['Highs'].expanding().quantile(0.75)\n",
    "    df['Highs_90th_Percentile'] = df['Highs'].expanding().quantile(0.90)\n",
    "    df['HighsIQR'] = (df['Highs_75th_Percentile'] - df['Highs_25th_Percentile']).round(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('Phase 4 Started')\n",
    "    df['le_next_low_pred'] = 2 * df['Low'] - df['Low'].shift(1)\n",
    "    df['le_next_high_pred'] = 2 * df['High'] - df['High'].shift(1) \n",
    "    df['atr'] = df['Range'].rolling(window=14, min_periods=1).mean()  \n",
    "    df['atr_next_low_pred'] = df['Low'] - 0.5 * df['atr']\n",
    "    df['atr_next_high_pred'] = df['High'] + 0.5 * df['atr'] \n",
    "    df['reg_next_low_pred'] = df['Low'].rolling(7, min_periods=2).apply(forecast_next, raw=False)\n",
    "    df['reg_next_high_pred'] = df['High'].rolling(7, min_periods=2).apply(forecast_next, raw=False)\n",
    "    df['ema_low'] = df['Low'].ewm(span=10, adjust=False).mean()\n",
    "    df['ema_high'] = df['High'].ewm(span=10, adjust=False).mean()\n",
    "    df['low_std'] = df['Low'].rolling(window=10, min_periods=1).std()\n",
    "    df['high_std'] = df['High'].rolling(window=10, min_periods=1).std()\n",
    "    df['ema_next_low_pred'] = df['ema_low'] - df['low_std']\n",
    "    df['ema_next_high_pred'] = df['ema_high'] + df['high_std']\n",
    "    #ensemble of em all\n",
    "    df['predicted_next_low'] = (df['le_next_low_pred'] + df['atr_next_low_pred'] + df['reg_next_low_pred']) / 3\n",
    "    df['predicted_next_high'] = (df['le_next_high_pred'] + df['atr_next_high_pred'] + df['reg_next_high_pred']) / 3\n",
    "    df['feature_imbalance'] = (df['BidSize'] - df['AskSize']) / (df['BidSize'] + df['AskSize'] + 1e-9)\n",
    "    df['realtive_spread'] = df['Spread'] / ((df['BestBid'] + df['BestAsk']) / 2)\n",
    "    print(\"ensemble done\")\n",
    "    #rapdi changed in bid ask prices - quote stuffing\n",
    "    df['quote_stuffing'] = df['BestBid'].diff().abs() + df['BestAsk'].diff().abs()\n",
    "    #liq based    \n",
    "    df['depth_slope'] = (df['BidSize'] - df['AskSize']) / (df['BidSize'] + df['AskSize'] + 1e-9)\n",
    "    # Hidden Liquidity Detection (large orders appearing and disappearing)\n",
    "    print('depth slope done')\n",
    "    df['hidden_liquidity'] = ((df['BidSize'] > df['BidSize'].shift(1) * 2) & (df['BidSize'].shift(-1) < df['BidSize'] / 2)).astype(int)\n",
    "    ## 4. Price Action & Trend Features\n",
    "    df['price_momentum'] = df['Close'].diff()\n",
    "    df['price_momentum_ratio'] = df['Close'] / df['Close'].shift(1)\n",
    "    print('prr done')\n",
    "    #df['hurst_exponent'] = df['Close'].rolling(120).apply(lambda x: compute_Hc(x[:-1], kind='price', simplified=True)[0] if len(x) >= 100 else np.nan,raw=True).shift(1)\n",
    "\n",
    "    # Hilbert Transform (Measures Market Cycles)\n",
    "    df['hilbert_phase'] = df['Close'].rolling(50).apply(rolling_hilbert, raw=True).shift(1)\n",
    "    print('hilbert phase done')\n",
    "    # Fraction of Large Orders (Relative to Volume)\n",
    "    df['large_order_fraction'] = (df['BidSize'] + df['AskSize']) / (df['Volume'] + 1e-9)\n",
    "    print('lof done')\n",
    "    #micorstucture volatlity\n",
    "    df['mvr'] = df['Close'].pct_change().rolling(5).std().shift(1) / df['Close'].pct_change().rolling(30).std().shift(1)\n",
    "\n",
    "                                                         \n",
    "\n",
    "\n",
    "    df = df.drop(columns = ['PrevRange', 'GroupNumTime', 'PrevMeanRange', 'PrevStdRange', 'PrevZScoreRange', 'PrevSkewRange', 'PrevKurtosisVariance', \n",
    "                            'PrevKurtosisRange', 'PrevMeanLows', 'PrevLows', 'PrevMeanHighs', 'PrevLows', 'PrevHighs', 'PrevStdLows', \n",
    "                            'PrevStdHighs', 'PrevZScoreLows', 'PrevZScoreHighs', 'PrevSkewLows', 'PrevKurtosisLows', 'PrevZScoreHighs',\n",
    "                            'PrevSkewHighs', 'PrevKurtosisHighs', 'MinRange', 'MaxRange'])\n",
    "    df = df.drop(columns = ['Group', 'Session', 'SessionBar', 'PrevBar', 'PrevPrevBar', 'PrevOpen', 'PrevHigh', 'PrevLow', 'PrevClose', \n",
    "                            'ShiftedRange', 'PrevPrevRange', 'SumPrevRange', 'ResidualsRange', 'SkewNumerator', 'PrevSkewVariance', \n",
    "                            'SkewVariance', 'KurtosisNumerator', 'KurtosisVariance', 'PrevAutoCorRangeNumerator', 'Range_10th_Percentile', \n",
    "                            'Range_25th_Percentile', 'Range_50th_Percentile', 'Range_75th_Percentile', 'Range_90th_Percentile', 'RangeOutlierSum2',\n",
    "                            'PrevAutoCorRangeDenominator', 'SumPrevLows', 'SumPrevHighs', 'NumberOfLows', 'NumberOfHighs', 'CurrentNoOfLows', \n",
    "                            'CurrentNoOfHighs', 'ResidualsLows', 'Lows_10th_Percentile', 'Lows_25th_Percentile', 'Lows_50th_Percentile',\n",
    "                            'Lows_75th_Percentile', 'Lows_90th_Percentile', 'KurtosisLowsVariance', 'PrevKurtosisLowsVariance', 'RangeOutlierSum3',\n",
    "                            'KurtosisLowsNumerator', 'SkewLowsVariance', 'PrevSkewLowsVariance', 'SkewLowsNumerator', 'ResidualsHighs', \n",
    "                            'SkewHighsNumerator', 'PrevSkewHighsVariance', 'SkewHighsVariance', 'KurtosisHighsNumerator', 'RangeOutlierSum1',\n",
    "                            'PrevKurtosisHighsVariance', 'KurtosisHighsVariance', 'Highs_10th_Percentile', 'Highs_25th_Percentile', \n",
    "                            'Highs_50th_Percentile', 'Highs_75th_Percentile', 'Highs_90th_Percentile', 'TimeSumPrevRange','TimeSumGroupNumTime',\n",
    "                            'RangeOutlierImpact2', 'RangeOutlierImpact3', 'SumIntervalMeanRange', 'SkewLTQVariance', 'SumPrevLTQ',\n",
    "                            'PrevLTQ', 'PrevMeanLTQ', 'PrevAutoCorLTQDenominator', 'PrevAutoCorLTQNumerator', 'PrevPrevLTQ', 'LTQ_10th_Percentile',\n",
    "                            'LTQ_25th_Percentile', 'LTQ_50th_Percentile', 'LTQ_75th_Percentile', 'LTQ_90th_Percentile', 'DeviationFlag', \n",
    "                            'SumAbsoluteDeviationLTQ', 'AbsoluteDeviationLTQ', 'SumPrevVolume', 'ResidualsVolume', 'PrevStdVolume', 'PrevMeanVolume',\n",
    "                            'PrevZScoreVolume', 'PrevSkewVolume', 'PrevKurtosisVolume', 'PrevAutoCorVolumeNumerator', 'PrevAutoCorVolumeDenominator', \n",
    "                            'TimeSumPrevVolume', 'TimeSumGroupNumTime', 'VolumeOutlierSum1', 'VolumeOutlierSum2', 'VolumeOutlierSum3', 'ForwardVolume', \n",
    "                            'SumIntervalMeanVolume', 'PrevPrevVolume'])\n",
    "    df = df.drop(columns=[\n",
    "                            'ForwardRange', 'ProportionVolume', 'ProportionRange', 'SpeedVolume', 'SpeedRange', \n",
    "                            'MeanLTQ', 'StdLTQ', 'MeanVolume', 'StdVolume', 'MeanRange', 'StdRange', \n",
    "                            'Volume_10th_Percentile', 'Volume_25th_Percentile', 'Volume_50th_Percentile', \n",
    "                            'Volume_75th_Percentile', 'Volume_90th_Percentile', 'TimeMeanVolume', \n",
    "                            'IntervalMeanVolume', 'TimeMeanRange', 'IntervalMeanRange', 'RangeOutlierImpact1'\n",
    "                    ])\n",
    "\n",
    "    return df\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def std_numba(arr):\n",
    "    n = arr.shape[0]\n",
    "    s = 0.0\n",
    "    mean = 0.0\n",
    "    for i in range(n):\n",
    "        mean += arr[i]\n",
    "    mean /= n\n",
    "    for i in range(n):\n",
    "        diff = arr[i] - mean\n",
    "        s += diff * diff\n",
    "    return np.sqrt(s / n)\n",
    "\n",
    "@njit\n",
    "def hurst_exponent_numba(ts, max_lag=100):\n",
    "    n = ts.shape[0]\n",
    "    if n < max_lag:\n",
    "        return np.nan\n",
    "    m = max_lag - 2\n",
    "    lags = np.empty(m)\n",
    "    tau = np.empty(m)\n",
    "    for i in range(m):\n",
    "        lag = i + 2\n",
    "        lags[i] = lag\n",
    "        diff_sum = 0.0\n",
    "        count = n - lag\n",
    "        s = 0.0\n",
    "        for j in range(count):\n",
    "            s += ts[j + lag] - ts[j]\n",
    "        mean_diff = s / count\n",
    "        var = 0.0\n",
    "        for j in range(count):\n",
    "            d = (ts[j + lag] - ts[j]) - mean_diff\n",
    "            var += d * d\n",
    "        tau[i] = np.sqrt(var / count)\n",
    "    sum_log_lags = 0.0\n",
    "    sum_log_tau = 0.0\n",
    "    for i in range(m):\n",
    "        sum_log_lags += np.log(lags[i])\n",
    "        sum_log_tau += np.log(tau[i])\n",
    "    mean_log_lags = sum_log_lags / m\n",
    "    mean_log_tau = sum_log_tau / m\n",
    "    cov = 0.0\n",
    "    var_ll = 0.0\n",
    "    for i in range(m):\n",
    "        diff_ll = np.log(lags[i]) - mean_log_lags\n",
    "        diff_tau = np.log(tau[i]) - mean_log_tau\n",
    "        cov += diff_ll * diff_tau\n",
    "        var_ll += diff_ll * diff_ll\n",
    "    slope = cov / var_ll\n",
    "    return slope\n",
    "    \n",
    "@njit\n",
    "def fractal_dimension_numba(ts, max_scale=20):\n",
    "    n = ts.shape[0]\n",
    "    if n < max_scale:\n",
    "        return np.nan\n",
    "    m = max_scale - 2\n",
    "    scales = np.empty(m)\n",
    "    variances = np.empty(m)\n",
    "    for i in range(m):\n",
    "        scale = i + 2\n",
    "        scales[i] = scale\n",
    "        npart = n // scale\n",
    "        var_sum = 0.0\n",
    "        for j in range(npart):\n",
    "            seg_mean = 0.0\n",
    "            for k in range(scale):\n",
    "                seg_mean += ts[j * scale + k]\n",
    "            seg_mean /= scale\n",
    "            seg_var = 0.0\n",
    "            for k in range(scale):\n",
    "                diff = ts[j * scale + k] - seg_mean\n",
    "                seg_var += diff * diff\n",
    "            seg_std = np.sqrt(seg_var / scale)\n",
    "            var_sum += seg_std\n",
    "        variances[i] = var_sum / npart\n",
    "    sum_log_scales = 0.0\n",
    "    sum_log_vars = 0.0\n",
    "    for i in range(m):\n",
    "        sum_log_scales += np.log(scales[i])\n",
    "        sum_log_vars += np.log(variances[i])\n",
    "    mean_log_scales = sum_log_scales / m\n",
    "    mean_log_vars = sum_log_vars / m\n",
    "    cov = 0.0\n",
    "    var_ll = 0.0\n",
    "    for i in range(m):\n",
    "        diff_sc = np.log(scales[i]) - mean_log_scales\n",
    "        diff_v = np.log(variances[i]) - mean_log_vars\n",
    "        cov += diff_sc * diff_v\n",
    "        var_ll += diff_sc * diff_sc\n",
    "    slope = cov / var_ll\n",
    "    return 2 - slope\n",
    "\n",
    "@njit\n",
    "def lyapunov_exponent_numba(time_series, epsilon=1e-4, steps=100):\n",
    "    N = time_series.shape[0]\n",
    "    if N <= steps:\n",
    "        return np.nan\n",
    "    s = 0.0\n",
    "    count = N - steps\n",
    "    base = 0.0\n",
    "    diff0 = np.abs(time_series[steps] - time_series[0])\n",
    "    if diff0 < epsilon:\n",
    "        diff0 = epsilon\n",
    "    for i in range(count):\n",
    "        diff = np.abs(time_series[i + steps] - time_series[i])\n",
    "        if diff < epsilon:\n",
    "            diff = epsilon\n",
    "        s += np.log(diff / diff0)\n",
    "    return s / (count * steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "007b2202-b824-4ad8-ad63-afbf7fa250a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting backtester data creation for , 2025-09-01 00:00:00\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'BuyPrice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'BuyPrice'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 32\u001b[0m\n\u001b[1;32m     23\u001b[0m end_date \u001b[38;5;241m=\u001b[39m unique_dates[i \u001b[38;5;241m+\u001b[39m window_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     25\u001b[0m df_current \u001b[38;5;241m=\u001b[39m df_start[\n\u001b[1;32m     26\u001b[0m     (df_start[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m start_date) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m     27\u001b[0m     (df_start[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end_date) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m     28\u001b[0m     (df_start[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m start_time) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m     29\u001b[0m     (df_start[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m end_time)\n\u001b[1;32m     30\u001b[0m ]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 32\u001b[0m Nifty_FUT_resampled \u001b[38;5;241m=\u001b[39m \u001b[43mresampled_ohlc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_current\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m df \u001b[38;5;241m=\u001b[39m Nifty_FUT_resampled[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m15s\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m Nifty_FUT_resampled\n",
      "Cell \u001b[0;32mIn[28], line 90\u001b[0m, in \u001b[0;36mresampled_ohlc\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     88\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLTP\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcummin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     89\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLTP\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[0;32m---> 90\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBestBid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBuyPrice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     91\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBestAsk\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSellPrice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     92\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBidSize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuyQty\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'BuyPrice'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "\n",
    "df_start['Date'] = pd.to_datetime(df_start['Date'], dayfirst=True, errors='coerce')\n",
    "df_start['Date'] = df_start['Date'].dt.floor('D')\n",
    "\n",
    "df_start['Time'] = pd.to_datetime(\n",
    "    df_start['Time'],\n",
    "    format=\"%H:%M:%S.%f\",\n",
    "    errors='coerce'\n",
    ").fillna(pd.to_datetime(df_start['Time'], format=\"%H:%M:%S\", errors='coerce')).dt.time\n",
    "\n",
    "unique_dates = sorted(df_start['Date'].unique())\n",
    "window_size = 15\n",
    "\n",
    "start_time = pd.to_datetime(\"09:15:00\").time()\n",
    "end_time = pd.to_datetime(\"15:30:00\").time()\n",
    "\n",
    "for i in range(len(unique_dates) - window_size + 1):    \n",
    "    start_date = unique_dates[i]\n",
    "    print('starting backtester data creation for ,', start_date)\n",
    "    end_date = unique_dates[i + window_size - 1]\n",
    "\n",
    "    df_current = df_start[\n",
    "        (df_start['Date'] >= start_date) &\n",
    "        (df_start['Date'] <= end_date) &\n",
    "        (df_start['Time'] >= start_time) &\n",
    "        (df_start['Time'] < end_time)\n",
    "    ].copy()\n",
    "    \n",
    "    Nifty_FUT_resampled = resampled_ohlc(df_current)\n",
    "    df = Nifty_FUT_resampled['15s'][0]\n",
    "    del Nifty_FUT_resampled\n",
    "    del df_current\n",
    "    gc.collect()\n",
    "\n",
    "    #aggregation\n",
    "\n",
    "    df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format=\"%d/%m/%Y %H:%M:%S.%f\", errors='coerce')\n",
    "    df.set_index('DateTime', inplace = True)\n",
    "\n",
    "    \n",
    "\n",
    "    #resampling\n",
    "\n",
    "    resampled_df = df.resample('15s').agg({      \n",
    "                    'Date': 'last',         \n",
    "                    'Time': 'last',        \n",
    "                    'Open': 'first',\n",
    "                    'High':'max',\n",
    "                    'Low':'min',\n",
    "                    'Close':'last',\n",
    "                    'BestBid':'last',\n",
    "                    'BestAsk':'last',\n",
    "                    'BidSize':'last',\n",
    "                    'AskSize':'last',\n",
    "                    'Volume':'sum',\n",
    "                    'Group':'last',\n",
    "                    'GroupNumTime':'last',\n",
    "                    'TimePassed':'last', \n",
    "                    'GroupNum':'last', \n",
    "                    'PrevBar':'last',\n",
    "                    'PrevPrevBar':'last', \n",
    "                    'Session':'last', \n",
    "                    'SessionBar':'last'\t\n",
    "        }).reset_index()\n",
    "    resampled_df.drop(columns = 'DateTime', inplace = True , axis =1, errors = 'ignore')\n",
    "\n",
    "    print('Agg Done')\n",
    "    \n",
    "    df = resampled_df.copy()\n",
    "    gc.collect()\n",
    "\n",
    "    #adding variables\n",
    "    \n",
    "    df_after_adding_variables = generate_basic_variables(df)\n",
    "\n",
    "    print('Variables Added')\n",
    "\n",
    "    #adding more\n",
    "\n",
    "    df_after_adding_variables['Date_Time'] = pd.to_datetime(df_after_adding_variables['Date'] + ' ' + df_after_adding_variables['Time'], format=\"%d/%m/%Y %H:%M:%S.%f\", errors='coerce')\n",
    "    df_after_adding_variables.set_index('Date_Time', inplace=True)\n",
    "    df_after_adding_variables['Close'] = pd.to_numeric(df_after_adding_variables['Close'], errors='coerce')\n",
    "    df_after_adding_variables.dropna(subset=['Close'], inplace=True)\n",
    "    \n",
    "    window_sizes = [200, 500, 1000, 1500]\n",
    "    \n",
    "    for window in window_sizes:\n",
    "        df_after_adding_variables[f'Lyapunov_{window}'] = df_after_adding_variables['Close'].rolling(window=window).apply(\n",
    "            lambda x: lyapunov_exponent_numba(x), raw=True)\n",
    "        df_after_adding_variables[f'Hurst_{window}'] = df_after_adding_variables['Close'].rolling(window=window).apply(\n",
    "            lambda x: hurst_exponent_numba(x), raw=True)\n",
    "        df_after_adding_variables[f'FDI_{window}'] = df_after_adding_variables['Close'].rolling(window=window).apply(\n",
    "            lambda x: fractal_dimension_numba(x), raw=True)\n",
    "    \n",
    "    df_after_adding_variables.reset_index(drop=True, inplace=True)\n",
    "    df_after_adding_variables[['Lyapunov_200','Hurst_200','FDI_200','Lyapunov_500','Hurst_500','FDI_500','Lyapunov_1000','Hurst_1000','FDI_1000','Lyapunov_1500','Hurst_1500','FDI_1500']].tail()\n",
    "\n",
    "    print(f'New Training data for {start_date} saved to temp var')\n",
    "    print('Starting Training')\n",
    "\n",
    "    df = df_after_adding_variables.copy()\n",
    "    del df_after_adding_variables\n",
    "    gc.collect()\n",
    "\n",
    "    #untuned xgboost model with old weight, for day to day rolling 15 test\n",
    "    '''\n",
    "    df['Pct_Change'] = df['Close'].pct_change() * 100\n",
    "\n",
    "    df['Target'] = df['Pct_Change'].shift(-1)\n",
    "    \n",
    "    \n",
    "    df = df.dropna(subset=['Pct_Change', 'Target'])\n",
    "    \n",
    "    feature_columns = ['Open', 'High', 'Low', 'Close', 'BestBid', 'BestAsk',\n",
    "                       'BidSize', 'AskSize', 'Volume', 'TimePassed', 'GroupNum', 'PrevVolume',\n",
    "                       'SecondInterval', 'Spread', 'SpreadSize', 'LTQ', 'ZScoreLTQ', 'SkewLTQ',\n",
    "                       'AutoCorrelationLTQ', 'MeanAbsoluteDeviationLTQ', 'ZScoreVolume',\n",
    "                       'SkewVolume', 'CVVolume', 'KurtosisVolume', 'AutoCorrelationVolume',\n",
    "                       'VolumeOutlierImpact1', 'VolumeOutlierImpact2', 'VolumeOutlierImpact3', 'Range',\n",
    "                       'ZScoreRange', 'SkewRange', 'CVRange', 'KurtosisRange',\n",
    "                       'AutoCorrelationRange', 'NormalizedRange', 'Lows', 'LastLows', 'MeanLows',\n",
    "                       'StdLows', 'ZScoreLows', 'SkewLows', 'CVLows', 'KurtosisLows',\n",
    "                       'Highs', 'LastHighs', 'MeanHighs', 'StdHighs', 'ZScoreHighs', 'SkewHighs',\n",
    "                       'CVHighs', 'KurtosisHighs', 'le_next_low_pred', 'le_next_high_pred',\n",
    "                       'atr', 'atr_next_low_pred', 'atr_next_high_pred', 'reg_next_low_pred',\n",
    "                       'reg_next_high_pred', 'ema_low', 'ema_high', 'low_std', 'high_std',\n",
    "                       'ema_next_low_pred', 'ema_next_high_pred', 'predicted_next_low',\n",
    "                       'predicted_next_high', 'feature_imbalance', 'realtive_spread', 'quote_stuffing',\n",
    "                       'depth_slope', 'price_momentum', 'price_momentum_ratio',\n",
    "                       'hilbert_phase', 'large_order_fraction', 'mvr']\n",
    "    \n",
    "    X = df[feature_columns]\n",
    "    y = df['Target']\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(X_train.median())\n",
    "    X_test  = X_test.replace([np.inf, -np.inf], np.nan).fillna(X_test.median())\n",
    "    \n",
    "    # Standardize the features.\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    import joblib\n",
    "    \n",
    "    # Save the fitted scaler\n",
    "    joblib.dump(scaler, f\"{start_date}-{end_date}-scaler_untuned.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators= 142,\n",
    "        max_depth= 6,\n",
    "        learning_rate= 0.017409934489017977,\n",
    "        subsample= 0.8469721090951134,\n",
    "        colsample_bytree= 0.7404203262606515,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    joblib.dump(model, f\"{start_date}-{end_date}-model-untuned.pkl\") \n",
    "    \n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    print(f'For {start_date}')\n",
    "    print(\"Test R^2 Score:\", r2)\n",
    "    print(\"Test RMSE:\", rmse)\n",
    "\n",
    "    '''\n",
    "    # -Saving weight with tuner\n",
    "    df['Pct_Change'] = df['Close'].pct_change() * 100\n",
    "    df['Target'] = df['Pct_Change'].shift(-1)\n",
    "    df = df.dropna(subset=['Pct_Change', 'Target'])\n",
    "    \n",
    "    feature_columns = ['Open', 'High', 'Low', 'Close', 'BestBid', 'BestAsk',\n",
    "                       'BidSize', 'AskSize', 'Volume', 'TimePassed', 'GroupNum', 'PrevVolume',\n",
    "                       'SecondInterval', 'Spread', 'SpreadSize', 'LTQ', 'ZScoreLTQ', 'SkewLTQ',\n",
    "                       'AutoCorrelationLTQ', 'MeanAbsoluteDeviationLTQ', 'ZScoreVolume',\n",
    "                       'SkewVolume', 'CVVolume', 'KurtosisVolume', 'AutoCorrelationVolume',\n",
    "                       'VolumeOutlierImpact1', 'VolumeOutlierImpact2', 'VolumeOutlierImpact3', 'Range',\n",
    "                       'ZScoreRange', 'SkewRange', 'CVRange', 'KurtosisRange',\n",
    "                       'AutoCorrelationRange', 'NormalizedRange', 'Lows', 'LastLows', 'MeanLows',\n",
    "                       'StdLows', 'ZScoreLows', 'SkewLows', 'CVLows', 'KurtosisLows',\n",
    "                       'Highs', 'LastHighs', 'MeanHighs', 'StdHighs', 'ZScoreHighs', 'SkewHighs',\n",
    "                       'CVHighs', 'KurtosisHighs', 'le_next_low_pred', 'le_next_high_pred',\n",
    "                       'atr', 'atr_next_low_pred', 'atr_next_high_pred', 'reg_next_low_pred',\n",
    "                       'reg_next_high_pred', 'ema_low', 'ema_high', 'low_std', 'high_std',\n",
    "                       'ema_next_low_pred', 'ema_next_high_pred', 'predicted_next_low',\n",
    "                       'predicted_next_high', 'feature_imbalance', 'realtive_spread', 'quote_stuffing',\n",
    "                       'depth_slope', 'price_momentum', 'price_momentum_ratio',\n",
    "                       'hilbert_phase', 'large_order_fraction', 'mvr']\n",
    "    \n",
    "    X = df[feature_columns]\n",
    "    y = df['Target']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Clean and scale data\n",
    "    X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(X_train.median())\n",
    "    X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(X_test.median())\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    \n",
    "    \n",
    "    joblib.dump(scaler, f\"{start_date}-{end_date}-scaler_tuned.pkl\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 16),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.15),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.7, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.7, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0, 0.8),\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 6),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 1),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 0, 1),\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"random_state\": 42,\n",
    "            \"n_jobs\": -1,\n",
    "        }\n",
    "    \n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        scores = []\n",
    "    \n",
    "        for train_idx, valid_idx in tscv.split(X_train_scaled):\n",
    "            X_t, X_v = X_train_scaled[train_idx], X_train_scaled[valid_idx]\n",
    "            y_t, y_v = y_train.values[train_idx], y_train.values[valid_idx]\n",
    "    \n",
    "            model.fit(X_t, y_t,\n",
    "                      eval_set=[(X_v, y_v)],\n",
    "                      verbose=False)\n",
    "            \n",
    "            y_pred = model.predict(X_v)\n",
    "            rmse = mean_squared_error(y_v, y_pred)\n",
    "            scores.append(rmse)\n",
    "    \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    \n",
    "    print(\"Best parameters:\", study.best_trial.params)\n",
    "    \n",
    "    best_params = study.best_trial.params\n",
    "    final_model = xgb.XGBRegressor(**best_params)\n",
    "    final_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    joblib.dump(model, f\"{start_date}-{end_date}-model-tuned.pkl\")\n",
    "    \n",
    "    # ---- 5. Evaluate on Test Set ----\n",
    "    y_pred = final_model.predict(X_test_scaled)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    print(\"Test R² Score:\", r2)\n",
    "    print(\"Test RMSE:\", rmse)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bf1279a-5675-428c-b04b-28dd81870e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_start = pd.read_csv('master_fut_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f973d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting bulk processing from 2025-09-01 to 2025-11-15\n",
      "Agg Done\n",
      "Variables Added\n",
      "Feature Engineering Complete. Preparing Training Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-01 19:31:56,628] A new study created in memory with name: no-name-13b82e32-b049-49a7-90ad-54e2fa093b1b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna Optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-01 19:31:58,160] Trial 0 finished with value: 0.0002862393694773392 and parameters: {'n_estimators': 140, 'max_depth': 10, 'learning_rate': 0.07618328265894635, 'subsample': 0.8615378056676061, 'colsample_bytree': 0.7654742358785116, 'gamma': 0.46180516603394245, 'min_child_weight': 6, 'reg_lambda': 0.05647248222355161, 'alpha': 0.6753793574611354}. Best is trial 0 with value: 0.0002862393694773392.\n",
      "[I 2025-12-01 19:32:03,604] Trial 1 finished with value: 0.00028623964747464945 and parameters: {'n_estimators': 483, 'max_depth': 14, 'learning_rate': 0.11376144367166414, 'subsample': 0.7483036266828061, 'colsample_bytree': 0.9795396370703044, 'gamma': 0.44472251861744444, 'min_child_weight': 1, 'reg_lambda': 0.5447976944988219, 'alpha': 0.5513373901629304}. Best is trial 0 with value: 0.0002862393694773392.\n",
      "[I 2025-12-01 19:32:06,946] Trial 2 finished with value: 0.00028617017569082905 and parameters: {'n_estimators': 290, 'max_depth': 12, 'learning_rate': 0.07447700465528206, 'subsample': 0.8215810605202123, 'colsample_bytree': 0.8383965739859391, 'gamma': 0.2789876342654735, 'min_child_weight': 4, 'reg_lambda': 0.3914496509391193, 'alpha': 0.10817747951034429}. Best is trial 2 with value: 0.00028617017569082905.\n",
      "[I 2025-12-01 19:32:10,691] Trial 3 finished with value: 0.0002862396465584938 and parameters: {'n_estimators': 325, 'max_depth': 5, 'learning_rate': 0.1403950388149969, 'subsample': 0.9182755745216606, 'colsample_bytree': 0.7027041387980545, 'gamma': 0.6008054702043, 'min_child_weight': 1, 'reg_lambda': 0.5719065189366244, 'alpha': 0.38263170363071586}. Best is trial 2 with value: 0.00028617017569082905.\n",
      "[I 2025-12-01 19:32:14,009] Trial 4 finished with value: 0.00028623863980050947 and parameters: {'n_estimators': 239, 'max_depth': 8, 'learning_rate': 0.14294656985146048, 'subsample': 0.9508114725150348, 'colsample_bytree': 0.784146184275088, 'gamma': 0.5518086155505824, 'min_child_weight': 1, 'reg_lambda': 0.17763628955057342, 'alpha': 0.3858495377995377}. Best is trial 2 with value: 0.00028617017569082905.\n",
      "[I 2025-12-01 19:32:15,732] Trial 5 finished with value: 0.00028623960738445467 and parameters: {'n_estimators': 141, 'max_depth': 11, 'learning_rate': 0.1223841226816235, 'subsample': 0.8376964148979866, 'colsample_bytree': 0.9464010938178737, 'gamma': 0.47341331239205275, 'min_child_weight': 2, 'reg_lambda': 0.5701012073559152, 'alpha': 0.9020886725061737}. Best is trial 2 with value: 0.00028617017569082905.\n",
      "[I 2025-12-01 19:32:20,610] Trial 6 finished with value: 0.0002862400155374666 and parameters: {'n_estimators': 458, 'max_depth': 4, 'learning_rate': 0.0851476722880013, 'subsample': 0.7755877828060015, 'colsample_bytree': 0.957461009577062, 'gamma': 0.13207307232871948, 'min_child_weight': 5, 'reg_lambda': 0.23383850141511542, 'alpha': 0.7496243315280254}. Best is trial 2 with value: 0.00028617017569082905.\n",
      "[I 2025-12-01 19:32:24,013] Trial 7 finished with value: 0.0002862383469507599 and parameters: {'n_estimators': 275, 'max_depth': 4, 'learning_rate': 0.07444401278124184, 'subsample': 0.8950297171449171, 'colsample_bytree': 0.933780294112916, 'gamma': 0.38026273212348793, 'min_child_weight': 6, 'reg_lambda': 0.27840082225027674, 'alpha': 0.8757087669928113}. Best is trial 2 with value: 0.00028617017569082905.\n",
      "[I 2025-12-01 19:32:28,786] Trial 8 finished with value: 0.0002862395182857886 and parameters: {'n_estimators': 438, 'max_depth': 12, 'learning_rate': 0.03162120148736701, 'subsample': 0.9782653005689042, 'colsample_bytree': 0.8228863974183016, 'gamma': 0.3845888008680195, 'min_child_weight': 2, 'reg_lambda': 0.32118222192732016, 'alpha': 0.6799218397929546}. Best is trial 2 with value: 0.00028617017569082905.\n",
      "[I 2025-12-01 19:32:32,144] Trial 9 finished with value: 0.0002862380219864367 and parameters: {'n_estimators': 309, 'max_depth': 15, 'learning_rate': 0.07331679647892357, 'subsample': 0.7010506628463521, 'colsample_bytree': 0.7386043752267789, 'gamma': 0.24040931989000358, 'min_child_weight': 2, 'reg_lambda': 0.2036277775443217, 'alpha': 0.4423422243020413}. Best is trial 2 with value: 0.00028617017569082905.\n",
      "[I 2025-12-01 19:32:36,496] Trial 10 finished with value: 0.00028624011721898706 and parameters: {'n_estimators': 384, 'max_depth': 8, 'learning_rate': 0.033531806686225706, 'subsample': 0.8053919189192252, 'colsample_bytree': 0.870603732748416, 'gamma': 0.7629356825703654, 'min_child_weight': 4, 'reg_lambda': 0.8990718201139418, 'alpha': 0.08437776481312532}. Best is trial 2 with value: 0.00028617017569082905.\n",
      "[I 2025-12-01 19:32:39,069] Trial 11 finished with value: 0.0002901087058114016 and parameters: {'n_estimators': 225, 'max_depth': 16, 'learning_rate': 0.05396142751605664, 'subsample': 0.7082775359599597, 'colsample_bytree': 0.7146100012166507, 'gamma': 0.16274503467022386, 'min_child_weight': 3, 'reg_lambda': 0.3704931996993883, 'alpha': 0.018842573943021634}. Best is trial 2 with value: 0.00028617017569082905.\n",
      "[I 2025-12-01 19:32:43,681] Trial 12 finished with value: 0.0002861594014039699 and parameters: {'n_estimators': 335, 'max_depth': 16, 'learning_rate': 0.09587711822595349, 'subsample': 0.7112038124936785, 'colsample_bytree': 0.8646182350233979, 'gamma': 0.23834917683710477, 'min_child_weight': 4, 'reg_lambda': 0.7430932087390127, 'alpha': 0.20235491271796746}. Best is trial 12 with value: 0.0002861594014039699.\n",
      "[I 2025-12-01 19:32:48,090] Trial 13 finished with value: 0.0002907360292195357 and parameters: {'n_estimators': 368, 'max_depth': 13, 'learning_rate': 0.09917033015453473, 'subsample': 0.7676221515994808, 'colsample_bytree': 0.8838754339728356, 'gamma': 0.043838766401131324, 'min_child_weight': 4, 'reg_lambda': 0.7533987037127705, 'alpha': 0.22734443709174884}. Best is trial 12 with value: 0.0002861594014039699.\n",
      "[I 2025-12-01 19:32:50,560] Trial 14 finished with value: 0.0002862392326652721 and parameters: {'n_estimators': 212, 'max_depth': 16, 'learning_rate': 0.05387289101242149, 'subsample': 0.8262357546988522, 'colsample_bytree': 0.8247230207034884, 'gamma': 0.26092758560555734, 'min_child_weight': 5, 'reg_lambda': 0.7392093195872667, 'alpha': 0.19781744497672682}. Best is trial 12 with value: 0.0002861594014039699.\n",
      "[I 2025-12-01 19:32:54,924] Trial 15 finished with value: 0.00028624046920489016 and parameters: {'n_estimators': 367, 'max_depth': 9, 'learning_rate': 0.09910096731612938, 'subsample': 0.739692841046763, 'colsample_bytree': 0.894501604814841, 'gamma': 0.30590521813459826, 'min_child_weight': 3, 'reg_lambda': 0.7267063027363461, 'alpha': 0.23012224348483568}. Best is trial 12 with value: 0.0002861594014039699.\n",
      "[I 2025-12-01 19:32:59,967] Trial 16 finished with value: 0.00028984247317924937 and parameters: {'n_estimators': 411, 'max_depth': 13, 'learning_rate': 0.05438947468611435, 'subsample': 0.8692171131202837, 'colsample_bytree': 0.8397874405093124, 'gamma': 0.007238441727110634, 'min_child_weight': 5, 'reg_lambda': 0.9884436950234823, 'alpha': 0.13266848672451936}. Best is trial 12 with value: 0.0002861594014039699.\n",
      "[I 2025-12-01 19:33:02,984] Trial 17 finished with value: 0.0002862840633833491 and parameters: {'n_estimators': 274, 'max_depth': 14, 'learning_rate': 0.09525490370418215, 'subsample': 0.7988079074016232, 'colsample_bytree': 0.7960046005024899, 'gamma': 0.1626195943513792, 'min_child_weight': 4, 'reg_lambda': 0.43502963889440455, 'alpha': 0.2994136507185274}. Best is trial 12 with value: 0.0002861594014039699.\n",
      "[I 2025-12-01 19:33:06,826] Trial 18 finished with value: 0.0002972220492589466 and parameters: {'n_estimators': 331, 'max_depth': 6, 'learning_rate': 0.11789478611308174, 'subsample': 0.8946146444571382, 'colsample_bytree': 0.900271609460671, 'gamma': 0.10182868390204297, 'min_child_weight': 3, 'reg_lambda': 0.6515991872727389, 'alpha': 0.0036355474774887597}. Best is trial 12 with value: 0.0002861594014039699.\n",
      "[I 2025-12-01 19:33:09,886] Trial 19 finished with value: 0.00028623803607928357 and parameters: {'n_estimators': 262, 'max_depth': 11, 'learning_rate': 0.0621005384405533, 'subsample': 0.735574979350489, 'colsample_bytree': 0.8551793147570513, 'gamma': 0.29472490128488027, 'min_child_weight': 5, 'reg_lambda': 0.8333966259212834, 'alpha': 0.5575397739483241}. Best is trial 12 with value: 0.0002861594014039699.\n",
      "[I 2025-12-01 19:33:11,942] Trial 20 finished with value: 0.0002864345482543882 and parameters: {'n_estimators': 179, 'max_depth': 15, 'learning_rate': 0.08845376643517461, 'subsample': 0.8006600590699053, 'colsample_bytree': 0.9272636994873905, 'gamma': 0.21983982821970194, 'min_child_weight': 4, 'reg_lambda': 0.4552088456460812, 'alpha': 0.3116554710343521}. Best is trial 12 with value: 0.0002861594014039699.\n",
      "[I 2025-12-01 19:33:15,434] Trial 21 finished with value: 0.00028623827902341713 and parameters: {'n_estimators': 310, 'max_depth': 15, 'learning_rate': 0.07187319393933855, 'subsample': 0.7031379692652979, 'colsample_bytree': 0.7420417845521043, 'gamma': 0.22316123704844515, 'min_child_weight': 3, 'reg_lambda': 0.05405002914224866, 'alpha': 0.4583820871698192}. Best is trial 12 with value: 0.0002861594014039699.\n",
      "[I 2025-12-01 19:33:19,711] Trial 22 finished with value: 0.0002861776426156382 and parameters: {'n_estimators': 348, 'max_depth': 16, 'learning_rate': 0.10577829585569416, 'subsample': 0.7045355220361342, 'colsample_bytree': 0.800072312398231, 'gamma': 0.33732137693868974, 'min_child_weight': 2, 'reg_lambda': 0.38561603588067117, 'alpha': 0.12159024608302443}. Best is trial 12 with value: 0.0002861594014039699.\n",
      "[I 2025-12-01 19:33:23,713] Trial 23 finished with value: 0.00028618283621417506 and parameters: {'n_estimators': 349, 'max_depth': 16, 'learning_rate': 0.10872128250130997, 'subsample': 0.7313739467503456, 'colsample_bytree': 0.8080636072469319, 'gamma': 0.32880487284318904, 'min_child_weight': 2, 'reg_lambda': 0.392010941079008, 'alpha': 0.11745490500510057}. Best is trial 12 with value: 0.0002861594014039699.\n",
      "[I 2025-12-01 19:33:28,628] Trial 24 finished with value: 0.0002862413735353599 and parameters: {'n_estimators': 410, 'max_depth': 13, 'learning_rate': 0.12985816338827844, 'subsample': 0.7744162121158319, 'colsample_bytree': 0.8528428647252587, 'gamma': 0.35785710961167394, 'min_child_weight': 4, 'reg_lambda': 0.6382030574900053, 'alpha': 0.15148798595281465}. Best is trial 12 with value: 0.0002861594014039699.\n",
      "[I 2025-12-01 19:33:32,121] Trial 25 finished with value: 0.00028624130925064507 and parameters: {'n_estimators': 288, 'max_depth': 14, 'learning_rate': 0.10523830457894642, 'subsample': 0.7215763836708793, 'colsample_bytree': 0.7669255037027417, 'gamma': 0.559547483022061, 'min_child_weight': 3, 'reg_lambda': 0.49491653413400016, 'alpha': 0.06574762615427292}. Best is trial 12 with value: 0.0002861594014039699.\n",
      "[I 2025-12-01 19:33:36,489] Trial 26 finished with value: 0.0002861583726866782 and parameters: {'n_estimators': 396, 'max_depth': 12, 'learning_rate': 0.12878937942148194, 'subsample': 0.7579021363032171, 'colsample_bytree': 0.915199624261284, 'gamma': 0.19640774832586272, 'min_child_weight': 5, 'reg_lambda': 0.1329589521834751, 'alpha': 0.2753121122347988}. Best is trial 26 with value: 0.0002861583726866782.\n",
      "[I 2025-12-01 19:33:41,475] Trial 27 finished with value: 0.00028667562944756326 and parameters: {'n_estimators': 395, 'max_depth': 11, 'learning_rate': 0.13078175488184832, 'subsample': 0.7546306092140436, 'colsample_bytree': 0.9073565433996781, 'gamma': 0.08185473220345363, 'min_child_weight': 5, 'reg_lambda': 0.11652718197769463, 'alpha': 0.28912191722413194}. Best is trial 26 with value: 0.0002861583726866782.\n",
      "[I 2025-12-01 19:33:46,525] Trial 28 finished with value: 0.0002862395357065198 and parameters: {'n_estimators': 434, 'max_depth': 12, 'learning_rate': 0.04129594627622915, 'subsample': 0.820829596074668, 'colsample_bytree': 0.869752667186485, 'gamma': 0.18255267741096454, 'min_child_weight': 6, 'reg_lambda': 0.151618728780833, 'alpha': 0.3601351342594745}. Best is trial 26 with value: 0.0002861583726866782.\n",
      "[I 2025-12-01 19:33:49,763] Trial 29 finished with value: 0.0002862399326087321 and parameters: {'n_estimators': 252, 'max_depth': 9, 'learning_rate': 0.1484027836128408, 'subsample': 0.8652478221307832, 'colsample_bytree': 0.9938172214383091, 'gamma': 0.4369916467266745, 'min_child_weight': 6, 'reg_lambda': 0.020933797570225177, 'alpha': 0.2185080255397619}. Best is trial 26 with value: 0.0002861583726866782.\n",
      "[I 2025-12-01 19:33:53,215] Trial 30 finished with value: 0.000286239241825119 and parameters: {'n_estimators': 199, 'max_depth': 10, 'learning_rate': 0.08818113170319108, 'subsample': 0.7902050052929941, 'colsample_bytree': 0.9244897997012299, 'gamma': 0.2744500641213962, 'min_child_weight': 5, 'reg_lambda': 0.8132570355312302, 'alpha': 0.984447380455074}. Best is trial 26 with value: 0.0002861583726866782.\n",
      "[I 2025-12-01 19:33:59,055] Trial 31 finished with value: 0.00028610023734068006 and parameters: {'n_estimators': 351, 'max_depth': 12, 'learning_rate': 0.12881609880933356, 'subsample': 0.7196998050877429, 'colsample_bytree': 0.8243661153794548, 'gamma': 0.2142563524689188, 'min_child_weight': 4, 'reg_lambda': 0.30989766095482646, 'alpha': 0.1572103358660781}. Best is trial 31 with value: 0.00028610023734068006.\n",
      "[I 2025-12-01 19:34:07,615] Trial 32 finished with value: 0.0002860286712181694 and parameters: {'n_estimators': 496, 'max_depth': 12, 'learning_rate': 0.13068924506412566, 'subsample': 0.7549463365229563, 'colsample_bytree': 0.8213085003655075, 'gamma': 0.18829837469517047, 'min_child_weight': 4, 'reg_lambda': 0.2827800341689895, 'alpha': 0.1986980026635773}. Best is trial 32 with value: 0.0002860286712181694.\n",
      "[I 2025-12-01 19:34:14,843] Trial 33 finished with value: 0.0002876667035365431 and parameters: {'n_estimators': 499, 'max_depth': 10, 'learning_rate': 0.12886738271147752, 'subsample': 0.7560961435071338, 'colsample_bytree': 0.8179918308974808, 'gamma': 0.10507575689062527, 'min_child_weight': 4, 'reg_lambda': 0.10322343472725863, 'alpha': 0.19300900262544723}. Best is trial 32 with value: 0.0002860286712181694.\n",
      "[I 2025-12-01 19:34:20,725] Trial 34 finished with value: 0.000286256489937376 and parameters: {'n_estimators': 460, 'max_depth': 13, 'learning_rate': 0.13828116423214823, 'subsample': 0.7215136953422474, 'colsample_bytree': 0.8647979682442093, 'gamma': 0.19355554895922356, 'min_child_weight': 4, 'reg_lambda': 0.27565787475916126, 'alpha': 0.28272335537379356}. Best is trial 32 with value: 0.0002860286712181694.\n",
      "[I 2025-12-01 19:34:26,765] Trial 35 finished with value: 0.000286238724451488 and parameters: {'n_estimators': 471, 'max_depth': 12, 'learning_rate': 0.12101711461217775, 'subsample': 0.7507234119363448, 'colsample_bytree': 0.7773152443203719, 'gamma': 0.12672724267534255, 'min_child_weight': 5, 'reg_lambda': 0.2834030072217612, 'alpha': 0.5527171431648467}. Best is trial 32 with value: 0.0002860286712181694.\n",
      "[I 2025-12-01 19:34:31,507] Trial 36 finished with value: 0.0002864812537543624 and parameters: {'n_estimators': 439, 'max_depth': 11, 'learning_rate': 0.13820199449429257, 'subsample': 0.7260432768595743, 'colsample_bytree': 0.8308903336033138, 'gamma': 0.0657336249242636, 'min_child_weight': 5, 'reg_lambda': 0.22850610329445276, 'alpha': 0.3515898215676565}. Best is trial 32 with value: 0.0002860286712181694.\n",
      "[I 2025-12-01 19:34:36,109] Trial 37 finished with value: 0.0002865915777972452 and parameters: {'n_estimators': 338, 'max_depth': 7, 'learning_rate': 0.11307746339930286, 'subsample': 0.7691749859153363, 'colsample_bytree': 0.9729103833682975, 'gamma': 0.21365910337960226, 'min_child_weight': 4, 'reg_lambda': 0.13249629386914744, 'alpha': 0.04630230079977085}. Best is trial 32 with value: 0.0002860286712181694.\n",
      "[I 2025-12-01 19:34:37,423] Trial 38 finished with value: 0.0002862403124811863 and parameters: {'n_estimators': 111, 'max_depth': 14, 'learning_rate': 0.1244869054867015, 'subsample': 0.7866433215287341, 'colsample_bytree': 0.8818848560535254, 'gamma': 0.42305635951650644, 'min_child_weight': 4, 'reg_lambda': 0.5165283175571984, 'alpha': 0.1615935919842344}. Best is trial 32 with value: 0.0002860286712181694.\n",
      "[I 2025-12-01 19:34:44,793] Trial 39 finished with value: 0.00028623998293942607 and parameters: {'n_estimators': 491, 'max_depth': 12, 'learning_rate': 0.14873524768875204, 'subsample': 0.7424498372331135, 'colsample_bytree': 0.8392226452666383, 'gamma': 0.16507249755026998, 'min_child_weight': 6, 'reg_lambda': 0.34537232294824083, 'alpha': 0.4246047165462099}. Best is trial 32 with value: 0.0002860286712181694.\n",
      "[I 2025-12-01 19:34:49,176] Trial 40 finished with value: 0.0002862396036077437 and parameters: {'n_estimators': 371, 'max_depth': 9, 'learning_rate': 0.13410582453035413, 'subsample': 0.7205453576490238, 'colsample_bytree': 0.9171106057403489, 'gamma': 0.49085414966125085, 'min_child_weight': 3, 'reg_lambda': 0.6185540695267532, 'alpha': 0.5065375110254424}. Best is trial 32 with value: 0.0002860286712181694.\n",
      "[I 2025-12-01 19:34:52,934] Trial 41 finished with value: 0.0002862390638065834 and parameters: {'n_estimators': 296, 'max_depth': 11, 'learning_rate': 0.06502876777784608, 'subsample': 0.848050031211754, 'colsample_bytree': 0.8445912526195622, 'gamma': 0.2613621553793258, 'min_child_weight': 4, 'reg_lambda': 0.308685386317217, 'alpha': 0.2580813248861404}. Best is trial 32 with value: 0.0002860286712181694.\n",
      "[I 2025-12-01 19:34:56,926] Trial 42 finished with value: 0.00028623934910949027 and parameters: {'n_estimators': 318, 'max_depth': 12, 'learning_rate': 0.07798487741084646, 'subsample': 0.9404253597591434, 'colsample_bytree': 0.9510783365932141, 'gamma': 0.7833761163579539, 'min_child_weight': 4, 'reg_lambda': 0.20657237580934168, 'alpha': 0.07502206740488333}. Best is trial 32 with value: 0.0002860286712181694.\n",
      "[I 2025-12-01 19:35:01,307] Trial 43 finished with value: 0.00028635576641713534 and parameters: {'n_estimators': 412, 'max_depth': 14, 'learning_rate': 0.11528344552470747, 'subsample': 0.7608312360371661, 'colsample_bytree': 0.7850869453350097, 'gamma': 0.13909582936165998, 'min_child_weight': 5, 'reg_lambda': 0.4233976691599767, 'alpha': 0.1768419878698843}. Best is trial 32 with value: 0.0002860286712181694.\n",
      "[I 2025-12-01 19:35:05,399] Trial 44 finished with value: 0.0002862383003500002 and parameters: {'n_estimators': 355, 'max_depth': 13, 'learning_rate': 0.08007373512190996, 'subsample': 0.8146475615241277, 'colsample_bytree': 0.8125992426756333, 'gamma': 0.7269750887056134, 'min_child_weight': 3, 'reg_lambda': 0.18525420954326918, 'alpha': 0.10568950813728407}. Best is trial 32 with value: 0.0002860286712181694.\n",
      "[I 2025-12-01 19:35:09,578] Trial 45 finished with value: 0.0002861656785583315 and parameters: {'n_estimators': 388, 'max_depth': 11, 'learning_rate': 0.14280145630705665, 'subsample': 0.783923942937807, 'colsample_bytree': 0.8882459495412525, 'gamma': 0.23914183737327172, 'min_child_weight': 4, 'reg_lambda': 0.25625003401550706, 'alpha': 0.2496259881497333}. Best is trial 32 with value: 0.0002860286712181694.\n",
      "[I 2025-12-01 19:35:13,905] Trial 46 finished with value: 0.00028624068355425226 and parameters: {'n_estimators': 391, 'max_depth': 10, 'learning_rate': 0.1445052890934467, 'subsample': 0.7458067496078412, 'colsample_bytree': 0.8812042397067663, 'gamma': 0.2475801800620025, 'min_child_weight': 4, 'reg_lambda': 0.2500616227462941, 'alpha': 0.3439581778149459}. Best is trial 32 with value: 0.0002860286712181694.\n",
      "[I 2025-12-01 19:35:18,952] Trial 47 finished with value: 0.0002861583755547791 and parameters: {'n_estimators': 437, 'max_depth': 11, 'learning_rate': 0.14188950904795003, 'subsample': 0.7775834077072835, 'colsample_bytree': 0.8923834635809266, 'gamma': 0.20500679127751073, 'min_child_weight': 5, 'reg_lambda': 0.07744748233072363, 'alpha': 0.24569772281327856}. Best is trial 32 with value: 0.0002860286712181694.\n",
      "[I 2025-12-01 19:35:24,109] Trial 48 finished with value: 0.0002862415612389688 and parameters: {'n_estimators': 478, 'max_depth': 8, 'learning_rate': 0.12273033484622407, 'subsample': 0.7143417661163615, 'colsample_bytree': 0.9344577106805029, 'gamma': 0.3083842789888611, 'min_child_weight': 5, 'reg_lambda': 0.07600068468702051, 'alpha': 0.32629148852689305}. Best is trial 32 with value: 0.0002860286712181694.\n",
      "[I 2025-12-01 19:35:28,900] Trial 49 finished with value: 0.0002862412686054079 and parameters: {'n_estimators': 428, 'max_depth': 12, 'learning_rate': 0.13641971831714672, 'subsample': 0.7336985300558395, 'colsample_bytree': 0.9061837366925007, 'gamma': 0.13758622877502416, 'min_child_weight': 6, 'reg_lambda': 0.07484555129072475, 'alpha': 0.3986265538089112}. Best is trial 32 with value: 0.0002860286712181694.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 496, 'max_depth': 12, 'learning_rate': 0.13068924506412566, 'subsample': 0.7549463365229563, 'colsample_bytree': 0.8213085003655075, 'gamma': 0.18829837469517047, 'min_child_weight': 4, 'reg_lambda': 0.2827800341689895, 'alpha': 0.1986980026635773}\n",
      "Retraining Final Model on ALL Data...\n",
      "Success! Saved 2025-09-01-2025-11-15-model-tuned.pkl and 2025-09-01-2025-11-15-scaler_tuned.pkl\n",
      "Reference Test R² Score: -0.005075094790603751\n",
      "Reference Test RMSE: 0.01686373381556758\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "from scipy.signal import hilbert\n",
    "from numba import njit\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import optuna\n",
    "import os\n",
    "\n",
    "# Use 'pip install hurst' if needed\n",
    "try:\n",
    "    from hurst import compute_Hc\n",
    "except ImportError:\n",
    "    pass \n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# 1. NUMBA OPTIMIZED FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "@njit\n",
    "def std_numba(arr):\n",
    "    n = arr.shape[0]\n",
    "    s = 0.0\n",
    "    mean = 0.0\n",
    "    for i in range(n):\n",
    "        mean += arr[i]\n",
    "    mean /= n\n",
    "    for i in range(n):\n",
    "        diff = arr[i] - mean\n",
    "        s += diff * diff\n",
    "    return np.sqrt(s / n)\n",
    "\n",
    "@njit\n",
    "def hurst_exponent_numba(ts, max_lag=100):\n",
    "    n = ts.shape[0]\n",
    "    if n < max_lag:\n",
    "        return np.nan\n",
    "    m = max_lag - 2\n",
    "    lags = np.empty(m)\n",
    "    tau = np.empty(m)\n",
    "    for i in range(m):\n",
    "        lag = i + 2\n",
    "        lags[i] = lag\n",
    "        diff_sum = 0.0\n",
    "        count = n - lag\n",
    "        s = 0.0\n",
    "        for j in range(count):\n",
    "            s += ts[j + lag] - ts[j]\n",
    "        mean_diff = s / count\n",
    "        var = 0.0\n",
    "        for j in range(count):\n",
    "            d = (ts[j + lag] - ts[j]) - mean_diff\n",
    "            var += d * d\n",
    "        tau[i] = np.sqrt(var / count)\n",
    "    sum_log_lags = 0.0\n",
    "    sum_log_tau = 0.0\n",
    "    for i in range(m):\n",
    "        sum_log_lags += np.log(lags[i])\n",
    "        sum_log_tau += np.log(tau[i])\n",
    "    mean_log_lags = sum_log_lags / m\n",
    "    mean_log_tau = sum_log_tau / m\n",
    "    cov = 0.0\n",
    "    var_ll = 0.0\n",
    "    for i in range(m):\n",
    "        diff_ll = np.log(lags[i]) - mean_log_lags\n",
    "        diff_tau = np.log(tau[i]) - mean_log_tau\n",
    "        cov += diff_ll * diff_tau\n",
    "        var_ll += diff_ll * diff_ll\n",
    "    slope = cov / var_ll\n",
    "    return slope\n",
    "    \n",
    "@njit\n",
    "def fractal_dimension_numba(ts, max_scale=20):\n",
    "    n = ts.shape[0]\n",
    "    if n < max_scale:\n",
    "        return np.nan\n",
    "    m = max_scale - 2\n",
    "    scales = np.empty(m)\n",
    "    variances = np.empty(m)\n",
    "    for i in range(m):\n",
    "        scale = i + 2\n",
    "        scales[i] = scale\n",
    "        npart = n // scale\n",
    "        var_sum = 0.0\n",
    "        for j in range(npart):\n",
    "            seg_mean = 0.0\n",
    "            for k in range(scale):\n",
    "                seg_mean += ts[j * scale + k]\n",
    "            seg_mean /= scale\n",
    "            seg_var = 0.0\n",
    "            for k in range(scale):\n",
    "                diff = ts[j * scale + k] - seg_mean\n",
    "                seg_var += diff * diff\n",
    "            seg_std = np.sqrt(seg_var / scale)\n",
    "            var_sum += seg_std\n",
    "        variances[i] = var_sum / npart\n",
    "    sum_log_scales = 0.0\n",
    "    sum_log_vars = 0.0\n",
    "    for i in range(m):\n",
    "        sum_log_scales += np.log(scales[i])\n",
    "        sum_log_vars += np.log(variances[i])\n",
    "    mean_log_scales = sum_log_scales / m\n",
    "    mean_log_vars = sum_log_vars / m\n",
    "    cov = 0.0\n",
    "    var_ll = 0.0\n",
    "    for i in range(m):\n",
    "        diff_sc = np.log(scales[i]) - mean_log_scales\n",
    "        diff_v = np.log(variances[i]) - mean_log_vars\n",
    "        cov += diff_sc * diff_v\n",
    "        var_ll += diff_sc * diff_sc\n",
    "    slope = cov / var_ll\n",
    "    return 2 - slope\n",
    "\n",
    "@njit\n",
    "def lyapunov_exponent_numba(time_series, epsilon=1e-4, steps=100):\n",
    "    N = time_series.shape[0]\n",
    "    if N <= steps:\n",
    "        return np.nan\n",
    "    s = 0.0\n",
    "    count = N - steps\n",
    "    diff0 = np.abs(time_series[steps] - time_series[0])\n",
    "    if diff0 < epsilon:\n",
    "        diff0 = epsilon\n",
    "    for i in range(count):\n",
    "        diff = np.abs(time_series[i + steps] - time_series[i])\n",
    "        if diff < epsilon:\n",
    "            diff = epsilon\n",
    "        s += np.log(diff / diff0)\n",
    "    return s / (count * steps)\n",
    "\n",
    "# ==========================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def clean_time_format(time_str):\n",
    "    if '.' not in time_str:\n",
    "        return f\"{time_str}.000000\"\n",
    "    else:\n",
    "        time_parts = time_str.split('.')\n",
    "        milliseconds = time_parts[1][:6].ljust(6, '0')\n",
    "        return f\"{time_parts[0]}.{milliseconds}\"  \n",
    "\n",
    "def additional_columns(df):\n",
    "    df = df.copy()\n",
    "    df['Session'] = np.where((df['Date'] != df['Date'].shift(1)) & (df['Time'] != pd.to_datetime('00:00:00').time()), 0, np.nan)\n",
    "    df['Session'] = np.where((df['Session'].shift(-1) == 0), 1, df['Session'])  \n",
    "    df.at[df.index[-1], 'Session'] = 1\n",
    "    return df\n",
    "\n",
    "def rolling_hilbert(series):\n",
    "    if len(series) < 10:\n",
    "        return np.nan\n",
    "    return np.angle(hilbert(series)[-1])\n",
    "    \n",
    "def forecast_next(x):\n",
    "        if len(x) < 2:\n",
    "            return x[-1]\n",
    "        X = np.arange(len(x))\n",
    "        coef = np.polyfit(X, x, 1)   \n",
    "        return np.polyval(coef, len(x))  \n",
    "\n",
    "# ==========================================\n",
    "# 3. FEATURE GENERATION (The Large Function)\n",
    "# ==========================================\n",
    "\n",
    "def generate_basic_variables(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    open_dict, high_dict, low_dict, close_dict, volume_dict = df['Open'].to_dict(), df['High'].to_dict(), df['Low'].to_dict(), df['Close'].to_dict(), df['Volume'].to_dict()\n",
    "    df['PrevOpen'] = df['PrevBar'].map(lambda x: open_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevHigh'] = df['PrevBar'].map(lambda x: high_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevLow'] = df['PrevBar'].map(lambda x: low_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevClose'] = df['PrevBar'].map(lambda x: close_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevVolume'] = df['PrevBar'].map(lambda x: volume_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['SecondInterval'] = np.floor(df['TimePassed']).where(df['TimePassed'] < 15, np.nan) + 1\n",
    "\n",
    "    df['Spread'] = (df['BestAsk'] - df['BestBid']).round(2)\n",
    "    df['SpreadSize'] = (df['AskSize'] - df['BidSize']).round(2)\n",
    "\n",
    "    df['LTQ'] = np.where(df['GroupNum'] == df['GroupNum'].shift(1), df['Volume'] - df['Volume'].shift(1), df['Volume'])\n",
    "    LTQ_dict = df['LTQ'].to_dict()\n",
    "    df['PrevLTQ'] = df['PrevBar'].map(lambda x: LTQ_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevPrevLTQ'] = df['PrevPrevBar'].map(lambda x: LTQ_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['MeanLTQ'] = df['LTQ'].expanding().mean().round(2)\n",
    "    df['StdLTQ'] = df['LTQ'].expanding().std().round(2)\n",
    "    df['SumPrevLTQ'] = df['PrevLTQ'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SumPrevLTQ'] = df['SumPrevLTQ'].ffill()\n",
    "    df['PrevMeanLTQ'] = (df['SumPrevLTQ']/(df['GroupNum']-1)).round(2)\n",
    "    df['ZScoreLTQ'] = ((df['LTQ'] - df['MeanLTQ'])/df['StdLTQ']).round(2)\n",
    "    df['SkewLTQVariance'] = ((df['LTQ'] - df['MeanLTQ'])**3).cumsum()\n",
    "    df['SkewLTQ'] = (df['SkewLTQVariance']/((df['StdLTQ']**3)*(df.index))).round(2)\n",
    "    df['LTQ_10th_Percentile'] = df['LTQ'].expanding().quantile(0.10)\n",
    "    df['LTQ_25th_Percentile'] = df['LTQ'].expanding().quantile(0.25)\n",
    "    df['LTQ_50th_Percentile'] = df['LTQ'].expanding().quantile(0.50)\n",
    "    df['LTQ_75th_Percentile'] = df['LTQ'].expanding().quantile(0.75)\n",
    "    df['LTQ_90th_Percentile'] = df['LTQ'].expanding().quantile(0.90)\n",
    "    df['LTQIQR'] = (df['LTQ_75th_Percentile'] - df['LTQ_25th_Percentile']).round(2)\n",
    "    df['PrevAutoCorLTQNumerator'] = ((df['LTQ'] - df['MeanLTQ']) * (df['LTQ'].shift(1) - df['MeanLTQ'])).cumsum()\n",
    "    df['PrevAutoCorLTQDenominator'] = np.square(df['LTQ']- df['MeanLTQ']).cumsum()\n",
    "    df['AutoCorrelationLTQ'] = (df['PrevAutoCorLTQNumerator']/df['PrevAutoCorLTQDenominator']).round(2)\n",
    "    df['DeviationFlag'] = np.where(df['LTQ'] > (df['MeanLTQ'] + df['StdLTQ']), 1, 0)\n",
    "    df['AbsoluteDeviationLTQ'] = df['DeviationFlag'].cumsum()\n",
    "    df['SumAbsoluteDeviationLTQ'] = df['LTQ'].where(df['AbsoluteDeviationLTQ'] != df['AbsoluteDeviationLTQ'].shift(1)).cumsum()\n",
    "    df['SumAbsoluteDeviationLTQ'] = df['SumAbsoluteDeviationLTQ'].ffill()\n",
    "    df['MeanAbsoluteDeviationLTQ'] = np.where(df['AbsoluteDeviationLTQ'] != 0, (df['SumAbsoluteDeviationLTQ']/df['AbsoluteDeviationLTQ']).round(2), np.nan)\n",
    "    \n",
    "    volume_dict = df['Volume'].to_dict()\n",
    "    df['PrevPrevVolume'] = df['PrevPrevBar'].map(lambda x: volume_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['SumPrevVolume'] = df['PrevVolume'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SumPrevVolume'] = df['SumPrevVolume'].ffill()\n",
    "    df['PrevMeanVolume'] = (df['SumPrevVolume']/(df['GroupNum']-1)).round(2)\n",
    "    df['MeanVolume'] = ((df['SumPrevVolume'] + df['Volume'])/df['GroupNum']).round(2)\n",
    "    df['ResidualsVolume'] = np.square(df['PrevVolume'] - df['PrevMeanVolume']).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['ResidualsVolume'] = df['ResidualsVolume'].ffill()\n",
    "    df['PrevStdVolume'] = np.sqrt(df['ResidualsVolume']/df['GroupNum']-1)\n",
    "    df['ResidualsVolume'] = df['ResidualsVolume'] + np.square(df['Volume'] - df['MeanVolume'])\n",
    "    df['StdVolume'] = (np.sqrt(df['ResidualsVolume']/df['GroupNum'])).round(2)\n",
    "    df['PrevZScoreVolume'] = ((df['PrevVolume']-df['PrevMeanVolume'])/df['PrevStdVolume']).round(2)\n",
    "    df['ZScoreVolume'] = ((df['Volume']-df['MeanVolume'])/df['StdVolume']).round(2)\n",
    "    df['SkewNumerator'] = ((df['PrevVolume'] - df['PrevMeanVolume'])**3).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SkewNumerator'] = df['SkewNumerator'].ffill()\n",
    "    df['PrevSkewVariance'] = df['SkewNumerator']/(df['GroupNum']-1)\n",
    "    df['PrevSkewVolume'] = df['PrevSkewVariance']/(df['PrevStdVolume']**3)\n",
    "    df['SkewVariance'] = (df['SkewNumerator'] + (df['Volume'] - df['MeanVolume'])**3)/df['GroupNum']\n",
    "    df['SkewVolume'] = (df['SkewVariance']/(df['StdVolume']**3)).round(2)\n",
    "    df['CVVolume'] = (df['StdVolume']/df['MeanVolume']).round(2)\n",
    "    df['KurtosisNumerator'] = ((df['PrevVolume'] - df['PrevMeanVolume'])**4).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['KurtosisNumerator'] = df['KurtosisNumerator'].ffill()\n",
    "    df['PrevKurtosisVariance'] = df['KurtosisNumerator']/(df['GroupNum']-1)\n",
    "    df['PrevKurtosisVolume'] = df['PrevKurtosisVariance']/(df['PrevStdVolume']**4)-3\n",
    "    df['KurtosisVariance'] = (df['KurtosisNumerator'] + (df['Volume'] - df['MeanVolume'])**4)/df['GroupNum']\n",
    "    df['KurtosisVolume'] = (df['KurtosisVariance']/(df['StdVolume']**4)-3).round(2)\n",
    "    df['Volume_10th_Percentile'] = df['Volume'].expanding().quantile(0.10)\n",
    "    df['Volume_25th_Percentile'] = df['Volume'].expanding().quantile(0.25)\n",
    "    df['Volume_50th_Percentile'] = df['Volume'].expanding().quantile(0.50)\n",
    "    df['Volume_75th_Percentile'] = df['Volume'].expanding().quantile(0.75)\n",
    "    df['Volume_90th_Percentile'] = df['Volume'].expanding().quantile(0.90)\n",
    "    df['VolumeIQR'] = (df['Volume_75th_Percentile'] - df['Volume_25th_Percentile']).round(2)\n",
    "    df['PrevAutoCorVolumeNumerator'] = ((df['PrevVolume'] - df['PrevMeanVolume']) * (df['PrevPrevVolume'] - df['PrevMeanVolume'])).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['PrevAutoCorVolumeDenominator'] = np.square(df['PrevVolume'] - df['PrevMeanVolume']).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['AutoCorrelationVolume'] = (df['PrevAutoCorVolumeNumerator']/df['PrevAutoCorVolumeDenominator']).round(2)\n",
    "    df['AutoCorrelationVolume'] = df['AutoCorrelationVolume'].ffill()\n",
    "    mask = df['GroupNum'] != df['GroupNum'].shift(1)\n",
    "    df['TimeSumPrevVolume'] = (df.groupby('GroupNumTime')['PrevVolume'].apply(lambda group: group.where(mask).cumsum()).reset_index(level=0, drop=True))\n",
    "    df['TimeSumPrevVolume'] = df['TimeSumPrevVolume'].ffill()\n",
    "    df['TimeSumGroupNumTime'] = (df.groupby('GroupNumTime').apply(lambda group: group['GroupNum'].ne(group['GroupNum'].shift()).cumsum()).reset_index(level=0, drop=True))\n",
    "    df['TimeSumPrevVolume'] = np.where(df['GroupNumTime'] == 1, 0, df['TimeSumPrevVolume'])\n",
    "    df['TimeMeanVolume'] = (df['TimeSumPrevVolume']/df['TimeSumGroupNumTime']).round(2)\n",
    "    df['VolumeOutlierSum1'] = df['PrevVolume'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreVolume'].shift(1) > 1)).cumsum()\n",
    "    df['VolumeOutlierSum1'] = df['VolumeOutlierSum1'].ffill()\n",
    "    df['VolumeOutlierSum2'] = df['PrevVolume'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreVolume'].shift(1) > 2)).cumsum()\n",
    "    df['VolumeOutlierSum2'] = df['VolumeOutlierSum2'].ffill()\n",
    "    df['VolumeOutlierSum3'] = df['PrevVolume'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreVolume'].shift(1) > 3)).cumsum()\n",
    "    df['VolumeOutlierSum3'] = df['VolumeOutlierSum3'].ffill()\n",
    "    df['VolumeOutlierImpact1'] = (df['VolumeOutlierSum1']/df['SumPrevVolume']).round(2)\n",
    "    df['VolumeOutlierImpact2'] = (df['VolumeOutlierSum2']/df['SumPrevVolume']).round(2)\n",
    "    df['VolumeOutlierImpact3'] = (df['VolumeOutlierSum3']/df['SumPrevVolume']).round(2)\n",
    "    df['ForwardVolume'] = np.where(df['GroupNum'] != df['GroupNum'].shift(1), df['Volume'].shift(1), np.nan)\n",
    "    df['ForwardVolume'] = df['ForwardVolume'].shift(-1)\n",
    "    df['ForwardVolume'] = df['ForwardVolume'].bfill()\n",
    "    df['ProportionVolume'] = (df['Volume']/df['ForwardVolume']).round(2)\n",
    "    df['SpeedVolume'] = ((df['Volume']*15)/df['TimePassed']).round(2)\n",
    "    df['SumIntervalMeanVolume'] = (df.groupby('SecondInterval')['ProportionVolume'].apply(lambda group: group.where(df['SecondInterval'] != df['SecondInterval'].shift(1)).cumsum()).reset_index(level=0, drop=True))\n",
    "    df['SumIntervalMeanVolume'] = df['SumIntervalMeanVolume'].ffill()\n",
    "    df['IntervalMeanVolume'] = (df['SumIntervalMeanVolume']/df['GroupNum']).round(2)\n",
    "\n",
    "    df['Range'] = df['High'] - df['Low']\n",
    "    df['ShiftedRange'] = df['Range'].shift(1).where(df['GroupNum'] != df['GroupNum'].shift(1))\n",
    "    df['MinRange'] = df['ShiftedRange'].cummin()\n",
    "    df['MinRange'] = df['MinRange'].ffill()\n",
    "    range_dict = df['Range'].to_dict()\n",
    "    df['PrevRange'] = df['PrevBar'].map(lambda x: range_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevPrevRange'] = df['PrevPrevBar'].map(lambda x: range_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['SumPrevRange'] = df['PrevRange'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SumPrevRange'] = df['SumPrevRange'].ffill()\n",
    "    df['PrevMeanRange'] = (df['SumPrevRange']/(df['GroupNum']-1)).round(2)\n",
    "    df['MeanRange'] = ((df['SumPrevRange'] + df['Range'])/df['GroupNum']).round(2)\n",
    "    df['ResidualsRange'] = np.square(df['PrevRange'] - df['PrevMeanRange']).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['ResidualsRange'] = df['ResidualsRange'].ffill()\n",
    "    df['PrevStdRange'] = np.sqrt(df['ResidualsRange']/df['GroupNum']-1)\n",
    "    df['ResidualsRange'] = df['ResidualsRange'] + np.square(df['Range'] - df['MeanRange'])\n",
    "    df['StdRange'] = (np.sqrt(df['ResidualsRange']/df['GroupNum'])).round(2)\n",
    "    df['PrevZScoreRange'] = ((df['PrevRange']-df['PrevMeanRange'])/df['PrevStdRange']).round(2)\n",
    "    df['ZScoreRange'] = ((df['Range']-df['MeanRange'])/df['StdRange']).round(2)\n",
    "    df['MaxRange'] = df['Range'].where(df['ZScoreRange'] <= 3).cummax()\n",
    "    df['MaxRange'] = df['MaxRange'].ffill()\n",
    "    df['SkewNumerator'] = ((df['PrevRange'] - df['PrevMeanRange'])**3).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SkewNumerator'] = df['SkewNumerator'].ffill()\n",
    "    df['PrevSkewVariance'] = df['SkewNumerator']/(df['GroupNum']-1)\n",
    "    df['PrevSkewRange'] = df['PrevSkewVariance']/(df['PrevStdRange']**3)\n",
    "    df['SkewVariance'] = (df['SkewNumerator'] + (df['Range'] - df['MeanRange'])**3)/df['GroupNum']\n",
    "    df['SkewRange'] = (df['SkewVariance']/(df['StdRange']**3)).round(2)\n",
    "    df['CVRange'] = (df['StdRange']/df['MeanRange']).round(2)\n",
    "    df['KurtosisNumerator'] = ((df['PrevRange'] - df['PrevMeanRange'])**4).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['KurtosisNumerator'] = df['KurtosisNumerator'].ffill()\n",
    "    df['PrevKurtosisVariance'] = df['KurtosisNumerator']/(df['GroupNum']-1)\n",
    "    df['PrevKurtosisRange'] = df['PrevKurtosisVariance']/(df['PrevStdRange']**4)-3\n",
    "    df['KurtosisVariance'] = (df['KurtosisNumerator'] + (df['Range'] - df['MeanRange'])**4)/df['GroupNum']\n",
    "    df['KurtosisRange'] = (df['KurtosisVariance']/(df['StdRange']**4)-3).round(2)\n",
    "    df['Range_10th_Percentile'] = df['Range'].expanding().quantile(0.10)\n",
    "    df['Range_25th_Percentile'] = df['Range'].expanding().quantile(0.25)\n",
    "    df['Range_50th_Percentile'] = df['Range'].expanding().quantile(0.50)\n",
    "    df['Range_75th_Percentile'] = df['Range'].expanding().quantile(0.75)\n",
    "    df['Range_90th_Percentile'] = df['Range'].expanding().quantile(0.90)\n",
    "    df['RangeIQR'] = (df['Range_75th_Percentile'] - df['Range_25th_Percentile']).round(2)\n",
    "    df['PrevAutoCorRangeNumerator'] = ((df['PrevRange'] - df['PrevMeanRange']) * (df['PrevPrevRange'] - df['PrevMeanRange'])).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['PrevAutoCorRangeDenominator'] = np.square(df['PrevRange'] - df['PrevMeanRange']).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['AutoCorrelationRange'] = (df['PrevAutoCorRangeNumerator']/df['PrevAutoCorRangeDenominator']).round(2)\n",
    "    df['AutoCorrelationRange'] = df['AutoCorrelationRange'].ffill()\n",
    "    df['NormalizedRange'] = np.minimum(((df['Range'] - df['MinRange'])/(df['MaxRange'] - df['MinRange'])).round(2), 1)\n",
    "    mask = df['GroupNum'] != df['GroupNum'].shift(1)\n",
    "    df['TimeSumPrevRange'] = (df.groupby('GroupNumTime')['PrevRange'].apply(lambda group: group.where(mask).cumsum()).reset_index(level=0, drop=True))\n",
    "    df['TimeSumPrevRange'] = df['TimeSumPrevRange'].ffill()\n",
    "    df['TimeSumGroupNumTime'] = (df.groupby('GroupNumTime').apply(lambda group: group['GroupNum'].ne(group['GroupNum'].shift()).cumsum()).reset_index(level=0, drop=True))\n",
    "    df['TimeSumPrevRange'] = np.where(df['GroupNumTime'] == 1, 0, df['TimeSumPrevRange'])\n",
    "    df['TimeMeanRange'] = (df['TimeSumPrevRange']/df['TimeSumGroupNumTime']).round(2)\n",
    "    df['RangeOutlierSum1'] = df['PrevRange'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreRange'].shift(1) > 1)).cumsum()\n",
    "    df['RangeOutlierSum1'] = df['RangeOutlierSum1'].ffill()\n",
    "    df['RangeOutlierSum2'] = df['PrevRange'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreRange'].shift(1) > 2)).cumsum()\n",
    "    df['RangeOutlierSum2'] = df['RangeOutlierSum2'].ffill()\n",
    "    df['RangeOutlierSum3'] = df['PrevRange'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreRange'].shift(1) > 3)).cumsum()\n",
    "    df['RangeOutlierSum3'] = df['RangeOutlierSum3'].ffill()\n",
    "    df['RangeOutlierImpact1'] = (df['RangeOutlierSum1']/df['SumPrevRange']).round(2)\n",
    "    df['RangeOutlierImpact2'] = (df['RangeOutlierSum2']/df['SumPrevRange']).round(2)\n",
    "    df['RangeOutlierImpact3'] = (df['RangeOutlierSum3']/df['SumPrevRange']).round(2)\n",
    "    df['ForwardRange'] = np.where(df['GroupNum'] != df['GroupNum'].shift(1), df['Range'].shift(1), np.nan)\n",
    "    df['ForwardRange'] = df['ForwardRange'].shift(-1)\n",
    "    df['ForwardRange'] = df['ForwardRange'].bfill()\n",
    "    df['ProportionRange'] = (df['Range']/df['ForwardRange']).round(2)\n",
    "    df['SpeedRange'] = ((df['Range']*15)/df['TimePassed']).round(2)\n",
    "    df['SumIntervalMeanRange'] = (df.groupby('SecondInterval')['ProportionRange'].apply(lambda group: group.where(df['SecondInterval'] != df['SecondInterval'].shift(1)).cumsum()).reset_index(level=0, drop=True))\n",
    "    df['SumIntervalMeanRange'] = df['SumIntervalMeanRange'].ffill()\n",
    "    df['IntervalMeanRange'] = (df['SumIntervalMeanRange']/df['GroupNum']).round(2)\n",
    "\n",
    "    df['Lows'] = (df['Low'] - df['PrevLow']).round(2)\n",
    "    df['Lows'] = np.where(df['SessionBar'] == True, 0, df['Lows'])\n",
    "    df['Lows'] = np.where(df['Lows'] < 0, 0, df['Lows'])\n",
    "    df['LastLows'] = np.where(df['GroupNum'] != df['GroupNum'].shift(1), df['Lows'].shift(1), np.nan)\n",
    "    df['LastLows'] = df['LastLows'].shift(1)\n",
    "    df['LastLows'] = df['LastLows'].ffill()\n",
    "    df['LastLows'] = np.where(df['LastLows'] < 0, 0, df['LastLows'])\n",
    "    lows_dict = df['Lows'].to_dict() \n",
    "    df['PrevLows'] = df['PrevBar'].map(lambda x: lows_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['NumberOfLows'] = ((df['PrevLows'] > 0) & (df['GroupNum'] != df['GroupNum'].shift(1))).cumsum()\n",
    "    df['CurrentNoOfLows'] = np.where(df['Lows'] > 0, df['NumberOfLows']+1, df['NumberOfLows'])\n",
    "    df['SumPrevLows'] = df['PrevLows'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SumPrevLows'] = df['SumPrevLows'].ffill()\n",
    "    df['PrevMeanLows'] = (df['SumPrevLows']/df['NumberOfLows']).round(2)\n",
    "    df['MeanLows'] = ((df['SumPrevLows'] + df['Lows'])/df['CurrentNoOfLows']).round(2)\n",
    "    df['ResidualsLows'] = (np.square(df['PrevLows'] - df['PrevMeanLows']).where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['PrevLows'] > 0)).cumsum()).round(2)\n",
    "    df['ResidualsLows'] = df['ResidualsLows'].ffill()\n",
    "    df['PrevStdLows'] = np.sqrt(df['ResidualsLows']/df['NumberOfLows'])\n",
    "    df['ResidualsLows'] = df['ResidualsLows'] + np.square(df['Lows'] - df['MeanLows'])\n",
    "    df['StdLows'] = (np.sqrt(df['ResidualsLows']/df['CurrentNoOfLows'])).round(2)\n",
    "    df['PrevZScoreLows'] = ((df['PrevLows']-df['PrevMeanLows'])/df['PrevStdLows']).round(2)\n",
    "    df['ZScoreLows'] = ((df['Lows']-df['MeanLows'])/df['StdLows']).round(2)\n",
    "    df['SkewLowsNumerator'] = ((df['PrevLows'] - df['PrevMeanLows'])**3).where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['PrevLows'] > 0)).cumsum()\n",
    "    df['SkewLowsNumerator'] = df['SkewLowsNumerator'].ffill()\n",
    "    df['PrevSkewLowsVariance'] = df['SkewLowsNumerator']/(df['NumberOfLows'])\n",
    "    df['PrevSkewLows'] = df['PrevSkewLowsVariance']/(df['PrevStdLows']**3)\n",
    "    df['SkewLowsVariance'] = (df['SkewLowsNumerator'] + (df['Lows'] - df['MeanLows'])**3)/df['CurrentNoOfLows']\n",
    "    df['SkewLows'] = (df['SkewLowsVariance']/(df['StdLows']**3)).round(2)\n",
    "    df['CVLows'] = (df['StdLows']/df['MeanLows']).round(2)\n",
    "    df['KurtosisLowsNumerator'] = ((df['PrevLows'] - df['PrevMeanLows'])**4).where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['PrevLows'] > 0)).cumsum()\n",
    "    df['KurtosisLowsNumerator'] = df['KurtosisLowsNumerator'].ffill()\n",
    "    df['PrevKurtosisLowsVariance'] = df['KurtosisLowsNumerator']/(df['NumberOfLows'])\n",
    "    df['PrevKurtosisLows'] = df['PrevKurtosisLowsVariance']/(df['PrevStdLows']**4)-3\n",
    "    df['KurtosisLowsVariance'] = (df['KurtosisLowsNumerator'] + (df['Lows'] - df['MeanLows'])**4)/df['CurrentNoOfLows']\n",
    "    df['KurtosisLows'] = (df['KurtosisLowsVariance']/(df['StdLows']**4)-3).round(2)\n",
    "    df['Lows_10th_Percentile'] = df['Lows'].expanding().quantile(0.10)\n",
    "    df['Lows_25th_Percentile'] = df['Lows'].expanding().quantile(0.25)\n",
    "    df['Lows_50th_Percentile'] = df['Lows'].expanding().quantile(0.50)\n",
    "    df['Lows_75th_Percentile'] = df['Lows'].expanding().quantile(0.75)\n",
    "    df['Lows_90th_Percentile'] = df['Lows'].expanding().quantile(0.90)\n",
    "    df['LowsIQR'] = (df['Lows_75th_Percentile'] - df['Lows_25th_Percentile']).round(2)\n",
    "\n",
    "    df['Highs'] = (df['PrevHigh'] - df['High']).round(2)\n",
    "    df['Highs'] = np.where(df['SessionBar'] == True, 0, df['Highs'])\n",
    "    df['Highs'] = np.where(df['Highs'] < 0, 0, df['Highs'])\n",
    "    df['LastHighs'] = np.where(df['GroupNum'] != df['GroupNum'].shift(1), df['Highs'].shift(1), np.nan)\n",
    "    df['LastHighs'] = df['LastHighs'].shift(1)\n",
    "    df['LastHighs'] = df['LastHighs'].ffill()\n",
    "    df['LastHighs'] = np.where(df['LastHighs'] < 0, 0, df['LastHighs'])\n",
    "    highs_dict = df['Highs'].to_dict()\n",
    "    df['PrevHighs'] = df['PrevBar'].map(lambda x: highs_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['NumberOfHighs'] = ((df['PrevHighs'] > 0) & (df['GroupNum'] != df['GroupNum'].shift(1))).cumsum()\n",
    "    df['CurrentNoOfHighs'] = np.where(df['Lows'] > 0, df['NumberOfHighs']+1, df['NumberOfHighs'])\n",
    "    df['SumPrevHighs'] = df['PrevHighs'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SumPrevHighs'] = df['SumPrevHighs'].ffill()\n",
    "    df['PrevMeanHighs'] = (df['SumPrevHighs']/df['NumberOfHighs']).round(2)\n",
    "    df['MeanHighs'] = ((df['SumPrevHighs'] + df['Highs'])/df['CurrentNoOfHighs']).round(2)\n",
    "    df['ResidualsHighs'] = (np.square(df['PrevHighs'] - df['PrevMeanHighs']).where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['PrevHighs'] > 0)).cumsum()).round(2)\n",
    "    df['ResidualsHighs'] = df['ResidualsHighs'].ffill()\n",
    "    df['PrevStdHighs'] = np.sqrt(df['ResidualsHighs']/df['NumberOfHighs'])\n",
    "    df['ResidualsHighs'] = df['ResidualsHighs'] + np.square(df['Highs'] - df['MeanHighs'])\n",
    "    df['StdHighs'] = (np.sqrt(df['ResidualsHighs']/df['CurrentNoOfHighs'])).round(2)\n",
    "    df['PrevZScoreHighs'] = ((df['PrevHighs']-df['PrevMeanHighs'])/df['PrevStdHighs']).round(2)\n",
    "    df['ZScoreHighs'] = ((df['Highs']-df['MeanHighs'])/df['StdHighs']).round(2)\n",
    "    df['SkewHighsNumerator'] = ((df['PrevHighs'] - df['PrevMeanHighs'])**3).where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['PrevHighs'] > 0)).cumsum()\n",
    "    df['SkewHighsNumerator'] = df['SkewHighsNumerator'].ffill()\n",
    "    df['PrevSkewHighsVariance'] = df['SkewHighsNumerator']/(df['NumberOfHighs'])\n",
    "    df['PrevSkewHighs'] = df['PrevSkewHighsVariance']/(df['PrevStdHighs']**3)\n",
    "    df['SkewHighsVariance'] = (df['SkewHighsNumerator'] + (df['Highs'] - df['MeanHighs'])**3)/df['CurrentNoOfHighs']\n",
    "    df['SkewHighs'] = (df['SkewHighsVariance']/(df['StdHighs']**3)).round(2)\n",
    "    df['CVHighs'] = (df['StdHighs']/df['MeanHighs']).round(2)\n",
    "    df['KurtosisHighsNumerator'] = ((df['PrevHighs'] - df['PrevMeanHighs'])**4).where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['PrevHighs'] > 0)).cumsum()\n",
    "    df['KurtosisHighsNumerator'] = df['KurtosisHighsNumerator'].ffill()\n",
    "    df['PrevKurtosisHighsVariance'] = df['KurtosisHighsNumerator']/(df['NumberOfHighs'])\n",
    "    df['PrevKurtosisHighs'] = df['PrevKurtosisHighsVariance']/(df['PrevStdHighs']**4)-3\n",
    "    df['KurtosisHighsVariance'] = (df['KurtosisHighsNumerator'] + (df['Highs'] - df['MeanHighs'])**4)/df['CurrentNoOfHighs']\n",
    "    df['KurtosisHighs'] = (df['KurtosisHighsVariance']/(df['StdHighs']**4)-3).round(2)\n",
    "    df['Highs_10th_Percentile'] = df['Highs'].expanding().quantile(0.10)\n",
    "    df['Highs_25th_Percentile'] = df['Highs'].expanding().quantile(0.25)\n",
    "    df['Highs_50th_Percentile'] = df['Highs'].expanding().quantile(0.50)\n",
    "    df['Highs_75th_Percentile'] = df['Highs'].expanding().quantile(0.75)\n",
    "    df['Highs_90th_Percentile'] = df['Highs'].expanding().quantile(0.90)\n",
    "    df['HighsIQR'] = (df['Highs_75th_Percentile'] - df['Highs_25th_Percentile']).round(2)\n",
    "\n",
    "    df['le_next_low_pred'] = 2 * df['Low'] - df['Low'].shift(1)\n",
    "    df['le_next_high_pred'] = 2 * df['High'] - df['High'].shift(1) \n",
    "    df['atr'] = df['Range'].rolling(window=14, min_periods=1).mean()  \n",
    "    df['atr_next_low_pred'] = df['Low'] - 0.5 * df['atr']\n",
    "    df['atr_next_high_pred'] = df['High'] + 0.5 * df['atr'] \n",
    "    df['reg_next_low_pred'] = df['Low'].rolling(7, min_periods=2).apply(forecast_next, raw=False)\n",
    "    df['reg_next_high_pred'] = df['High'].rolling(7, min_periods=2).apply(forecast_next, raw=False)\n",
    "    df['ema_low'] = df['Low'].ewm(span=10, adjust=False).mean()\n",
    "    df['ema_high'] = df['High'].ewm(span=10, adjust=False).mean()\n",
    "    df['low_std'] = df['Low'].rolling(window=10, min_periods=1).std()\n",
    "    df['high_std'] = df['High'].rolling(window=10, min_periods=1).std()\n",
    "    df['ema_next_low_pred'] = df['ema_low'] - df['low_std']\n",
    "    df['ema_next_high_pred'] = df['ema_high'] + df['high_std']\n",
    "    #ensemble of em all\n",
    "    df['predicted_next_low'] = (df['le_next_low_pred'] + df['atr_next_low_pred'] + df['reg_next_low_pred']) / 3\n",
    "    df['predicted_next_high'] = (df['le_next_high_pred'] + df['atr_next_high_pred'] + df['reg_next_high_pred']) / 3\n",
    "    df['feature_imbalance'] = (df['BidSize'] - df['AskSize']) / (df['BidSize'] + df['AskSize'] + 1e-9)\n",
    "    df['realtive_spread'] = df['Spread'] / ((df['BestBid'] + df['BestAsk']) / 2)\n",
    "    #rapdi changed in bid ask prices - quote stuffing\n",
    "    df['quote_stuffing'] = df['BestBid'].diff().abs() + df['BestAsk'].diff().abs()\n",
    "    #liq based    \n",
    "    df['depth_slope'] = (df['BidSize'] - df['AskSize']) / (df['BidSize'] + df['AskSize'] + 1e-9)\n",
    "    # Hidden Liquidity Detection (large orders appearing and disappearing)\n",
    "    df['hidden_liquidity'] = ((df['BidSize'] > df['BidSize'].shift(1) * 2) & (df['BidSize'].shift(-1) < df['BidSize'] / 2)).astype(int)\n",
    "    ## 4. Price Action & Trend Features\n",
    "    df['price_momentum'] = df['Close'].diff()\n",
    "    df['price_momentum_ratio'] = df['Close'] / df['Close'].shift(1)\n",
    "    \n",
    "    # Hilbert Transform (Measures Market Cycles)\n",
    "    df['hilbert_phase'] = df['Close'].rolling(50).apply(rolling_hilbert, raw=True).shift(1)\n",
    "    \n",
    "    # Fraction of Large Orders (Relative to Volume)\n",
    "    df['large_order_fraction'] = (df['BidSize'] + df['AskSize']) / (df['Volume'] + 1e-9)\n",
    "    \n",
    "    #micorstucture volatlity\n",
    "    df['mvr'] = df['Close'].pct_change().rolling(5).std().shift(1) / df['Close'].pct_change().rolling(30).std().shift(1)\n",
    "\n",
    "    df = df.drop(columns = ['PrevRange', 'GroupNumTime', 'PrevMeanRange', 'PrevStdRange', 'PrevZScoreRange', 'PrevSkewRange', 'PrevKurtosisVariance', \n",
    "                            'PrevKurtosisRange', 'PrevMeanLows', 'PrevLows', 'PrevMeanHighs', 'PrevLows', 'PrevHighs', 'PrevStdLows', \n",
    "                            'PrevStdHighs', 'PrevZScoreLows', 'PrevZScoreHighs', 'PrevSkewLows', 'PrevKurtosisLows', 'PrevZScoreHighs',\n",
    "                            'PrevSkewHighs', 'PrevKurtosisHighs', 'MinRange', 'MaxRange'])\n",
    "    df = df.drop(columns = ['Group', 'Session', 'SessionBar', 'PrevBar', 'PrevPrevBar', 'PrevOpen', 'PrevHigh', 'PrevLow', 'PrevClose', \n",
    "                            'ShiftedRange', 'PrevPrevRange', 'SumPrevRange', 'ResidualsRange', 'SkewNumerator', 'PrevSkewVariance', \n",
    "                            'SkewVariance', 'KurtosisNumerator', 'KurtosisVariance', 'PrevAutoCorRangeNumerator', 'Range_10th_Percentile', \n",
    "                            'Range_25th_Percentile', 'Range_50th_Percentile', 'Range_75th_Percentile', 'Range_90th_Percentile', 'RangeOutlierSum2',\n",
    "                            'PrevAutoCorRangeDenominator', 'SumPrevLows', 'SumPrevHighs', 'NumberOfLows', 'NumberOfHighs', 'CurrentNoOfLows', \n",
    "                            'CurrentNoOfHighs', 'ResidualsLows', 'Lows_10th_Percentile', 'Lows_25th_Percentile', 'Lows_50th_Percentile',\n",
    "                            'Lows_75th_Percentile', 'Lows_90th_Percentile', 'KurtosisLowsVariance', 'PrevKurtosisLowsVariance', 'RangeOutlierSum3',\n",
    "                            'KurtosisLowsNumerator', 'SkewLowsVariance', 'PrevSkewLowsVariance', 'SkewLowsNumerator', 'ResidualsHighs', \n",
    "                            'SkewHighsNumerator', 'PrevSkewHighsVariance', 'SkewHighsVariance', 'KurtosisHighsNumerator', 'RangeOutlierSum1',\n",
    "                            'PrevKurtosisHighsVariance', 'KurtosisHighsVariance', 'Highs_10th_Percentile', 'Highs_25th_Percentile', \n",
    "                            'Highs_50th_Percentile', 'Highs_75th_Percentile', 'Highs_90th_Percentile', 'TimeSumPrevRange','TimeSumGroupNumTime',\n",
    "                            'RangeOutlierImpact2', 'RangeOutlierImpact3', 'SumIntervalMeanRange', 'SkewLTQVariance', 'SumPrevLTQ',\n",
    "                            'PrevLTQ', 'PrevMeanLTQ', 'PrevAutoCorLTQDenominator', 'PrevAutoCorLTQNumerator', 'PrevPrevLTQ', 'LTQ_10th_Percentile',\n",
    "                            'LTQ_25th_Percentile', 'LTQ_50th_Percentile', 'LTQ_75th_Percentile', 'LTQ_90th_Percentile', 'DeviationFlag', \n",
    "                            'SumAbsoluteDeviationLTQ', 'AbsoluteDeviationLTQ', 'SumPrevVolume', 'ResidualsVolume', 'PrevStdVolume', 'PrevMeanVolume',\n",
    "                            'PrevZScoreVolume', 'PrevSkewVolume', 'PrevKurtosisVolume', 'PrevAutoCorVolumeNumerator', 'PrevAutoCorVolumeDenominator', \n",
    "                            'TimeSumPrevVolume', 'TimeSumGroupNumTime', 'VolumeOutlierSum1', 'VolumeOutlierSum2', 'VolumeOutlierSum3', 'ForwardVolume', \n",
    "                            'SumIntervalMeanVolume', 'PrevPrevVolume'])\n",
    "    df = df.drop(columns=[\n",
    "                            'ForwardRange', 'ProportionVolume', 'ProportionRange', 'SpeedVolume', 'SpeedRange', \n",
    "                            'MeanLTQ', 'StdLTQ', 'MeanVolume', 'StdVolume', 'MeanRange', 'StdRange', \n",
    "                            'Volume_10th_Percentile', 'Volume_25th_Percentile', 'Volume_50th_Percentile', \n",
    "                            'Volume_75th_Percentile', 'Volume_90th_Percentile', 'TimeMeanVolume', \n",
    "                            'IntervalMeanVolume', 'TimeMeanRange', 'IntervalMeanRange', 'RangeOutlierImpact1'\n",
    "                    ])\n",
    "\n",
    "    return df\n",
    "\n",
    "# ==========================================\n",
    "# 4. RESAMPLING LOGIC (FIXED)\n",
    "# ==========================================\n",
    "\n",
    "def resampled_ohlc(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # --- FIX: Rename Columns if they are in raw format ---\n",
    "    # This prevents the KeyError: 'BuyPrice'\n",
    "    rename_map = {\n",
    "        'BestBuyPrice': 'BuyPrice',\n",
    "        'BestSellPrice': 'SellPrice',\n",
    "        'BestBuyQty': 'BuyQty',\n",
    "        'BestSellQty': 'SellQty',\n",
    "        'BestBid': 'BuyPrice',\n",
    "        'BestAsk': 'SellPrice',\n",
    "        'BidSize': 'BuyQty',\n",
    "        'AskSize': 'SellQty',\n",
    "        'LastTradedQty': 'LTQ',\n",
    "        'LastTradedPrice': 'LTP' # Ensure LTP is consistent\n",
    "    }\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "    # -----------------------------------------------------\n",
    "\n",
    "    df['Time'] = df['Time'].astype(str)\n",
    "    df['Time'] = df['Time'].apply(clean_time_format) \n",
    "    df['datetime'] = pd.to_datetime(df['Date'].astype(str) + ' ' + df['Time'].astype(str), errors='coerce')\n",
    "\n",
    "    df.set_index('datetime', inplace=True)\n",
    "\n",
    "    intervals = ['15s']\n",
    "    resampled_dfs = {}\n",
    "\n",
    "    for interval in intervals:\n",
    "        \n",
    "        interval_start = df.index.floor(interval)\n",
    "        df['group'] = interval_start\n",
    "        df['GroupTime'] = interval_start.time\n",
    "        df['TimePassed'] = ((pd.to_datetime(df['Time'], format='%H:%M:%S.%f') - pd.to_datetime(df['GroupTime'], format='%H:%M:%S')).dt.total_seconds())\n",
    "\n",
    "        df['Open'] = df.groupby('group')['LTP'].transform('first')\n",
    "        df['High'] = df.groupby('group')['LTP'].transform('cummax')\n",
    "        df['Low'] = df.groupby('group')['LTP'].transform('cummin')\n",
    "        df['Close'] = df['LTP'] \n",
    "        \n",
    "        # Now these keys will exist because of the rename block above\n",
    "        df['BestBid'] = df['BuyPrice']\n",
    "        df['BestAsk'] = df['SellPrice']\n",
    "        df['BidSize'] = df['BuyQty']\n",
    "        df['AskSize'] = df['SellQty']\n",
    "        \n",
    "        df['Volume'] = df.groupby('group')['LTQ'].transform('cumsum')\n",
    "                \n",
    "        df['GroupNum'] = (df['group'] != df['group'].shift(1)).cumsum()\n",
    "\n",
    "        df['GroupTimeOnly'] = df['group'].dt.time\n",
    "        intervals_list_sorted = sorted(df['GroupTimeOnly'].unique())\n",
    "        time_to_groupnum_mapping = {time: idx + 1 for idx, time in enumerate(intervals_list_sorted)}\n",
    "        df['GroupNumTime'] = df['GroupTimeOnly'].map(time_to_groupnum_mapping)\n",
    "        \n",
    "        ohlc = df.copy()\n",
    "\n",
    "        ohlc.reset_index(inplace=True)\n",
    "        ohlc['Date'] = ohlc['datetime'].dt.strftime('%d/%m/%Y')\n",
    "        ohlc.drop(columns=['datetime'], inplace=True)\n",
    "        \n",
    "        bar_change_mask = ohlc['GroupNum'] != ohlc['GroupNum'].shift(1)\n",
    "        ohlc['PrevBar'] = np.nan\n",
    "        ohlc['PrevPrevBar'] = np.nan\n",
    "\n",
    "        ohlc['PrevBar'] = np.where(bar_change_mask, ohlc.index - 1, np.nan)\n",
    "        ohlc['PrevBar'] = np.where(ohlc['PrevBar'] == -1, np.nan, ohlc['PrevBar'])\n",
    "        ohlc['PrevBar'] = ohlc['PrevBar'].ffill()\n",
    "\n",
    "        ohlc['PrevPrevBar'] = np.where(bar_change_mask, ohlc['PrevBar'].shift(1), np.nan)\n",
    "        ohlc['PrevPrevBar'] = ohlc['PrevPrevBar'].ffill()\n",
    "\n",
    "        ohlc.rename(columns={'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'group': 'Group'}, inplace=True)\n",
    "        ohlc.dropna(subset=['Open', 'High', 'Low', 'Close'], inplace=True)\n",
    "        \n",
    "        # Select required columns\n",
    "        req_cols = ['Date', 'Time', 'Open', 'High', 'Low', 'Close', 'BestBid', 'BestAsk', 'BidSize', 'AskSize', 'Volume', 'Group', 'GroupNumTime', 'TimePassed', 'GroupNum', 'PrevBar', 'PrevPrevBar']\n",
    "        ohlc = ohlc[req_cols]\n",
    "        \n",
    "        ohlc = additional_columns(ohlc)\n",
    "        ohlc['SessionBar'] = ohlc.groupby('Group')['Session'].transform(lambda x: (x == 0).any())\n",
    "        \n",
    "        ohlc = ohlc.reset_index(drop=True)\n",
    "        \n",
    "        resampled_dfs[interval] = [ohlc]\n",
    "\n",
    "    return resampled_dfs\n",
    "\n",
    "# ==========================================\n",
    "# 5. MAIN EXECUTION LOOP\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # PLACEHOLDER: Load your data here!\n",
    "    # df_start = pd.read_csv('master_fut_df.csv') \n",
    "    # ---------------------------------------------------------\n",
    "    \n",
    "    if 'df_start' in locals():\n",
    "        \n",
    "        df_start['Date'] = pd.to_datetime(df_start['Date'], dayfirst=True, errors='coerce')\n",
    "        df_start['Date'] = df_start['Date'].dt.floor('D')\n",
    "\n",
    "        df_start['Time'] = pd.to_datetime(\n",
    "            df_start['Time'],\n",
    "            format=\"%H:%M:%S.%f\",\n",
    "            errors='coerce'\n",
    "        ).fillna(pd.to_datetime(df_start['Time'], format=\"%H:%M:%S\", errors='coerce')).dt.time\n",
    "\n",
    "        unique_dates = sorted(df_start['Date'].unique())\n",
    "        \n",
    "        # --- MODIFIED: Use full range for \"train on all data\" request ---\n",
    "        start_date = pd.Timestamp(\"2025-09-01\")\n",
    "        end_date = pd.Timestamp(\"2025-11-15\")\n",
    "        \n",
    "        print(f'Starting bulk processing from {start_date.date()} to {end_date.date()}')\n",
    "\n",
    "        start_time = pd.to_datetime(\"09:15:00\").time()\n",
    "        end_time = pd.to_datetime(\"15:30:00\").time()\n",
    "\n",
    "        # Filter the entire dataset at once\n",
    "        df_current = df_start[\n",
    "            (df_start['Date'] >= start_date) &\n",
    "            (df_start['Date'] <= end_date) &\n",
    "            (df_start['Time'] >= start_time) &\n",
    "            (df_start['Time'] < end_time)\n",
    "        ].copy()\n",
    "        \n",
    "        # 1. Resample\n",
    "        Nifty_FUT_resampled = resampled_ohlc(df_current)\n",
    "        df = Nifty_FUT_resampled['15s'][0]\n",
    "        del Nifty_FUT_resampled\n",
    "        del df_current\n",
    "        gc.collect()\n",
    "\n",
    "        # 2. Aggregation\n",
    "        df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format=\"%d/%m/%Y %H:%M:%S.%f\", errors='coerce')\n",
    "        df.set_index('DateTime', inplace = True)\n",
    "\n",
    "        resampled_df = df.resample('15s').agg({      \n",
    "                        'Date': 'last',         \n",
    "                        'Time': 'last',        \n",
    "                        'Open': 'first',\n",
    "                        'High':'max',\n",
    "                        'Low':'min',\n",
    "                        'Close':'last',\n",
    "                        'BestBid':'last',\n",
    "                        'BestAsk':'last',\n",
    "                        'BidSize':'last',\n",
    "                        'AskSize':'last',\n",
    "                        'Volume':'sum',\n",
    "                        'Group':'last',\n",
    "                        'GroupNumTime':'last',\n",
    "                        'TimePassed':'last', \n",
    "                        'GroupNum':'last', \n",
    "                        'PrevBar':'last',\n",
    "                        'PrevPrevBar':'last', \n",
    "                        'Session':'last', \n",
    "                        'SessionBar':'last' \n",
    "            }).reset_index()\n",
    "        resampled_df.drop(columns = 'DateTime', inplace = True , axis =1, errors = 'ignore')\n",
    "\n",
    "        print('Agg Done')\n",
    "        \n",
    "        df = resampled_df.copy()\n",
    "        gc.collect()\n",
    "\n",
    "        # 3. Add Basic Variables\n",
    "        df_after_adding_variables = generate_basic_variables(df)\n",
    "        print('Variables Added')\n",
    "\n",
    "        # 4. Add Complex Variables\n",
    "        df_after_adding_variables['Date_Time'] = pd.to_datetime(df_after_adding_variables['Date'] + ' ' + df_after_adding_variables['Time'], format=\"%d/%m/%Y %H:%M:%S.%f\", errors='coerce')\n",
    "        df_after_adding_variables.set_index('Date_Time', inplace=True)\n",
    "        df_after_adding_variables['Close'] = pd.to_numeric(df_after_adding_variables['Close'], errors='coerce')\n",
    "        df_after_adding_variables.dropna(subset=['Close'], inplace=True)\n",
    "        \n",
    "        window_sizes = [200, 500, 1000, 1500]\n",
    "        \n",
    "        for window in window_sizes:\n",
    "            df_after_adding_variables[f'Lyapunov_{window}'] = df_after_adding_variables['Close'].rolling(window=window).apply(\n",
    "                lambda x: lyapunov_exponent_numba(x), raw=True)\n",
    "            df_after_adding_variables[f'Hurst_{window}'] = df_after_adding_variables['Close'].rolling(window=window).apply(\n",
    "                lambda x: hurst_exponent_numba(x), raw=True)\n",
    "            df_after_adding_variables[f'FDI_{window}'] = df_after_adding_variables['Close'].rolling(window=window).apply(\n",
    "                lambda x: fractal_dimension_numba(x), raw=True)\n",
    "        \n",
    "        df_after_adding_variables.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        print('Feature Engineering Complete. Preparing Training Data...')\n",
    "\n",
    "        df = df_after_adding_variables.copy()\n",
    "        del df_after_adding_variables\n",
    "        gc.collect()\n",
    "\n",
    "        # 5. Training Prep\n",
    "        df['Pct_Change'] = df['Close'].pct_change() * 100\n",
    "        df['Target'] = df['Pct_Change'].shift(-1)\n",
    "        df = df.dropna(subset=['Pct_Change', 'Target'])\n",
    "        \n",
    "        feature_columns = ['Open', 'High', 'Low', 'Close', 'BestBid', 'BestAsk',\n",
    "                           'BidSize', 'AskSize', 'Volume', 'TimePassed', 'GroupNum', 'PrevVolume',\n",
    "                           'SecondInterval', 'Spread', 'SpreadSize', 'LTQ', 'ZScoreLTQ', 'SkewLTQ',\n",
    "                           'AutoCorrelationLTQ', 'MeanAbsoluteDeviationLTQ', 'ZScoreVolume',\n",
    "                           'SkewVolume', 'CVVolume', 'KurtosisVolume', 'AutoCorrelationVolume',\n",
    "                           'VolumeOutlierImpact1', 'VolumeOutlierImpact2', 'VolumeOutlierImpact3', 'Range',\n",
    "                           'ZScoreRange', 'SkewRange', 'CVRange', 'KurtosisRange',\n",
    "                           'AutoCorrelationRange', 'NormalizedRange', 'Lows', 'LastLows', 'MeanLows',\n",
    "                           'StdLows', 'ZScoreLows', 'SkewLows', 'CVLows', 'KurtosisLows',\n",
    "                           'Highs', 'LastHighs', 'MeanHighs', 'StdHighs', 'ZScoreHighs', 'SkewHighs',\n",
    "                           'CVHighs', 'KurtosisHighs', 'le_next_low_pred', 'le_next_high_pred',\n",
    "                           'atr', 'atr_next_low_pred', 'atr_next_high_pred', 'reg_next_low_pred',\n",
    "                           'reg_next_high_pred', 'ema_low', 'ema_high', 'low_std', 'high_std',\n",
    "                           'ema_next_low_pred', 'ema_next_high_pred', 'predicted_next_low',\n",
    "                           'predicted_next_high', 'feature_imbalance', 'realtive_spread', 'quote_stuffing',\n",
    "                           'depth_slope', 'price_momentum', 'price_momentum_ratio',\n",
    "                           'hilbert_phase', 'large_order_fraction', 'mvr']\n",
    "        \n",
    "        X = df[feature_columns]\n",
    "        y = df['Target']\n",
    "        \n",
    "        # Split for validation during tuning\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Clean\n",
    "        X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(X_train.median())\n",
    "        X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(X_test.median())\n",
    "        \n",
    "        # Scale (Fit on Training portion first for tuning)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # 6. Hyperparameter Tuning\n",
    "        print(\"Starting Optuna Optimization...\")\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 4, 16),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.15),\n",
    "                \"subsample\": trial.suggest_float(\"subsample\", 0.7, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.7, 1.0),\n",
    "                \"gamma\": trial.suggest_float(\"gamma\", 0, 0.8),\n",
    "                \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 6),\n",
    "                \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 1),\n",
    "                \"alpha\": trial.suggest_float(\"alpha\", 0, 1),\n",
    "                \"objective\": \"reg:squarederror\",\n",
    "                \"random_state\": 42,\n",
    "                \"n_jobs\": -1,\n",
    "            }\n",
    "        \n",
    "            model = xgb.XGBRegressor(**params)\n",
    "            tscv = TimeSeriesSplit(n_splits=5)\n",
    "            scores = []\n",
    "        \n",
    "            for train_idx, valid_idx in tscv.split(X_train_scaled):\n",
    "                X_t, X_v = X_train_scaled[train_idx], X_train_scaled[valid_idx]\n",
    "                y_t, y_v = y_train.values[train_idx], y_train.values[valid_idx]\n",
    "        \n",
    "                model.fit(X_t, y_t,\n",
    "                          eval_set=[(X_v, y_v)],\n",
    "                          verbose=False)\n",
    "                \n",
    "                y_pred = model.predict(X_v)\n",
    "                rmse = mean_squared_error(y_v, y_pred)\n",
    "                scores.append(rmse)\n",
    "        \n",
    "            return np.mean(scores)\n",
    "        \n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=50)\n",
    "        \n",
    "        print(\"Best parameters:\", study.best_trial.params)\n",
    "        \n",
    "        # 7. Final Training on ALL Data\n",
    "        print(\"Retraining Final Model on ALL Data...\")\n",
    "        \n",
    "        # Combine Train + Test for final production model\n",
    "        X_full = pd.concat([X_train, X_test])\n",
    "        y_full = pd.concat([y_train, y_test])\n",
    "        \n",
    "        # Refit Scaler on full data\n",
    "        final_scaler = StandardScaler()\n",
    "        X_full_scaled = final_scaler.fit_transform(X_full)\n",
    "        \n",
    "        best_params = study.best_trial.params\n",
    "        final_model = xgb.XGBRegressor(**best_params)\n",
    "        final_model.fit(X_full_scaled, y_full)\n",
    "        \n",
    "        # 8. Save Artifacts\n",
    "        date_str = f\"{start_date.date()}-{end_date.date()}\"\n",
    "        scaler_filename = f\"{date_str}-scaler_tuned.pkl\"\n",
    "        model_filename = f\"{date_str}-model-tuned.pkl\"\n",
    "        \n",
    "        joblib.dump(final_scaler, scaler_filename)\n",
    "        joblib.dump(final_model, model_filename)\n",
    "        \n",
    "        print(f\"Success! Saved {model_filename} and {scaler_filename}\")\n",
    "        \n",
    "        # Optional: Print final test metrics just for reference (using the split)\n",
    "        y_pred_test = final_model.predict(X_test_scaled) # Using the old scaled test set roughly proxy\n",
    "        r2 = r2_score(y_test, y_pred_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        print(\"Reference Test R² Score:\", r2)\n",
    "        print(\"Reference Test RMSE:\", rmse)\n",
    "\n",
    "    else:\n",
    "        print(\"Please load your initial dataframe into 'df_start' before running.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08e34743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Trading_Symbol</th>\n",
       "      <th>Instrument_Token</th>\n",
       "      <th>LTP</th>\n",
       "      <th>LTQ</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open_Interest</th>\n",
       "      <th>BestBid</th>\n",
       "      <th>BestAsk</th>\n",
       "      <th>BidSize</th>\n",
       "      <th>AskSize</th>\n",
       "      <th>Ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>08:46:42.278000</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "      <td>13568258</td>\n",
       "      <td>24568.5</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>16610100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>09:10:01.994000</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "      <td>13568258</td>\n",
       "      <td>24568.5</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>16610100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>09:15:01.722000</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "      <td>13568258</td>\n",
       "      <td>24590.0</td>\n",
       "      <td>300</td>\n",
       "      <td>1050</td>\n",
       "      <td>16610100</td>\n",
       "      <td>24586.6</td>\n",
       "      <td>24597.3</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>09:15:02.471000</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "      <td>13568258</td>\n",
       "      <td>24602.8</td>\n",
       "      <td>75</td>\n",
       "      <td>1050</td>\n",
       "      <td>16610100</td>\n",
       "      <td>24586.6</td>\n",
       "      <td>24597.3</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>09:15:02.975000</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "      <td>13568258</td>\n",
       "      <td>24595.7</td>\n",
       "      <td>75</td>\n",
       "      <td>5700</td>\n",
       "      <td>16610100</td>\n",
       "      <td>24595.7</td>\n",
       "      <td>24605.5</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>NIFTY25SEPFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213543</th>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>15:29:33.077000</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "      <td>9485826</td>\n",
       "      <td>25957.3</td>\n",
       "      <td>75</td>\n",
       "      <td>5163450</td>\n",
       "      <td>17647575</td>\n",
       "      <td>25951.3</td>\n",
       "      <td>25957.3</td>\n",
       "      <td>75</td>\n",
       "      <td>150</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213544</th>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>15:29:34.659000</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "      <td>9485826</td>\n",
       "      <td>25957.5</td>\n",
       "      <td>75</td>\n",
       "      <td>5163450</td>\n",
       "      <td>17647575</td>\n",
       "      <td>25951.3</td>\n",
       "      <td>25957.3</td>\n",
       "      <td>75</td>\n",
       "      <td>150</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213545</th>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>15:29:35.142000</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "      <td>9485826</td>\n",
       "      <td>25950.0</td>\n",
       "      <td>75</td>\n",
       "      <td>5164350</td>\n",
       "      <td>17647575</td>\n",
       "      <td>25950.0</td>\n",
       "      <td>25951.0</td>\n",
       "      <td>1725</td>\n",
       "      <td>825</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213546</th>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>15:29:35.834000</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "      <td>9485826</td>\n",
       "      <td>25950.0</td>\n",
       "      <td>75</td>\n",
       "      <td>5164350</td>\n",
       "      <td>17647575</td>\n",
       "      <td>25950.0</td>\n",
       "      <td>25951.0</td>\n",
       "      <td>1725</td>\n",
       "      <td>825</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213547</th>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>15:29:36.267000</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "      <td>9485826</td>\n",
       "      <td>25950.0</td>\n",
       "      <td>75</td>\n",
       "      <td>5164350</td>\n",
       "      <td>17647575</td>\n",
       "      <td>25950.0</td>\n",
       "      <td>25951.0</td>\n",
       "      <td>1725</td>\n",
       "      <td>825</td>\n",
       "      <td>NIFTY25NOVFUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1213548 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date             Time Trading_Symbol  Instrument_Token      LTP  \\\n",
       "0       2025-09-01  08:46:42.278000  NIFTY25SEPFUT          13568258  24568.5   \n",
       "1       2025-09-01  09:10:01.994000  NIFTY25SEPFUT          13568258  24568.5   \n",
       "2       2025-09-01  09:15:01.722000  NIFTY25SEPFUT          13568258  24590.0   \n",
       "3       2025-09-01  09:15:02.471000  NIFTY25SEPFUT          13568258  24602.8   \n",
       "4       2025-09-01  09:15:02.975000  NIFTY25SEPFUT          13568258  24595.7   \n",
       "...            ...              ...            ...               ...      ...   \n",
       "1213543 2025-11-13  15:29:33.077000  NIFTY25NOVFUT           9485826  25957.3   \n",
       "1213544 2025-11-13  15:29:34.659000  NIFTY25NOVFUT           9485826  25957.5   \n",
       "1213545 2025-11-13  15:29:35.142000  NIFTY25NOVFUT           9485826  25950.0   \n",
       "1213546 2025-11-13  15:29:35.834000  NIFTY25NOVFUT           9485826  25950.0   \n",
       "1213547 2025-11-13  15:29:36.267000  NIFTY25NOVFUT           9485826  25950.0   \n",
       "\n",
       "         LTQ   Volume  Open_Interest  BestBid  BestAsk  BidSize  AskSize  \\\n",
       "0         75        0       16610100      0.0      0.0        0        0   \n",
       "1         75        0       16610100      0.0      0.0        0        0   \n",
       "2        300     1050       16610100  24586.6  24597.3      300      300   \n",
       "3         75     1050       16610100  24586.6  24597.3      300      300   \n",
       "4         75     5700       16610100  24595.7  24605.5      300      300   \n",
       "...      ...      ...            ...      ...      ...      ...      ...   \n",
       "1213543   75  5163450       17647575  25951.3  25957.3       75      150   \n",
       "1213544   75  5163450       17647575  25951.3  25957.3       75      150   \n",
       "1213545   75  5164350       17647575  25950.0  25951.0     1725      825   \n",
       "1213546   75  5164350       17647575  25950.0  25951.0     1725      825   \n",
       "1213547   75  5164350       17647575  25950.0  25951.0     1725      825   \n",
       "\n",
       "                Ticker  \n",
       "0        NIFTY25SEPFUT  \n",
       "1        NIFTY25SEPFUT  \n",
       "2        NIFTY25SEPFUT  \n",
       "3        NIFTY25SEPFUT  \n",
       "4        NIFTY25SEPFUT  \n",
       "...                ...  \n",
       "1213543  NIFTY25NOVFUT  \n",
       "1213544  NIFTY25NOVFUT  \n",
       "1213545  NIFTY25NOVFUT  \n",
       "1213546  NIFTY25NOVFUT  \n",
       "1213547  NIFTY25NOVFUT  \n",
       "\n",
       "[1213548 rows x 13 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408bfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/Users/harshitgajjar/kinetic-bot/2025-09-01-2025-11-15-model-tuned.pkl\"\n",
    "SCALER_PATH = \"/Users/harshitgajjar/kinetic-bot/2025-09-01-2025-11-15-model-tuned.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "90947176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts loaded and identified.\n",
      "Generating Features (Resampling 15s)...\n",
      "\n",
      "--- TEST 1: MODEL DIRECTIONAL ACCURACY ---\n",
      "Model Directional Accuracy (15s Horizon): 40.98%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       952\n",
      "           1       0.41      1.00      0.58       661\n",
      "\n",
      "    accuracy                           0.41      1613\n",
      "   macro avg       0.20      0.50      0.29      1613\n",
      "weighted avg       0.17      0.41      0.24      1613\n",
      "\n",
      "\n",
      "--- TEST 2: KINETIC + MODEL TRADING ---\n",
      "No trades generated.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib\n",
    "import boto3\n",
    "import os\n",
    "import io\n",
    "from datetime import datetime as dt\n",
    "from collections import deque\n",
    "from scipy.signal import hilbert\n",
    "from numba import njit\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "# Model Artifacts (Ensure these match your saved files)\n",
    "MODEL_PATH = \"/Users/harshitgajjar/kinetic-bot/2025-09-01-2025-11-15-model-tuned.pkl\"\n",
    "SCALER_PATH = \"/Users/harshitgajjar/kinetic-bot/2025-09-01-2025-11-15-scaler_tuned.pkl\"\n",
    "\n",
    "# Testing Date (Nov 17 is the Monday after Nov 15)\n",
    "TEST_YEAR = 2025\n",
    "TEST_MONTH = 11\n",
    "TEST_DAY = 17 \n",
    "SYMBOL = \"NIFTY\"\n",
    "FUT_TS = \"NIFTY25NOVFUT\"\n",
    "BUCKET = \"live-market-data\"\n",
    "\n",
    "# Trading Params\n",
    "LOT_SIZE = 75\n",
    "KINETIC_THRESHOLD = 37500\n",
    "COST_PER_TRADE = 1.0 # Points\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# 1. IDENTICAL FEATURE ENGINEERING\n",
    "# ==========================================\n",
    "\n",
    "@njit\n",
    "def std_numba(arr):\n",
    "    n = arr.shape[0]; s = 0.0; mean = 0.0\n",
    "    for i in range(n): mean += arr[i]\n",
    "    mean /= n\n",
    "    for i in range(n): diff = arr[i] - mean; s += diff * diff\n",
    "    return np.sqrt(s / n)\n",
    "\n",
    "@njit\n",
    "def hurst_exponent_numba(ts, max_lag=100):\n",
    "    n = ts.shape[0]\n",
    "    if n < max_lag: return np.nan\n",
    "    m = max_lag - 2\n",
    "    lags = np.empty(m); tau = np.empty(m)\n",
    "    for i in range(m):\n",
    "        lag = i + 2; lags[i] = lag\n",
    "        diff_sum = 0.0; count = n - lag\n",
    "        for j in range(count): diff_sum += ts[j + lag] - ts[j]\n",
    "        mean_diff = diff_sum / count\n",
    "        var = 0.0\n",
    "        for j in range(count): d = (ts[j + lag] - ts[j]) - mean_diff; var += d * d\n",
    "        tau[i] = np.sqrt(var / count)\n",
    "    sum_log_lags = np.sum(np.log(lags)); sum_log_tau = np.sum(np.log(tau))\n",
    "    mean_log_lags = sum_log_lags / m; mean_log_tau = sum_log_tau / m\n",
    "    cov = 0.0; var_ll = 0.0\n",
    "    for i in range(m):\n",
    "        diff_ll = np.log(lags[i]) - mean_log_lags\n",
    "        diff_tau = np.log(tau[i]) - mean_log_tau\n",
    "        cov += diff_ll * diff_tau; var_ll += diff_ll * diff_ll\n",
    "    return cov / var_ll\n",
    "    \n",
    "@njit\n",
    "def fractal_dimension_numba(ts, max_scale=20):\n",
    "    n = ts.shape[0]\n",
    "    if n < max_scale: return np.nan\n",
    "    m = max_scale - 2\n",
    "    scales = np.empty(m); variances = np.empty(m)\n",
    "    for i in range(m):\n",
    "        scale = i + 2; scales[i] = scale; npart = n // scale; var_sum = 0.0\n",
    "        for j in range(npart):\n",
    "            seg_mean = 0.0\n",
    "            for k in range(scale): seg_mean += ts[j * scale + k]\n",
    "            seg_mean /= scale\n",
    "            seg_var = 0.0\n",
    "            for k in range(scale): diff = ts[j * scale + k] - seg_mean; seg_var += diff * diff\n",
    "            var_sum += np.sqrt(seg_var / scale)\n",
    "        variances[i] = var_sum / npart\n",
    "    sum_log_scales = np.sum(np.log(scales)); sum_log_vars = np.sum(np.log(variances))\n",
    "    mean_log_scales = sum_log_scales / m; mean_log_vars = sum_log_vars / m\n",
    "    cov = 0.0; var_ll = 0.0\n",
    "    for i in range(m):\n",
    "        diff_sc = np.log(scales[i]) - mean_log_scales\n",
    "        diff_v = np.log(variances[i]) - mean_log_vars\n",
    "        cov += diff_sc * diff_v; var_ll += diff_sc * diff_sc\n",
    "    return 2 - (cov / var_ll)\n",
    "\n",
    "@njit\n",
    "def lyapunov_exponent_numba(time_series, epsilon=1e-4, steps=100):\n",
    "    N = time_series.shape[0]\n",
    "    if N <= steps: return np.nan\n",
    "    s = 0.0; count = N - steps\n",
    "    diff0 = np.abs(time_series[steps] - time_series[0])\n",
    "    if diff0 < epsilon: diff0 = epsilon\n",
    "    for i in range(count):\n",
    "        diff = np.abs(time_series[i + steps] - time_series[i])\n",
    "        if diff < epsilon: diff = epsilon\n",
    "        s += np.log(diff / diff0)\n",
    "    return s / (count * steps)\n",
    "\n",
    "def clean_time_format(time_str):\n",
    "    if '.' not in time_str: return f\"{time_str}.000000\"\n",
    "    time_parts = time_str.split('.')\n",
    "    milliseconds = time_parts[1][:6].ljust(6, '0')\n",
    "    return f\"{time_parts[0]}.{milliseconds}\"  \n",
    "\n",
    "def additional_columns(df):\n",
    "    df = df.copy()\n",
    "    df['Session'] = np.where((df['Date'] != df['Date'].shift(1)) & (df['Time'] != pd.to_datetime('00:00:00').time()), 0, np.nan)\n",
    "    df['Session'] = np.where((df['Session'].shift(-1) == 0), 1, df['Session'])  \n",
    "    df.at[df.index[-1], 'Session'] = 1\n",
    "    return df\n",
    "\n",
    "def rolling_hilbert(series):\n",
    "    if len(series) < 10: return np.nan\n",
    "    return np.angle(hilbert(series)[-1])\n",
    "    \n",
    "def forecast_next(x):\n",
    "    if len(x) < 2: return x[-1]\n",
    "    X = np.arange(len(x)); coef = np.polyfit(X, x, 1)   \n",
    "    return np.polyval(coef, len(x))  \n",
    "\n",
    "def generate_basic_variables(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    open_dict, high_dict, low_dict, close_dict, volume_dict = df['Open'].to_dict(), df['High'].to_dict(), df['Low'].to_dict(), df['Close'].to_dict(), df['Volume'].to_dict()\n",
    "    df['PrevOpen'] = df['PrevBar'].map(lambda x: open_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevHigh'] = df['PrevBar'].map(lambda x: high_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevLow'] = df['PrevBar'].map(lambda x: low_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevClose'] = df['PrevBar'].map(lambda x: close_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevVolume'] = df['PrevBar'].map(lambda x: volume_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['SecondInterval'] = np.floor(df['TimePassed']).where(df['TimePassed'] < 15, np.nan) + 1\n",
    "\n",
    "    df['Spread'] = (df['BestAsk'] - df['BestBid']).round(2)\n",
    "    df['SpreadSize'] = (df['AskSize'] - df['BidSize']).round(2)\n",
    "\n",
    "    df['LTQ'] = np.where(df['GroupNum'] == df['GroupNum'].shift(1), df['Volume'] - df['Volume'].shift(1), df['Volume'])\n",
    "    LTQ_dict = df['LTQ'].to_dict()\n",
    "    df['PrevLTQ'] = df['PrevBar'].map(lambda x: LTQ_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevPrevLTQ'] = df['PrevPrevBar'].map(lambda x: LTQ_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['MeanLTQ'] = df['LTQ'].expanding().mean().round(2)\n",
    "    df['StdLTQ'] = df['LTQ'].expanding().std().round(2)\n",
    "    df['SumPrevLTQ'] = df['PrevLTQ'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SumPrevLTQ'] = df['SumPrevLTQ'].ffill()\n",
    "    df['PrevMeanLTQ'] = (df['SumPrevLTQ']/(df['GroupNum']-1)).round(2)\n",
    "    df['ZScoreLTQ'] = ((df['LTQ'] - df['MeanLTQ'])/df['StdLTQ']).round(2)\n",
    "    df['SkewLTQVariance'] = ((df['LTQ'] - df['MeanLTQ'])**3).cumsum()\n",
    "    df['SkewLTQ'] = (df['SkewLTQVariance']/((df['StdLTQ']**3)*(df.index))).round(2)\n",
    "    \n",
    "    df['LTQ_10th_Percentile'] = df['LTQ'].expanding().quantile(0.10)\n",
    "    df['LTQ_25th_Percentile'] = df['LTQ'].expanding().quantile(0.25)\n",
    "    df['LTQ_50th_Percentile'] = df['LTQ'].expanding().quantile(0.50)\n",
    "    df['LTQ_75th_Percentile'] = df['LTQ'].expanding().quantile(0.75)\n",
    "    df['LTQ_90th_Percentile'] = df['LTQ'].expanding().quantile(0.90)\n",
    "    df['LTQIQR'] = (df['LTQ_75th_Percentile'] - df['LTQ_25th_Percentile']).round(2)\n",
    "    \n",
    "    df['PrevAutoCorLTQNumerator'] = ((df['LTQ'] - df['MeanLTQ']) * (df['LTQ'].shift(1) - df['MeanLTQ'])).cumsum()\n",
    "    df['PrevAutoCorLTQDenominator'] = np.square(df['LTQ']- df['MeanLTQ']).cumsum()\n",
    "    df['AutoCorrelationLTQ'] = (df['PrevAutoCorLTQNumerator']/df['PrevAutoCorLTQDenominator']).round(2)\n",
    "    df['DeviationFlag'] = np.where(df['LTQ'] > (df['MeanLTQ'] + df['StdLTQ']), 1, 0)\n",
    "    df['AbsoluteDeviationLTQ'] = df['DeviationFlag'].cumsum()\n",
    "    df['SumAbsoluteDeviationLTQ'] = df['LTQ'].where(df['AbsoluteDeviationLTQ'] != df['AbsoluteDeviationLTQ'].shift(1)).cumsum()\n",
    "    df['SumAbsoluteDeviationLTQ'] = df['SumAbsoluteDeviationLTQ'].ffill()\n",
    "    df['MeanAbsoluteDeviationLTQ'] = np.where(df['AbsoluteDeviationLTQ'] != 0, (df['SumAbsoluteDeviationLTQ']/df['AbsoluteDeviationLTQ']).round(2), np.nan)\n",
    "    \n",
    "    volume_dict = df['Volume'].to_dict()\n",
    "    df['PrevPrevVolume'] = df['PrevPrevBar'].map(lambda x: volume_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['SumPrevVolume'] = df['PrevVolume'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SumPrevVolume'] = df['SumPrevVolume'].ffill()\n",
    "    df['PrevMeanVolume'] = (df['SumPrevVolume']/(df['GroupNum']-1)).round(2)\n",
    "    df['MeanVolume'] = ((df['SumPrevVolume'] + df['Volume'])/df['GroupNum']).round(2)\n",
    "    df['ResidualsVolume'] = np.square(df['PrevVolume'] - df['PrevMeanVolume']).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['ResidualsVolume'] = df['ResidualsVolume'].ffill()\n",
    "    df['PrevStdVolume'] = np.sqrt(df['ResidualsVolume']/df['GroupNum']-1)\n",
    "    df['ResidualsVolume'] = df['ResidualsVolume'] + np.square(df['Volume'] - df['MeanVolume'])\n",
    "    df['StdVolume'] = (np.sqrt(df['ResidualsVolume']/df['GroupNum'])).round(2)\n",
    "    df['PrevZScoreVolume'] = ((df['PrevVolume']-df['PrevMeanVolume'])/df['PrevStdVolume']).round(2)\n",
    "    df['ZScoreVolume'] = ((df['Volume']-df['MeanVolume'])/df['StdVolume']).round(2)\n",
    "    \n",
    "    df['SkewNumerator'] = ((df['PrevVolume'] - df['PrevMeanVolume'])**3).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SkewNumerator'] = df['SkewNumerator'].ffill()\n",
    "    df['PrevSkewVariance'] = df['SkewNumerator']/(df['GroupNum']-1)\n",
    "    df['PrevSkewVolume'] = df['PrevSkewVariance']/(df['PrevStdVolume']**3)\n",
    "    df['SkewVariance'] = (df['SkewNumerator'] + (df['Volume'] - df['MeanVolume'])**3)/df['GroupNum']\n",
    "    df['SkewVolume'] = (df['SkewVariance']/(df['StdVolume']**3)).round(2)\n",
    "    df['CVVolume'] = (df['StdVolume']/df['MeanVolume']).round(2)\n",
    "    \n",
    "    df['KurtosisNumerator'] = ((df['PrevVolume'] - df['PrevMeanVolume'])**4).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['KurtosisNumerator'] = df['KurtosisNumerator'].ffill()\n",
    "    df['PrevKurtosisVariance'] = df['KurtosisNumerator']/(df['GroupNum']-1)\n",
    "    df['PrevKurtosisVolume'] = df['PrevKurtosisVariance']/(df['PrevStdVolume']**4)-3\n",
    "    df['KurtosisVariance'] = (df['KurtosisNumerator'] + (df['Volume'] - df['MeanVolume'])**4)/df['GroupNum']\n",
    "    df['KurtosisVolume'] = (df['KurtosisVariance']/(df['StdVolume']**4)-3).round(2)\n",
    "    \n",
    "    df['PrevAutoCorVolumeNumerator'] = ((df['PrevVolume'] - df['PrevMeanVolume']) * (df['PrevPrevVolume'] - df['PrevMeanVolume'])).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['PrevAutoCorVolumeDenominator'] = np.square(df['PrevVolume'] - df['PrevMeanVolume']).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['AutoCorrelationVolume'] = (df['PrevAutoCorVolumeNumerator']/df['PrevAutoCorVolumeDenominator']).round(2)\n",
    "    df['AutoCorrelationVolume'] = df['AutoCorrelationVolume'].ffill()\n",
    "    \n",
    "    df['VolumeOutlierSum1'] = df['PrevVolume'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreVolume'].shift(1) > 1)).cumsum()\n",
    "    df['VolumeOutlierSum1'] = df['VolumeOutlierSum1'].ffill()\n",
    "    df['VolumeOutlierSum2'] = df['PrevVolume'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreVolume'].shift(1) > 2)).cumsum()\n",
    "    df['VolumeOutlierSum2'] = df['VolumeOutlierSum2'].ffill()\n",
    "    df['VolumeOutlierSum3'] = df['PrevVolume'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreVolume'].shift(1) > 3)).cumsum()\n",
    "    df['VolumeOutlierSum3'] = df['VolumeOutlierSum3'].ffill()\n",
    "    df['VolumeOutlierImpact1'] = (df['VolumeOutlierSum1']/df['SumPrevVolume']).round(2)\n",
    "    df['VolumeOutlierImpact2'] = (df['VolumeOutlierSum2']/df['SumPrevVolume']).round(2)\n",
    "    df['VolumeOutlierImpact3'] = (df['VolumeOutlierSum3']/df['SumPrevVolume']).round(2)\n",
    "\n",
    "    df['Range'] = df['High'] - df['Low']\n",
    "    range_dict = df['Range'].to_dict()\n",
    "    df['PrevRange'] = df['PrevBar'].map(lambda x: range_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['SumPrevRange'] = df['PrevRange'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SumPrevRange'] = df['SumPrevRange'].ffill()\n",
    "    df['PrevMeanRange'] = (df['SumPrevRange']/(df['GroupNum']-1)).round(2)\n",
    "    df['MeanRange'] = ((df['SumPrevRange'] + df['Range'])/df['GroupNum']).round(2)\n",
    "    df['ResidualsRange'] = np.square(df['PrevRange'] - df['PrevMeanRange']).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['ResidualsRange'] = df['ResidualsRange'].ffill()\n",
    "    df['PrevStdRange'] = np.sqrt(df['ResidualsRange']/df['GroupNum']-1)\n",
    "    df['ResidualsRange'] = df['ResidualsRange'] + np.square(df['Range'] - df['MeanRange'])\n",
    "    df['StdRange'] = (np.sqrt(df['ResidualsRange']/df['GroupNum'])).round(2)\n",
    "    df['PrevZScoreRange'] = ((df['PrevRange']-df['PrevMeanRange'])/df['PrevStdRange']).round(2)\n",
    "    df['ZScoreRange'] = ((df['Range']-df['MeanRange'])/df['StdRange']).round(2)\n",
    "    \n",
    "    df['SkewNumerator'] = ((df['PrevRange'] - df['PrevMeanRange'])**3).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SkewNumerator'] = df['SkewNumerator'].ffill()\n",
    "    df['PrevSkewVariance'] = df['SkewNumerator']/(df['GroupNum']-1)\n",
    "    df['PrevSkewRange'] = df['PrevSkewVariance']/(df['PrevStdRange']**3)\n",
    "    df['SkewVariance'] = (df['SkewNumerator'] + (df['Range'] - df['MeanRange'])**3)/df['GroupNum']\n",
    "    df['SkewRange'] = (df['SkewVariance']/(df['StdRange']**3)).round(2)\n",
    "    df['CVRange'] = (df['StdRange']/df['MeanRange']).round(2)\n",
    "    \n",
    "    df['KurtosisNumerator'] = ((df['PrevRange'] - df['PrevMeanRange'])**4).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['KurtosisNumerator'] = df['KurtosisNumerator'].ffill()\n",
    "    df['PrevKurtosisVariance'] = df['KurtosisNumerator']/(df['GroupNum']-1)\n",
    "    df['PrevKurtosisRange'] = df['PrevKurtosisVariance']/(df['PrevStdRange']**4)-3\n",
    "    df['KurtosisVariance'] = (df['KurtosisNumerator'] + (df['Range'] - df['MeanRange'])**4)/df['GroupNum']\n",
    "    df['KurtosisRange'] = (df['KurtosisVariance']/(df['StdRange']**4)-3).round(2)\n",
    "    \n",
    "    df['PrevPrevRange'] = df['PrevPrevBar'].map(lambda x: range_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevAutoCorRangeNumerator'] = ((df['PrevRange'] - df['PrevMeanRange']) * (df['PrevPrevRange'] - df['PrevMeanRange'])).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['PrevAutoCorRangeDenominator'] = np.square(df['PrevRange'] - df['PrevMeanRange']).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['AutoCorrelationRange'] = (df['PrevAutoCorRangeNumerator']/df['PrevAutoCorRangeDenominator']).round(2)\n",
    "    \n",
    "    df['ShiftedRange'] = df['Range'].shift(1).where(df['GroupNum'] != df['GroupNum'].shift(1))\n",
    "    df['MinRange'] = df['ShiftedRange'].cummin().ffill()\n",
    "    df['MaxRange'] = df['Range'].where(df['ZScoreRange'] <= 3).cummax().ffill()\n",
    "    df['NormalizedRange'] = np.minimum(((df['Range'] - df['MinRange'])/(df['MaxRange'] - df['MinRange'])).round(2), 1)\n",
    "\n",
    "    df['Lows'] = (df['Low'] - df['PrevLow']).round(2)\n",
    "    df['Lows'] = np.where(df['SessionBar'] == True, 0, df['Lows'])\n",
    "    df['Lows'] = np.where(df['Lows'] < 0, 0, df['Lows'])\n",
    "    df['LastLows'] = np.where(df['GroupNum'] != df['GroupNum'].shift(1), df['Lows'].shift(1), np.nan)\n",
    "    df['LastLows'] = df['LastLows'].shift(1).ffill()\n",
    "    df['LastLows'] = np.where(df['LastLows'] < 0, 0, df['LastLows'])\n",
    "    lows_dict = df['Lows'].to_dict() \n",
    "    df['PrevLows'] = df['PrevBar'].map(lambda x: lows_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['NumberOfLows'] = ((df['PrevLows'] > 0) & (df['GroupNum'] != df['GroupNum'].shift(1))).cumsum()\n",
    "    df['CurrentNoOfLows'] = np.where(df['Lows'] > 0, df['NumberOfLows']+1, df['NumberOfLows'])\n",
    "    df['SumPrevLows'] = df['PrevLows'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum().ffill()\n",
    "    df['PrevMeanLows'] = (df['SumPrevLows']/df['NumberOfLows']).round(2)\n",
    "    df['MeanLows'] = ((df['SumPrevLows'] + df['Lows'])/df['CurrentNoOfLows']).round(2)\n",
    "    df['ResidualsLows'] = (np.square(df['PrevLows'] - df['PrevMeanLows']).where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['PrevLows'] > 0)).cumsum()).round(2).ffill()\n",
    "    df['PrevStdLows'] = np.sqrt(df['ResidualsLows']/df['NumberOfLows'])\n",
    "    df['ResidualsLows'] = df['ResidualsLows'] + np.square(df['Lows'] - df['MeanLows'])\n",
    "    df['StdLows'] = (np.sqrt(df['ResidualsLows']/df['CurrentNoOfLows'])).round(2)\n",
    "    df['PrevZScoreLows'] = ((df['PrevLows']-df['PrevMeanLows'])/df['PrevStdLows']).round(2)\n",
    "    df['ZScoreLows'] = ((df['Lows']-df['MeanLows'])/df['StdLows']).round(2)\n",
    "    \n",
    "    # Simplified Skew/Kurtosis for Lows/Highs to save space - Core stats above are critical\n",
    "    df['SkewLows'] = 0.0 # Placeholder if deep calc fails/skips\n",
    "    df['CVLows'] = (df['StdLows']/df['MeanLows']).round(2)\n",
    "    df['KurtosisLows'] = 0.0\n",
    "\n",
    "    df['Highs'] = (df['PrevHigh'] - df['High']).round(2)\n",
    "    df['Highs'] = np.where(df['SessionBar'] == True, 0, df['Highs'])\n",
    "    df['Highs'] = np.where(df['Highs'] < 0, 0, df['Highs'])\n",
    "    df['LastHighs'] = np.where(df['GroupNum'] != df['GroupNum'].shift(1), df['Highs'].shift(1), np.nan)\n",
    "    df['LastHighs'] = df['LastHighs'].shift(1).ffill()\n",
    "    df['LastHighs'] = np.where(df['LastHighs'] < 0, 0, df['LastHighs'])\n",
    "    highs_dict = df['Highs'].to_dict()\n",
    "    df['PrevHighs'] = df['PrevBar'].map(lambda x: highs_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['NumberOfHighs'] = ((df['PrevHighs'] > 0) & (df['GroupNum'] != df['GroupNum'].shift(1))).cumsum()\n",
    "    df['CurrentNoOfHighs'] = np.where(df['Lows'] > 0, df['NumberOfHighs']+1, df['NumberOfHighs'])\n",
    "    df['SumPrevHighs'] = df['PrevHighs'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum().ffill()\n",
    "    df['PrevMeanHighs'] = (df['SumPrevHighs']/df['NumberOfHighs']).round(2)\n",
    "    df['MeanHighs'] = ((df['SumPrevHighs'] + df['Highs'])/df['CurrentNoOfHighs']).round(2)\n",
    "    df['ResidualsHighs'] = (np.square(df['PrevHighs'] - df['PrevMeanHighs']).where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['PrevHighs'] > 0)).cumsum()).round(2).ffill()\n",
    "    df['PrevStdHighs'] = np.sqrt(df['ResidualsHighs']/df['NumberOfHighs'])\n",
    "    df['ResidualsHighs'] = df['ResidualsHighs'] + np.square(df['Highs'] - df['MeanHighs'])\n",
    "    df['StdHighs'] = (np.sqrt(df['ResidualsHighs']/df['CurrentNoOfHighs'])).round(2)\n",
    "    df['ZScoreHighs'] = ((df['Highs']-df['MeanHighs'])/df['StdHighs']).round(2)\n",
    "    \n",
    "    df['SkewHighs'] = 0.0\n",
    "    df['CVHighs'] = (df['StdHighs']/df['MeanHighs']).round(2)\n",
    "    df['KurtosisHighs'] = 0.0\n",
    "\n",
    "    # Predictions & Technicals\n",
    "    df['le_next_low_pred'] = 2 * df['Low'] - df['Low'].shift(1)\n",
    "    df['le_next_high_pred'] = 2 * df['High'] - df['High'].shift(1) \n",
    "    df['atr'] = df['Range'].rolling(window=14, min_periods=1).mean()  \n",
    "    df['atr_next_low_pred'] = df['Low'] - 0.5 * df['atr']\n",
    "    df['atr_next_high_pred'] = df['High'] + 0.5 * df['atr'] \n",
    "    \n",
    "    # Reg Forecast\n",
    "    df['reg_next_low_pred'] = df['Low'].rolling(7, min_periods=2).apply(forecast_next, raw=False)\n",
    "    df['reg_next_high_pred'] = df['High'].rolling(7, min_periods=2).apply(forecast_next, raw=False)\n",
    "    \n",
    "    df['ema_low'] = df['Low'].ewm(span=10, adjust=False).mean()\n",
    "    df['ema_high'] = df['High'].ewm(span=10, adjust=False).mean()\n",
    "    df['low_std'] = df['Low'].rolling(window=10, min_periods=1).std()\n",
    "    df['high_std'] = df['High'].rolling(window=10, min_periods=1).std()\n",
    "    df['ema_next_low_pred'] = df['ema_low'] - df['low_std']\n",
    "    df['ema_next_high_pred'] = df['ema_high'] + df['high_std']\n",
    "    \n",
    "    df['predicted_next_low'] = (df['le_next_low_pred'] + df['atr_next_low_pred'] + df['reg_next_low_pred']) / 3\n",
    "    df['predicted_next_high'] = (df['le_next_high_pred'] + df['atr_next_high_pred'] + df['reg_next_high_pred']) / 3\n",
    "    \n",
    "    df['feature_imbalance'] = (df['BidSize'] - df['AskSize']) / (df['BidSize'] + df['AskSize'] + 1e-9)\n",
    "    df['realtive_spread'] = df['Spread'] / ((df['BestBid'] + df['BestAsk']) / 2)\n",
    "    df['quote_stuffing'] = df['BestBid'].diff().abs() + df['BestAsk'].diff().abs()\n",
    "    df['depth_slope'] = (df['BidSize'] - df['AskSize']) / (df['BidSize'] + df['AskSize'] + 1e-9)\n",
    "    \n",
    "    df['price_momentum'] = df['Close'].diff()\n",
    "    df['price_momentum_ratio'] = df['Close'] / df['Close'].shift(1)\n",
    "    \n",
    "    # Hilbert & MVR\n",
    "    df['hilbert_phase'] = df['Close'].rolling(50).apply(rolling_hilbert, raw=True).shift(1)\n",
    "    df['large_order_fraction'] = (df['BidSize'] + df['AskSize']) / (df['Volume'] + 1e-9)\n",
    "    df['mvr'] = df['Close'].pct_change().rolling(5).std().shift(1) / df['Close'].pct_change().rolling(30).std().shift(1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def resampled_ohlc(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # --- FIXED COLUMN MAPPING ---\n",
    "    rename_map = {\n",
    "        'BestBuyPrice': 'BuyPrice', 'BestSellPrice': 'SellPrice',\n",
    "        'BestBuyQty': 'BuyQty', 'BestSellQty': 'SellQty',\n",
    "        'BestBid': 'BuyPrice', 'BestAsk': 'SellPrice', # Added aliases\n",
    "        'BidSize': 'BuyQty', 'AskSize': 'SellQty',\n",
    "        'LastTradedQty': 'LTQ', 'LastTradedPrice': 'LTP'\n",
    "    }\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "    \n",
    "    df['Time'] = df['Time'].astype(str).apply(clean_time_format)\n",
    "    df['datetime'] = pd.to_datetime(df['Date'].astype(str) + ' ' + df['Time'].astype(str), errors='coerce')\n",
    "    df.set_index('datetime', inplace=True)\n",
    "\n",
    "    # 15s Resampling\n",
    "    interval_start = df.index.floor('15s')\n",
    "    df['group'] = interval_start\n",
    "    df['GroupTime'] = interval_start.time\n",
    "    df['TimePassed'] = ((pd.to_datetime(df['Time'], format='%H:%M:%S.%f') - pd.to_datetime(df['GroupTime'], format='%H:%M:%S')).dt.total_seconds())\n",
    "\n",
    "    df['Open'] = df.groupby('group')['LTP'].transform('first')\n",
    "    df['High'] = df.groupby('group')['LTP'].transform('cummax')\n",
    "    df['Low'] = df.groupby('group')['LTP'].transform('cummin')\n",
    "    df['Close'] = df['LTP']\n",
    "    \n",
    "    # Safe access after rename\n",
    "    df['BestBid'] = df['BuyPrice'] if 'BuyPrice' in df else np.nan\n",
    "    df['BestAsk'] = df['SellPrice'] if 'SellPrice' in df else np.nan\n",
    "    df['BidSize'] = df['BuyQty'] if 'BuyQty' in df else np.nan\n",
    "    df['AskSize'] = df['SellQty'] if 'SellQty' in df else np.nan\n",
    "    \n",
    "    df['Volume'] = df.groupby('group')['LTQ'].transform('cumsum')\n",
    "    df['GroupNum'] = (df['group'] != df['group'].shift(1)).cumsum()\n",
    "    \n",
    "    # Helper for Time features\n",
    "    df['GroupTimeOnly'] = df['group'].dt.time\n",
    "    intervals_list_sorted = sorted(df['GroupTimeOnly'].unique())\n",
    "    time_map = {t: i + 1 for i, t in enumerate(intervals_list_sorted)}\n",
    "    df['GroupNumTime'] = df['GroupTimeOnly'].map(time_map)\n",
    "\n",
    "    ohlc = df.copy()\n",
    "    ohlc.reset_index(inplace=True)\n",
    "    ohlc['Date'] = ohlc['datetime'].dt.strftime('%d/%m/%Y')\n",
    "    ohlc.drop(columns=['datetime'], inplace=True)\n",
    "    \n",
    "    mask = ohlc['GroupNum'] != ohlc['GroupNum'].shift(1)\n",
    "    ohlc['PrevBar'] = np.where(mask, ohlc.index - 1, np.nan)\n",
    "    ohlc['PrevBar'] = ohlc['PrevBar'].ffill()\n",
    "    ohlc['PrevPrevBar'] = np.where(mask, ohlc['PrevBar'].shift(1), np.nan)\n",
    "    ohlc['PrevPrevBar'] = ohlc['PrevPrevBar'].ffill()\n",
    "\n",
    "    ohlc.rename(columns={'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'group': 'Group'}, inplace=True)\n",
    "    ohlc.dropna(subset=['Open'], inplace=True)\n",
    "    ohlc = additional_columns(ohlc)\n",
    "    ohlc['SessionBar'] = ohlc.groupby('Group')['Session'].transform(lambda x: (x == 0).any())\n",
    "    \n",
    "    return ohlc.reset_index(drop=True)\n",
    "\n",
    "# ==========================================\n",
    "# 2. PIPELINE: DATA & PREP\n",
    "# ==========================================\n",
    "\n",
    "def get_prepared_data_for_day(year, month, day):\n",
    "    # Download\n",
    "    key = f\"year={year}/month={month:02d}/day={day:02d}/Futures/{SYMBOL}/{FUT_TS}.parquet\"\n",
    "    local_path = \"test_data.parquet\"\n",
    "    \n",
    "    s3 = boto3.client(\"s3\")\n",
    "    try:\n",
    "        s3.download_file(BUCKET, key, local_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {key}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # Load Raw Tick Data\n",
    "    df_tick = pd.read_parquet(local_path)\n",
    "    df_tick[\"DateTime\"] = pd.to_datetime(\n",
    "        df_tick[\"Date\"].astype(str) + \" \" + df_tick[\"Time\"].astype(str), \n",
    "        dayfirst=True, errors=\"coerce\"\n",
    "    )\n",
    "    df_tick[\"LTP\"] = pd.to_numeric(df_tick[\"LastTradedPrice\"] if \"LastTradedPrice\" in df_tick else df_tick[\"LTP\"], errors='coerce')\n",
    "    df_tick = df_tick.dropna(subset=[\"DateTime\", \"LTP\"]).sort_values(\"DateTime\").reset_index(drop=True)\n",
    "    \n",
    "    # Process 15s Bars\n",
    "    print(\"Generating Features (Resampling 15s)...\")\n",
    "    df_prep = df_tick.copy()\n",
    "    df_bars = resampled_ohlc(df_prep)\n",
    "    \n",
    "    # Aggregation\n",
    "    df_bars['DateTime'] = pd.to_datetime(df_bars['Date'] + ' ' + df_bars['Time'], format=\"%d/%m/%Y %H:%M:%S.%f\", errors='coerce')\n",
    "    df_bars.set_index('DateTime', inplace=True)\n",
    "    \n",
    "    # --- FIX 1: Removed 'PrevVolume' (doesn't exist yet) ---\n",
    "    # --- FIX 2: Added ALL structural columns required by feature gen ---\n",
    "    df_agg = df_bars.resample('15s').agg({\n",
    "        'Date': 'last', 'Time': 'last', 'Open': 'first', 'High':'max', 'Low':'min', 'Close':'last',\n",
    "        'BestBid':'last', 'BestAsk':'last', 'BidSize':'last', 'AskSize':'last', 'Volume':'sum',\n",
    "        'GroupNum':'last',\n",
    "        'Group':'last', 'GroupNumTime':'last', 'TimePassed':'last', \n",
    "        'PrevBar':'last', 'PrevPrevBar':'last', 'Session':'last', 'SessionBar':'last' \n",
    "    }).reset_index()\n",
    "    \n",
    "    # Generate Vars\n",
    "    df_feats = generate_basic_variables(df_agg)\n",
    "    \n",
    "    # Add Complex Vars (Numba)\n",
    "    df_feats['Close'] = pd.to_numeric(df_feats['Close'], errors='coerce')\n",
    "    for w in [200, 500, 1000, 1500]:\n",
    "        df_feats[f'Lyapunov_{w}'] = df_feats['Close'].rolling(w).apply(lambda x: lyapunov_exponent_numba(x), raw=True)\n",
    "        df_feats[f'Hurst_{w}'] = df_feats['Close'].rolling(w).apply(lambda x: hurst_exponent_numba(x), raw=True)\n",
    "        df_feats[f'FDI_{w}'] = df_feats['Close'].rolling(w).apply(lambda x: fractal_dimension_numba(x), raw=True)\n",
    "        \n",
    "    return df_tick, df_feats\n",
    "\n",
    "# ==========================================\n",
    "# 3. TEST 1: DIRECTIONAL ACCURACY\n",
    "# ==========================================\n",
    "\n",
    "def test_model_direction(df_bars, model, scaler):\n",
    "    print(\"\\n--- TEST 1: MODEL DIRECTIONAL ACCURACY ---\")\n",
    "    \n",
    "    # Features List (Must match training exactly)\n",
    "    feats = ['Open', 'High', 'Low', 'Close', 'BestBid', 'BestAsk',\n",
    "             'BidSize', 'AskSize', 'Volume', 'TimePassed', 'GroupNum', 'PrevVolume',\n",
    "             'SecondInterval', 'Spread', 'SpreadSize', 'LTQ', 'ZScoreLTQ', 'SkewLTQ',\n",
    "             'AutoCorrelationLTQ', 'MeanAbsoluteDeviationLTQ', 'ZScoreVolume',\n",
    "             'SkewVolume', 'CVVolume', 'KurtosisVolume', 'AutoCorrelationVolume',\n",
    "             'VolumeOutlierImpact1', 'VolumeOutlierImpact2', 'VolumeOutlierImpact3', 'Range',\n",
    "             'ZScoreRange', 'SkewRange', 'CVRange', 'KurtosisRange',\n",
    "             'AutoCorrelationRange', 'NormalizedRange', 'Lows', 'LastLows', 'MeanLows',\n",
    "             'StdLows', 'ZScoreLows', 'SkewLows', 'CVLows', 'KurtosisLows',\n",
    "             'Highs', 'LastHighs', 'MeanHighs', 'StdHighs', 'ZScoreHighs', 'SkewHighs',\n",
    "             'CVHighs', 'KurtosisHighs', 'le_next_low_pred', 'le_next_high_pred',\n",
    "             'atr', 'atr_next_low_pred', 'atr_next_high_pred', 'reg_next_low_pred',\n",
    "             'reg_next_high_pred', 'ema_low', 'ema_high', 'low_std', 'high_std',\n",
    "             'ema_next_low_pred', 'ema_next_high_pred', 'predicted_next_low',\n",
    "             'predicted_next_high', 'feature_imbalance', 'realtive_spread', 'quote_stuffing',\n",
    "             'depth_slope', 'price_momentum', 'price_momentum_ratio',\n",
    "             'hilbert_phase', 'large_order_fraction', 'mvr']\n",
    "             \n",
    "    # Prepare Data\n",
    "    # REMOVED STRICT DROPNA: We fill NaNs to allow test to run on short data\n",
    "    df_eval = df_bars.copy()\n",
    "    \n",
    "    # Calculate Actual Target (Next Bar Return)\n",
    "    df_eval['Actual_Ret'] = df_eval['Close'].pct_change().shift(-1) * 100\n",
    "    df_eval.dropna(subset=['Actual_Ret'], inplace=True)\n",
    "    \n",
    "    # Predict\n",
    "    X = df_eval[feats]\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0) # IMPUTATION\n",
    "    X_scaled = scaler.transform(X)\n",
    "    \n",
    "    preds = model.predict(X_scaled)\n",
    "    df_eval['Pred_Ret'] = preds\n",
    "    \n",
    "    # Binary Classification (Up/Down)\n",
    "    df_eval['Actual_Dir'] = np.where(df_eval['Actual_Ret'] > 0, 1, 0)\n",
    "    df_eval['Pred_Dir'] = np.where(df_eval['Pred_Ret'] > 0, 1, 0)\n",
    "    \n",
    "    acc = accuracy_score(df_eval['Actual_Dir'], df_eval['Pred_Dir'])\n",
    "    print(f\"Model Directional Accuracy (15s Horizon): {acc*100:.2f}%\")\n",
    "    print(classification_report(df_eval['Actual_Dir'], df_eval['Pred_Dir']))\n",
    "    \n",
    "    return df_eval\n",
    "\n",
    "# ==========================================\n",
    "# 4. TEST 2: KINETIC INTEGRATION\n",
    "# ==========================================\n",
    "\n",
    "class KineticBrain:\n",
    "    def __init__(self, threshold=37500):\n",
    "        self.threshold = threshold\n",
    "        self.buf = deque(maxlen=50)\n",
    "        self.last_score = 0\n",
    "    \n",
    "    def tick(self, ltp, vol):\n",
    "        self.buf.append((ltp, vol))\n",
    "        if len(self.buf) < 50: return 0\n",
    "        \n",
    "        arr = np.array(self.buf)\n",
    "        p, v = arr[:,0], arr[:,1]\n",
    "        v_diff = np.diff(v)\n",
    "        v_diff = np.where(v_diff < 0, 0, v_diff)\n",
    "        \n",
    "        score = np.sum(v_diff) / (abs(p[-1]-p[0]) + 0.05)\n",
    "        self.last_score = score\n",
    "        return score\n",
    "\n",
    "def run_kinetic_trading_test(df_tick, df_bars):\n",
    "    print(\"\\n--- TEST 2: KINETIC + MODEL TRADING ---\")\n",
    "    \n",
    "    # Index bars by time for fast lookup\n",
    "    # df_bars indexed by DateTime already? No, reset_index happened.\n",
    "    df_bars_lookup = df_bars.set_index('DateTime').sort_index()\n",
    "    \n",
    "    brain = KineticBrain(threshold=KINETIC_THRESHOLD)\n",
    "    trades = []\n",
    "    in_trade = False\n",
    "    \n",
    "    # Iteration\n",
    "    for i, row in df_tick.iterrows():\n",
    "        ts = row['DateTime']\n",
    "        ltp = row['LTP']\n",
    "        vol = row['Volume'] if 'Volume' in row else 0 # Or OpenInterest/LTQ cumsum\n",
    "        \n",
    "        k_score = brain.tick(ltp, vol)\n",
    "        \n",
    "        if not in_trade and k_score > KINETIC_THRESHOLD:\n",
    "            # TRIGGER FIRED: CHECK MODEL\n",
    "            # Find the latest CLOSED bar prediction\n",
    "            # We use 'asof' to find the nearest bar BEFORE current tick\n",
    "            try:\n",
    "                # We need the prediction made at the end of the previous 15s interval\n",
    "                # which predicts the current interval\n",
    "                latest_bar_idx = df_bars_lookup.index.asof(ts)\n",
    "                \n",
    "                if pd.isna(latest_bar_idx): continue\n",
    "                \n",
    "                bar_data = df_bars_lookup.loc[latest_bar_idx]\n",
    "                \n",
    "                # Check if we have a prediction\n",
    "                if 'Pred_Ret' not in bar_data: continue\n",
    "                \n",
    "                model_pred = bar_data['Pred_Ret']\n",
    "                \n",
    "                # LOGIC: KINETIC + MODEL\n",
    "                if model_pred > 0.05: # Strong Buy signal (>0.05% expected return)\n",
    "                    # ENTRY LONG\n",
    "                    trades.append({\n",
    "                        'Entry_Time': ts, 'Type': 'LONG', 'Entry_Price': ltp, 'Kinetic': k_score, 'Model_Pred': model_pred\n",
    "                    })\n",
    "                    in_trade = True # Simple one trade at a time logic\n",
    "                    \n",
    "                elif model_pred < -0.05: # Strong Sell signal\n",
    "                    # ENTRY SHORT\n",
    "                    trades.append({\n",
    "                        'Entry_Time': ts, 'Type': 'SHORT', 'Entry_Price': ltp, 'Kinetic': k_score, 'Model_Pred': model_pred\n",
    "                    })\n",
    "                    in_trade = True\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "                \n",
    "        # Simple Exit Logic (e.g., 5 mins or fixed TP/SL - just for simulation)\n",
    "        # For this test, let's just log entries to see alignment\n",
    "        if in_trade:\n",
    "            # Artificial exit after 5 mins or if day ends\n",
    "            last_entry = trades[-1]\n",
    "            if (ts - last_entry['Entry_Time']).total_seconds() > 300:\n",
    "                trades[-1]['Exit_Time'] = ts\n",
    "                trades[-1]['Exit_Price'] = ltp\n",
    "                in_trade = False\n",
    "\n",
    "    # PnL Calc\n",
    "    res = pd.DataFrame(trades)\n",
    "    if not res.empty:\n",
    "        res['PnL_Pts'] = np.where(res['Type']=='LONG', res['Exit_Price']-res['Entry_Price'], res['Entry_Price']-res['Exit_Price'])\n",
    "        res['PnL_Pts'] -= COST_PER_TRADE\n",
    "        print(f\"Total Trades: {len(res)}\")\n",
    "        print(f\"Total PnL: {res['PnL_Pts'].sum():.2f} pts\")\n",
    "        print(f\"Win Rate: {(res['PnL_Pts']>0).mean():.2f}\")\n",
    "    else:\n",
    "        print(\"No trades generated.\")\n",
    "\n",
    "# ==========================================\n",
    "# MAIN\n",
    "# ==========================================\n",
    "def main():\n",
    "    # 1. Load Model\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(\"Model file not found. Run training first.\")\n",
    "        return\n",
    "        \n",
    "    # --- SMART LOAD FIX: Check which object is which ---\n",
    "    obj1 = joblib.load(MODEL_PATH)\n",
    "    obj2 = joblib.load(SCALER_PATH)\n",
    "    \n",
    "    model = None\n",
    "    scaler = None\n",
    "    \n",
    "    # Identify Model (Has predict method)\n",
    "    if hasattr(obj1, 'predict'): model = obj1\n",
    "    elif hasattr(obj2, 'predict'): model = obj2\n",
    "        \n",
    "    # Identify Scaler (Has transform method)\n",
    "    if hasattr(obj1, 'transform'): scaler = obj1\n",
    "    elif hasattr(obj2, 'transform'): scaler = obj2\n",
    "        \n",
    "    if model is None or scaler is None:\n",
    "        print(\"Error: Could not identify Model and Scaler correctly.\")\n",
    "        return\n",
    "\n",
    "    print(\"Artifacts loaded and identified.\")\n",
    "    \n",
    "    # 2. Get Data\n",
    "    df_tick, df_bars = get_prepared_data_for_day(TEST_YEAR, TEST_MONTH, TEST_DAY)\n",
    "    if df_tick is None: return\n",
    "    \n",
    "    # 3. Test Direction\n",
    "    df_scored = test_model_direction(df_bars, model, scaler)\n",
    "    \n",
    "    # 4. Test Integration\n",
    "    run_kinetic_trading_test(df_tick, df_scored)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95dd9b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for 18-11-2025...\n",
      "Generating Features (Resampling 15s)...\n",
      "\n",
      "--- TEST: QUANT KINETIC TRADING (The Essence) ---\n",
      "Total Trades: 30\n",
      "Total PnL: -51.10 pts\n",
      "Win Rate: 40.00%\n",
      "\n",
      "--- Trade Diagnostics ---\n",
      "               Entry_Time   Type  Imbalance  ZScore  PnL_Pts\n",
      "0 2025-11-18 09:16:13.305   LONG   0.907692    5.01     14.7\n",
      "1 2025-11-18 09:21:50.408  SHORT  -0.750000    2.60    -19.7\n",
      "2 2025-11-18 09:28:42.958   LONG   0.333333    2.13      5.1\n",
      "3 2025-11-18 09:34:37.162   LONG   0.333333    2.44    -11.6\n",
      "4 2025-11-18 09:41:03.915  SHORT  -0.800000    4.12     -8.0\n",
      "5 2025-11-18 09:50:26.218  SHORT  -0.833333    1.86      4.1\n",
      "6 2025-11-18 10:00:28.318  SHORT  -0.600000    3.31    -33.4\n",
      "7 2025-11-18 10:19:58.677   LONG   0.333333    1.65      7.3\n",
      "8 2025-11-18 10:30:02.821   LONG   0.333333    1.71    -11.0\n",
      "9 2025-11-18 10:41:04.208  SHORT  -0.833333    2.82      4.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib\n",
    "import boto3\n",
    "import os\n",
    "import io\n",
    "from datetime import datetime as dt\n",
    "from collections import deque\n",
    "from scipy.signal import hilbert\n",
    "from numba import njit\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "# Testing Date\n",
    "TEST_YEAR = 2025\n",
    "TEST_MONTH = 11\n",
    "TEST_DAY = 18 \n",
    "SYMBOL = \"NIFTY\"\n",
    "FUT_TS = \"NIFTY25NOVFUT\"\n",
    "BUCKET = \"live-market-data\"\n",
    "\n",
    "# Trading Params\n",
    "LOT_SIZE = 75\n",
    "KINETIC_THRESHOLD = 37500\n",
    "COST_PER_TRADE = 1.0 # Points\n",
    "\n",
    "# QUANT THRESHOLDS (The \"Essence\")\n",
    "IMBALANCE_THRESH = 0.3  # > 0.3 means Buyers are 30% stronger\n",
    "ZSCORE_THRESH = 1.5     # Volume must be 1.5 sigma above mean\n",
    "HURST_THRESH = 0.45     # Only trade if market isn't purely mean-reverting\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# 1. IDENTICAL FEATURE ENGINEERING\n",
    "# ==========================================\n",
    "\n",
    "@njit\n",
    "def std_numba(arr):\n",
    "    n = arr.shape[0]; s = 0.0; mean = 0.0\n",
    "    for i in range(n): mean += arr[i]\n",
    "    mean /= n\n",
    "    for i in range(n): diff = arr[i] - mean; s += diff * diff\n",
    "    return np.sqrt(s / n)\n",
    "\n",
    "@njit\n",
    "def hurst_exponent_numba(ts, max_lag=100):\n",
    "    n = ts.shape[0]\n",
    "    if n < max_lag: return np.nan\n",
    "    m = max_lag - 2\n",
    "    lags = np.empty(m); tau = np.empty(m)\n",
    "    for i in range(m):\n",
    "        lag = i + 2; lags[i] = lag\n",
    "        diff_sum = 0.0; count = n - lag\n",
    "        for j in range(count): diff_sum += ts[j + lag] - ts[j]\n",
    "        mean_diff = diff_sum / count\n",
    "        var = 0.0\n",
    "        for j in range(count): d = (ts[j + lag] - ts[j]) - mean_diff; var += d * d\n",
    "        tau[i] = np.sqrt(var / count)\n",
    "    sum_log_lags = np.sum(np.log(lags)); sum_log_tau = np.sum(np.log(tau))\n",
    "    mean_log_lags = sum_log_lags / m; mean_log_tau = sum_log_tau / m\n",
    "    cov = 0.0; var_ll = 0.0\n",
    "    for i in range(m):\n",
    "        diff_ll = np.log(lags[i]) - mean_log_lags\n",
    "        diff_tau = np.log(tau[i]) - mean_log_tau\n",
    "        cov += diff_ll * diff_tau; var_ll += diff_ll * diff_ll\n",
    "    return cov / var_ll\n",
    "    \n",
    "@njit\n",
    "def fractal_dimension_numba(ts, max_scale=20):\n",
    "    n = ts.shape[0]\n",
    "    if n < max_scale: return np.nan\n",
    "    m = max_scale - 2\n",
    "    scales = np.empty(m); variances = np.empty(m)\n",
    "    for i in range(m):\n",
    "        scale = i + 2; scales[i] = scale; npart = n // scale; var_sum = 0.0\n",
    "        for j in range(npart):\n",
    "            seg_mean = 0.0\n",
    "            for k in range(scale): seg_mean += ts[j * scale + k]\n",
    "            seg_mean /= scale\n",
    "            seg_var = 0.0\n",
    "            for k in range(scale): diff = ts[j * scale + k] - seg_mean; seg_var += diff * diff\n",
    "            var_sum += np.sqrt(seg_var / scale)\n",
    "        variances[i] = var_sum / npart\n",
    "    sum_log_scales = np.sum(np.log(scales)); sum_log_vars = np.sum(np.log(variances))\n",
    "    mean_log_scales = sum_log_scales / m; mean_log_vars = sum_log_vars / m\n",
    "    cov = 0.0; var_ll = 0.0\n",
    "    for i in range(m):\n",
    "        diff_sc = np.log(scales[i]) - mean_log_scales\n",
    "        diff_v = np.log(variances[i]) - mean_log_vars\n",
    "        cov += diff_sc * diff_v; var_ll += diff_sc * diff_sc\n",
    "    return 2 - (cov / var_ll)\n",
    "\n",
    "@njit\n",
    "def lyapunov_exponent_numba(time_series, epsilon=1e-4, steps=100):\n",
    "    N = time_series.shape[0]\n",
    "    if N <= steps: return np.nan\n",
    "    s = 0.0; count = N - steps\n",
    "    diff0 = np.abs(time_series[steps] - time_series[0])\n",
    "    if diff0 < epsilon: diff0 = epsilon\n",
    "    for i in range(count):\n",
    "        diff = np.abs(time_series[i + steps] - time_series[i])\n",
    "        if diff < epsilon: diff = epsilon\n",
    "        s += np.log(diff / diff0)\n",
    "    return s / (count * steps)\n",
    "\n",
    "def clean_time_format(time_str):\n",
    "    if '.' not in time_str: return f\"{time_str}.000000\"\n",
    "    time_parts = time_str.split('.')\n",
    "    milliseconds = time_parts[1][:6].ljust(6, '0')\n",
    "    return f\"{time_parts[0]}.{milliseconds}\"  \n",
    "\n",
    "def additional_columns(df):\n",
    "    df = df.copy()\n",
    "    df['Session'] = np.where((df['Date'] != df['Date'].shift(1)) & (df['Time'] != pd.to_datetime('00:00:00').time()), 0, np.nan)\n",
    "    df['Session'] = np.where((df['Session'].shift(-1) == 0), 1, df['Session'])  \n",
    "    df.at[df.index[-1], 'Session'] = 1\n",
    "    return df\n",
    "\n",
    "def rolling_hilbert(series):\n",
    "    if len(series) < 10: return np.nan\n",
    "    return np.angle(hilbert(series)[-1])\n",
    "    \n",
    "def forecast_next(x):\n",
    "    if len(x) < 2: return x[-1]\n",
    "    X = np.arange(len(x)); coef = np.polyfit(X, x, 1)   \n",
    "    return np.polyval(coef, len(x))  \n",
    "\n",
    "def generate_basic_variables(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    open_dict, high_dict, low_dict, close_dict, volume_dict = df['Open'].to_dict(), df['High'].to_dict(), df['Low'].to_dict(), df['Close'].to_dict(), df['Volume'].to_dict()\n",
    "    df['PrevOpen'] = df['PrevBar'].map(lambda x: open_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevHigh'] = df['PrevBar'].map(lambda x: high_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevLow'] = df['PrevBar'].map(lambda x: low_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevClose'] = df['PrevBar'].map(lambda x: close_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevVolume'] = df['PrevBar'].map(lambda x: volume_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['SecondInterval'] = np.floor(df['TimePassed']).where(df['TimePassed'] < 15, np.nan) + 1\n",
    "\n",
    "    df['Spread'] = (df['BestAsk'] - df['BestBid']).round(2)\n",
    "    df['SpreadSize'] = (df['AskSize'] - df['BidSize']).round(2)\n",
    "\n",
    "    df['LTQ'] = np.where(df['GroupNum'] == df['GroupNum'].shift(1), df['Volume'] - df['Volume'].shift(1), df['Volume'])\n",
    "    LTQ_dict = df['LTQ'].to_dict()\n",
    "    df['PrevLTQ'] = df['PrevBar'].map(lambda x: LTQ_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevPrevLTQ'] = df['PrevPrevBar'].map(lambda x: LTQ_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['MeanLTQ'] = df['LTQ'].expanding().mean().round(2)\n",
    "    df['StdLTQ'] = df['LTQ'].expanding().std().round(2)\n",
    "    df['SumPrevLTQ'] = df['PrevLTQ'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SumPrevLTQ'] = df['SumPrevLTQ'].ffill()\n",
    "    df['PrevMeanLTQ'] = (df['SumPrevLTQ']/(df['GroupNum']-1)).round(2)\n",
    "    df['ZScoreLTQ'] = ((df['LTQ'] - df['MeanLTQ'])/df['StdLTQ']).round(2)\n",
    "    df['SkewLTQVariance'] = ((df['LTQ'] - df['MeanLTQ'])**3).cumsum()\n",
    "    df['SkewLTQ'] = (df['SkewLTQVariance']/((df['StdLTQ']**3)*(df.index))).round(2)\n",
    "    \n",
    "    # ... [Assuming full feature logic exists, using simplified for robustness in this snippet]\n",
    "    # Critical Features for Logic:\n",
    "    df['feature_imbalance'] = (df['BidSize'] - df['AskSize']) / (df['BidSize'] + df['AskSize'] + 1e-9)\n",
    "    df['realtive_spread'] = df['Spread'] / ((df['BestBid'] + df['BestAsk']) / 2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def resampled_ohlc(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # --- FIXED COLUMN MAPPING ---\n",
    "    rename_map = {\n",
    "        'BestBuyPrice': 'BuyPrice', 'BestSellPrice': 'SellPrice',\n",
    "        'BestBuyQty': 'BuyQty', 'BestSellQty': 'SellQty',\n",
    "        'BestBid': 'BuyPrice', 'BestAsk': 'SellPrice', # Added aliases\n",
    "        'BidSize': 'BuyQty', 'AskSize': 'SellQty',\n",
    "        'LastTradedQty': 'LTQ', 'LastTradedPrice': 'LTP'\n",
    "    }\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "    \n",
    "    df['Time'] = df['Time'].astype(str).apply(clean_time_format)\n",
    "    df['datetime'] = pd.to_datetime(df['Date'].astype(str) + ' ' + df['Time'].astype(str), errors='coerce')\n",
    "    df.set_index('datetime', inplace=True)\n",
    "\n",
    "    # 15s Resampling\n",
    "    interval_start = df.index.floor('15s')\n",
    "    df['group'] = interval_start\n",
    "    df['GroupTime'] = interval_start.time\n",
    "    df['TimePassed'] = ((pd.to_datetime(df['Time'], format='%H:%M:%S.%f') - pd.to_datetime(df['GroupTime'], format='%H:%M:%S')).dt.total_seconds())\n",
    "\n",
    "    df['Open'] = df.groupby('group')['LTP'].transform('first')\n",
    "    df['High'] = df.groupby('group')['LTP'].transform('cummax')\n",
    "    df['Low'] = df.groupby('group')['LTP'].transform('cummin')\n",
    "    df['Close'] = df['LTP']\n",
    "    \n",
    "    # Safe access after rename\n",
    "    df['BestBid'] = df['BuyPrice'] if 'BuyPrice' in df else np.nan\n",
    "    df['BestAsk'] = df['SellPrice'] if 'SellPrice' in df else np.nan\n",
    "    df['BidSize'] = df['BuyQty'] if 'BuyQty' in df else np.nan\n",
    "    df['AskSize'] = df['SellQty'] if 'SellQty' in df else np.nan\n",
    "    \n",
    "    df['Volume'] = df.groupby('group')['LTQ'].transform('cumsum')\n",
    "    df['GroupNum'] = (df['group'] != df['group'].shift(1)).cumsum()\n",
    "    \n",
    "    # Helper for Time features\n",
    "    df['GroupTimeOnly'] = df['group'].dt.time\n",
    "    intervals_list_sorted = sorted(df['GroupTimeOnly'].unique())\n",
    "    time_map = {t: i + 1 for i, t in enumerate(intervals_list_sorted)}\n",
    "    df['GroupNumTime'] = df['GroupTimeOnly'].map(time_map)\n",
    "\n",
    "    ohlc = df.copy()\n",
    "    ohlc.reset_index(inplace=True)\n",
    "    ohlc['Date'] = ohlc['datetime'].dt.strftime('%d/%m/%Y')\n",
    "    ohlc.drop(columns=['datetime'], inplace=True)\n",
    "    \n",
    "    mask = ohlc['GroupNum'] != ohlc['GroupNum'].shift(1)\n",
    "    ohlc['PrevBar'] = np.where(mask, ohlc.index - 1, np.nan)\n",
    "    ohlc['PrevBar'] = ohlc['PrevBar'].ffill()\n",
    "    ohlc['PrevPrevBar'] = np.where(mask, ohlc['PrevBar'].shift(1), np.nan)\n",
    "    ohlc['PrevPrevBar'] = ohlc['PrevPrevBar'].ffill()\n",
    "\n",
    "    ohlc.rename(columns={'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'group': 'Group'}, inplace=True)\n",
    "    ohlc.dropna(subset=['Open'], inplace=True)\n",
    "    ohlc = additional_columns(ohlc)\n",
    "    ohlc['SessionBar'] = ohlc.groupby('Group')['Session'].transform(lambda x: (x == 0).any())\n",
    "    \n",
    "    return ohlc.reset_index(drop=True)\n",
    "\n",
    "# ==========================================\n",
    "# 2. PIPELINE: DATA & PREP\n",
    "# ==========================================\n",
    "\n",
    "def get_prepared_data_for_day(year, month, day):\n",
    "    # Download\n",
    "    key = f\"year={year}/month={month:02d}/day={day:02d}/Futures/{SYMBOL}/{FUT_TS}.parquet\"\n",
    "    local_path = \"test_data.parquet\"\n",
    "    \n",
    "    s3 = boto3.client(\"s3\")\n",
    "    try:\n",
    "        s3.download_file(BUCKET, key, local_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {key}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # Load Raw Tick Data\n",
    "    df_tick = pd.read_parquet(local_path)\n",
    "    df_tick[\"DateTime\"] = pd.to_datetime(\n",
    "        df_tick[\"Date\"].astype(str) + \" \" + df_tick[\"Time\"].astype(str), \n",
    "        dayfirst=True, errors=\"coerce\"\n",
    "    )\n",
    "    df_tick[\"LTP\"] = pd.to_numeric(df_tick[\"LastTradedPrice\"] if \"LastTradedPrice\" in df_tick else df_tick[\"LTP\"], errors='coerce')\n",
    "    df_tick = df_tick.dropna(subset=[\"DateTime\", \"LTP\"]).sort_values(\"DateTime\").reset_index(drop=True)\n",
    "    \n",
    "    # Process 15s Bars\n",
    "    print(\"Generating Features (Resampling 15s)...\")\n",
    "    df_prep = df_tick.copy()\n",
    "    df_bars = resampled_ohlc(df_prep)\n",
    "    \n",
    "    # Aggregation\n",
    "    df_bars['DateTime'] = pd.to_datetime(df_bars['Date'] + ' ' + df_bars['Time'], format=\"%d/%m/%Y %H:%M:%S.%f\", errors='coerce')\n",
    "    df_bars.set_index('DateTime', inplace=True)\n",
    "    \n",
    "    # --- FIX 1: Removed 'PrevVolume' (doesn't exist yet) ---\n",
    "    # --- FIX 2: Added ALL structural columns required by feature gen ---\n",
    "    df_agg = df_bars.resample('15s').agg({\n",
    "        'Date': 'last', 'Time': 'last', 'Open': 'first', 'High':'max', 'Low':'min', 'Close':'last',\n",
    "        'BestBid':'last', 'BestAsk':'last', 'BidSize':'last', 'AskSize':'last', 'Volume':'sum',\n",
    "        'GroupNum':'last',\n",
    "        'Group':'last', 'GroupNumTime':'last', 'TimePassed':'last', \n",
    "        'PrevBar':'last', 'PrevPrevBar':'last', 'Session':'last', 'SessionBar':'last' \n",
    "    }).reset_index()\n",
    "    \n",
    "    # Generate Vars\n",
    "    df_feats = generate_basic_variables(df_agg)\n",
    "    \n",
    "    # Add Complex Vars (Numba)\n",
    "    df_feats['Close'] = pd.to_numeric(df_feats['Close'], errors='coerce')\n",
    "    for w in [200]: # 500, 1000, 1500 omitted for speed in test\n",
    "        df_feats[f'Lyapunov_{w}'] = df_feats['Close'].rolling(w).apply(lambda x: lyapunov_exponent_numba(x), raw=True)\n",
    "        df_feats[f'Hurst_{w}'] = df_feats['Close'].rolling(w).apply(lambda x: hurst_exponent_numba(x), raw=True)\n",
    "        \n",
    "    return df_tick, df_feats\n",
    "\n",
    "# ==========================================\n",
    "# 3. TEST: QUANT KINETIC LOGIC\n",
    "# ==========================================\n",
    "\n",
    "class KineticBrain:\n",
    "    def __init__(self, threshold=37500):\n",
    "        self.threshold = threshold\n",
    "        self.buf = deque(maxlen=50)\n",
    "        self.last_score = 0\n",
    "    \n",
    "    def tick(self, ltp, vol):\n",
    "        self.buf.append((ltp, vol))\n",
    "        if len(self.buf) < 50: return 0\n",
    "        \n",
    "        arr = np.array(self.buf)\n",
    "        p, v = arr[:,0], arr[:,1]\n",
    "        v_diff = np.diff(v)\n",
    "        v_diff = np.where(v_diff < 0, 0, v_diff)\n",
    "        \n",
    "        score = np.sum(v_diff) / (abs(p[-1]-p[0]) + 0.05)\n",
    "        self.last_score = score\n",
    "        return score\n",
    "\n",
    "def run_kinetic_trading_test(df_tick, df_bars):\n",
    "    print(\"\\n--- TEST: QUANT KINETIC TRADING (The Essence) ---\")\n",
    "    \n",
    "    # Index bars by time for fast lookup\n",
    "    df_bars_lookup = df_bars.set_index('DateTime').sort_index()\n",
    "    \n",
    "    brain = KineticBrain(threshold=KINETIC_THRESHOLD)\n",
    "    trades = []\n",
    "    in_trade = False\n",
    "    \n",
    "    # Iteration\n",
    "    for i, row in df_tick.iterrows():\n",
    "        ts = row['DateTime']\n",
    "        ltp = row['LTP']\n",
    "        vol = row['Volume'] if 'Volume' in row else 0 \n",
    "        \n",
    "        k_score = brain.tick(ltp, vol)\n",
    "        \n",
    "        if not in_trade and k_score > KINETIC_THRESHOLD:\n",
    "            # TRIGGER FIRED: CHECK ESSENCE FEATURES\n",
    "            try:\n",
    "                # Find the latest CLOSED bar feature data\n",
    "                latest_bar_idx = df_bars_lookup.index.asof(ts)\n",
    "                if pd.isna(latest_bar_idx): continue\n",
    "                \n",
    "                bar_data = df_bars_lookup.loc[latest_bar_idx]\n",
    "                \n",
    "                # EXTRACT ESSENCE\n",
    "                imbalance = bar_data.get('feature_imbalance', 0)\n",
    "                zscore = bar_data.get('ZScoreLTQ', 0)\n",
    "                hurst = bar_data.get('Hurst_200', 0.5)\n",
    "                \n",
    "                # --- QUANT LOGIC ---\n",
    "                # 1. Feature Imbalance: Positive = Buyers, Negative = Sellers\n",
    "                # 2. ZScore: Must be > 1.5 (Significant Volume)\n",
    "                # 3. Hurst: If < 0.45 (Mean Reverting), maybe fade? Let's stick to Trend for now.\n",
    "                \n",
    "                is_valid_setup = (zscore > ZSCORE_THRESH) or (hurst > HURST_THRESH)\n",
    "                \n",
    "                if is_valid_setup:\n",
    "                    if imbalance > IMBALANCE_THRESH: \n",
    "                        # Strong Buyers + Kinetic -> LONG\n",
    "                        trades.append({\n",
    "                            'Entry_Time': ts, 'Type': 'LONG', 'Entry_Price': ltp, \n",
    "                            'Kinetic': k_score, 'Imbalance': imbalance, 'ZScore': zscore\n",
    "                        })\n",
    "                        in_trade = True \n",
    "                        \n",
    "                    elif imbalance < -IMBALANCE_THRESH: \n",
    "                        # Strong Sellers + Kinetic -> SHORT\n",
    "                        trades.append({\n",
    "                            'Entry_Time': ts, 'Type': 'SHORT', 'Entry_Price': ltp, \n",
    "                            'Kinetic': k_score, 'Imbalance': imbalance, 'ZScore': zscore\n",
    "                        })\n",
    "                        in_trade = True\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "                \n",
    "        # Simple Simulation Exit\n",
    "        if in_trade:\n",
    "            last_entry = trades[-1]\n",
    "            # Exit after 5 mins\n",
    "            if (ts - last_entry['Entry_Time']).total_seconds() > 300:\n",
    "                trades[-1]['Exit_Time'] = ts\n",
    "                trades[-1]['Exit_Price'] = ltp\n",
    "                in_trade = False\n",
    "\n",
    "    # PnL Calc\n",
    "    res = pd.DataFrame(trades)\n",
    "    if not res.empty:\n",
    "        res['PnL_Pts'] = np.where(res['Type']=='LONG', res['Exit_Price']-res['Entry_Price'], res['Entry_Price']-res['Exit_Price'])\n",
    "        res['PnL_Pts'] -= COST_PER_TRADE\n",
    "        print(f\"Total Trades: {len(res)}\")\n",
    "        print(f\"Total PnL: {res['PnL_Pts'].sum():.2f} pts\")\n",
    "        print(f\"Win Rate: {(res['PnL_Pts']>0).mean()*100:.2f}%\")\n",
    "        \n",
    "        # Diagnostics\n",
    "        print(\"\\n--- Trade Diagnostics ---\")\n",
    "        print(res[['Entry_Time', 'Type', 'Imbalance', 'ZScore', 'PnL_Pts']].head(10))\n",
    "    else:\n",
    "        print(\"No trades generated. (Tighten Thresholds?)\")\n",
    "\n",
    "# ==========================================\n",
    "# MAIN\n",
    "# ==========================================\n",
    "def main():\n",
    "    # 2. Get Data\n",
    "    print(f\"Loading Data for {TEST_DAY}-{TEST_MONTH}-{TEST_YEAR}...\")\n",
    "    df_tick, df_bars = get_prepared_data_for_day(TEST_YEAR, TEST_MONTH, TEST_DAY)\n",
    "    if df_tick is None: return\n",
    "    \n",
    "    # 3. Test Integration (No Model, Just Quant)\n",
    "    run_kinetic_trading_test(df_tick, df_bars)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e97f6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for 18-11-2025...\n",
      "Generating Features (Resampling 15s)...\n",
      "\n",
      "--- TEST: QUANT KINETIC TRADING (Managed) ---\n",
      "Total Trades: 15\n",
      "Total PnL: -23.60 pts\n",
      "Win Rate: 46.67%\n",
      "\n",
      "--- Trade Diagnostics ---\n",
      "               Entry_Time   Type Reason  PnL_Pts\n",
      "0 2025-11-18 09:16:35.251   LONG     TP     29.0\n",
      "1 2025-11-18 09:41:03.915  SHORT     SL    -16.1\n",
      "2 2025-11-18 09:50:26.218  SHORT     TP     29.0\n",
      "3 2025-11-18 10:00:28.318  SHORT     SL    -19.9\n",
      "4 2025-11-18 10:01:19.025  SHORT     SL    -16.0\n",
      "5 2025-11-18 10:24:51.980   LONG     SL    -16.5\n",
      "6 2025-11-18 11:00:26.243   LONG   TIME      5.8\n",
      "7 2025-11-18 11:41:45.296  SHORT     SL    -17.7\n",
      "8 2025-11-18 13:13:45.330   LONG   TIME      9.5\n",
      "9 2025-11-18 13:48:12.270  SHORT   TIME      3.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib\n",
    "import boto3\n",
    "import os\n",
    "import io\n",
    "from datetime import datetime as dt\n",
    "from collections import deque\n",
    "from scipy.signal import hilbert\n",
    "from numba import njit\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "# Model Artifacts\n",
    "MODEL_PATH = \"2025-09-01-2025-11-15-model-tuned.pkl\"\n",
    "SCALER_PATH = \"2025-09-01-2025-11-15-scaler-tuned.pkl\"\n",
    "\n",
    "# Testing Date (Nov 18 is a Tuesday)\n",
    "TEST_YEAR = 2025\n",
    "TEST_MONTH = 11\n",
    "TEST_DAY = 18 \n",
    "SYMBOL = \"NIFTY\"\n",
    "FUT_TS = \"NIFTY25NOVFUT\"\n",
    "BUCKET = \"live-market-data\"\n",
    "\n",
    "# Trading Params\n",
    "LOT_SIZE = 75\n",
    "KINETIC_THRESHOLD = 37500\n",
    "COST_PER_TRADE = 1.0 \n",
    "\n",
    "# QUANT LOGIC SETTINGS\n",
    "IMBALANCE_THRESH = 0.4  # Increased from 0.3 for stronger conviction\n",
    "ZSCORE_THRESH = 1.5     \n",
    "HURST_THRESH = 0.45     \n",
    "\n",
    "# TRADE MANAGEMENT (The \"Shield\")\n",
    "STOP_LOSS = 15.0        # Max tolerance for pain\n",
    "TAKE_PROFIT = 30.0      # 1:2 Risk Reward\n",
    "MAX_HOLD_SEC = 600      # 10 Minutes max\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# 1. IDENTICAL FEATURE ENGINEERING\n",
    "# ==========================================\n",
    "\n",
    "@njit\n",
    "def std_numba(arr):\n",
    "    n = arr.shape[0]; s = 0.0; mean = 0.0\n",
    "    for i in range(n): mean += arr[i]\n",
    "    mean /= n\n",
    "    for i in range(n): diff = arr[i] - mean; s += diff * diff\n",
    "    return np.sqrt(s / n)\n",
    "\n",
    "@njit\n",
    "def hurst_exponent_numba(ts, max_lag=100):\n",
    "    n = ts.shape[0]\n",
    "    if n < max_lag: return np.nan\n",
    "    m = max_lag - 2\n",
    "    lags = np.empty(m); tau = np.empty(m)\n",
    "    for i in range(m):\n",
    "        lag = i + 2; lags[i] = lag\n",
    "        diff_sum = 0.0; count = n - lag\n",
    "        for j in range(count): diff_sum += ts[j + lag] - ts[j]\n",
    "        mean_diff = diff_sum / count\n",
    "        var = 0.0\n",
    "        for j in range(count): d = (ts[j + lag] - ts[j]) - mean_diff; var += d * d\n",
    "        tau[i] = np.sqrt(var / count)\n",
    "    sum_log_lags = np.sum(np.log(lags)); sum_log_tau = np.sum(np.log(tau))\n",
    "    mean_log_lags = sum_log_lags / m; mean_log_tau = sum_log_tau / m\n",
    "    cov = 0.0; var_ll = 0.0\n",
    "    for i in range(m):\n",
    "        diff_ll = np.log(lags[i]) - mean_log_lags\n",
    "        diff_tau = np.log(tau[i]) - mean_log_tau\n",
    "        cov += diff_ll * diff_tau; var_ll += diff_ll * diff_ll\n",
    "    return cov / var_ll\n",
    "    \n",
    "@njit\n",
    "def fractal_dimension_numba(ts, max_scale=20):\n",
    "    n = ts.shape[0]\n",
    "    if n < max_scale: return np.nan\n",
    "    m = max_scale - 2\n",
    "    scales = np.empty(m); variances = np.empty(m)\n",
    "    for i in range(m):\n",
    "        scale = i + 2; scales[i] = scale; npart = n // scale; var_sum = 0.0\n",
    "        for j in range(npart):\n",
    "            seg_mean = 0.0\n",
    "            for k in range(scale): seg_mean += ts[j * scale + k]\n",
    "            seg_mean /= scale\n",
    "            seg_var = 0.0\n",
    "            for k in range(scale): diff = ts[j * scale + k] - seg_mean; seg_var += diff * diff\n",
    "            var_sum += np.sqrt(seg_var / scale)\n",
    "        variances[i] = var_sum / npart\n",
    "    sum_log_scales = np.sum(np.log(scales)); sum_log_vars = np.sum(np.log(variances))\n",
    "    mean_log_scales = sum_log_scales / m; mean_log_vars = sum_log_vars / m\n",
    "    cov = 0.0; var_ll = 0.0\n",
    "    for i in range(m):\n",
    "        diff_sc = np.log(scales[i]) - mean_log_scales\n",
    "        diff_v = np.log(variances[i]) - mean_log_vars\n",
    "        cov += diff_sc * diff_v; var_ll += diff_sc * diff_sc\n",
    "    return 2 - (cov / var_ll)\n",
    "\n",
    "@njit\n",
    "def lyapunov_exponent_numba(time_series, epsilon=1e-4, steps=100):\n",
    "    N = time_series.shape[0]\n",
    "    if N <= steps: return np.nan\n",
    "    s = 0.0; count = N - steps\n",
    "    diff0 = np.abs(time_series[steps] - time_series[0])\n",
    "    if diff0 < epsilon: diff0 = epsilon\n",
    "    for i in range(count):\n",
    "        diff = np.abs(time_series[i + steps] - time_series[i])\n",
    "        if diff < epsilon: diff = epsilon\n",
    "        s += np.log(diff / diff0)\n",
    "    return s / (count * steps)\n",
    "\n",
    "def clean_time_format(time_str):\n",
    "    if '.' not in time_str: return f\"{time_str}.000000\"\n",
    "    time_parts = time_str.split('.')\n",
    "    milliseconds = time_parts[1][:6].ljust(6, '0')\n",
    "    return f\"{time_parts[0]}.{milliseconds}\"  \n",
    "\n",
    "def additional_columns(df):\n",
    "    df = df.copy()\n",
    "    df['Session'] = np.where((df['Date'] != df['Date'].shift(1)) & (df['Time'] != pd.to_datetime('00:00:00').time()), 0, np.nan)\n",
    "    df['Session'] = np.where((df['Session'].shift(-1) == 0), 1, df['Session'])  \n",
    "    df.at[df.index[-1], 'Session'] = 1\n",
    "    return df\n",
    "\n",
    "def rolling_hilbert(series):\n",
    "    if len(series) < 10: return np.nan\n",
    "    return np.angle(hilbert(series)[-1])\n",
    "    \n",
    "def forecast_next(x):\n",
    "    if len(x) < 2: return x[-1]\n",
    "    X = np.arange(len(x)); coef = np.polyfit(X, x, 1)   \n",
    "    return np.polyval(coef, len(x))  \n",
    "\n",
    "def generate_basic_variables(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    open_dict, high_dict, low_dict, close_dict, volume_dict = df['Open'].to_dict(), df['High'].to_dict(), df['Low'].to_dict(), df['Close'].to_dict(), df['Volume'].to_dict()\n",
    "    df['PrevOpen'] = df['PrevBar'].map(lambda x: open_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevHigh'] = df['PrevBar'].map(lambda x: high_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevLow'] = df['PrevBar'].map(lambda x: low_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevClose'] = df['PrevBar'].map(lambda x: close_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevVolume'] = df['PrevBar'].map(lambda x: volume_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['SecondInterval'] = np.floor(df['TimePassed']).where(df['TimePassed'] < 15, np.nan) + 1\n",
    "\n",
    "    df['Spread'] = (df['BestAsk'] - df['BestBid']).round(2)\n",
    "    df['SpreadSize'] = (df['AskSize'] - df['BidSize']).round(2)\n",
    "\n",
    "    df['LTQ'] = np.where(df['GroupNum'] == df['GroupNum'].shift(1), df['Volume'] - df['Volume'].shift(1), df['Volume'])\n",
    "    LTQ_dict = df['LTQ'].to_dict()\n",
    "    df['PrevLTQ'] = df['PrevBar'].map(lambda x: LTQ_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevPrevLTQ'] = df['PrevPrevBar'].map(lambda x: LTQ_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['MeanLTQ'] = df['LTQ'].expanding().mean().round(2)\n",
    "    df['StdLTQ'] = df['LTQ'].expanding().std().round(2)\n",
    "    df['SumPrevLTQ'] = df['PrevLTQ'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SumPrevLTQ'] = df['SumPrevLTQ'].ffill()\n",
    "    df['PrevMeanLTQ'] = (df['SumPrevLTQ']/(df['GroupNum']-1)).round(2)\n",
    "    df['ZScoreLTQ'] = ((df['LTQ'] - df['MeanLTQ'])/df['StdLTQ']).round(2)\n",
    "    df['SkewLTQVariance'] = ((df['LTQ'] - df['MeanLTQ'])**3).cumsum()\n",
    "    df['SkewLTQ'] = (df['SkewLTQVariance']/((df['StdLTQ']**3)*(df.index))).round(2)\n",
    "    \n",
    "    # Simplified Percentiles for speed in test mode (expanding is slow)\n",
    "    # Using rolling approximation or full expanding if critical\n",
    "    df['LTQ_10th_Percentile'] = df['LTQ'].expanding().quantile(0.10)\n",
    "    df['LTQ_25th_Percentile'] = df['LTQ'].expanding().quantile(0.25)\n",
    "    df['LTQ_50th_Percentile'] = df['LTQ'].expanding().quantile(0.50)\n",
    "    df['LTQ_75th_Percentile'] = df['LTQ'].expanding().quantile(0.75)\n",
    "    df['LTQ_90th_Percentile'] = df['LTQ'].expanding().quantile(0.90)\n",
    "    \n",
    "    df['PrevAutoCorLTQNumerator'] = ((df['LTQ'] - df['MeanLTQ']) * (df['LTQ'].shift(1) - df['MeanLTQ'])).cumsum()\n",
    "    df['PrevAutoCorLTQDenominator'] = np.square(df['LTQ']- df['MeanLTQ']).cumsum()\n",
    "    df['AutoCorrelationLTQ'] = (df['PrevAutoCorLTQNumerator']/df['PrevAutoCorLTQDenominator']).round(2)\n",
    "    df['DeviationFlag'] = np.where(df['LTQ'] > (df['MeanLTQ'] + df['StdLTQ']), 1, 0)\n",
    "    df['AbsoluteDeviationLTQ'] = df['DeviationFlag'].cumsum()\n",
    "    df['SumAbsoluteDeviationLTQ'] = df['LTQ'].where(df['AbsoluteDeviationLTQ'] != df['AbsoluteDeviationLTQ'].shift(1)).cumsum()\n",
    "    df['SumAbsoluteDeviationLTQ'] = df['SumAbsoluteDeviationLTQ'].ffill()\n",
    "    df['MeanAbsoluteDeviationLTQ'] = np.where(df['AbsoluteDeviationLTQ'] != 0, (df['SumAbsoluteDeviationLTQ']/df['AbsoluteDeviationLTQ']).round(2), np.nan)\n",
    "    \n",
    "    volume_dict = df['Volume'].to_dict()\n",
    "    df['PrevPrevVolume'] = df['PrevPrevBar'].map(lambda x: volume_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['SumPrevVolume'] = df['PrevVolume'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SumPrevVolume'] = df['SumPrevVolume'].ffill()\n",
    "    df['PrevMeanVolume'] = (df['SumPrevVolume']/(df['GroupNum']-1)).round(2)\n",
    "    df['MeanVolume'] = ((df['SumPrevVolume'] + df['Volume'])/df['GroupNum']).round(2)\n",
    "    df['ResidualsVolume'] = np.square(df['PrevVolume'] - df['PrevMeanVolume']).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['ResidualsVolume'] = df['ResidualsVolume'].ffill()\n",
    "    df['PrevStdVolume'] = np.sqrt(df['ResidualsVolume']/df['GroupNum']-1)\n",
    "    df['ResidualsVolume'] = df['ResidualsVolume'] + np.square(df['Volume'] - df['MeanVolume'])\n",
    "    df['StdVolume'] = (np.sqrt(df['ResidualsVolume']/df['GroupNum'])).round(2)\n",
    "    df['PrevZScoreVolume'] = ((df['PrevVolume']-df['PrevMeanVolume'])/df['PrevStdVolume']).round(2)\n",
    "    df['ZScoreVolume'] = ((df['Volume']-df['MeanVolume'])/df['StdVolume']).round(2)\n",
    "    \n",
    "    df['SkewNumerator'] = ((df['PrevVolume'] - df['PrevMeanVolume'])**3).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SkewNumerator'] = df['SkewNumerator'].ffill()\n",
    "    df['PrevSkewVariance'] = df['SkewNumerator']/(df['GroupNum']-1)\n",
    "    df['PrevSkewVolume'] = df['PrevSkewVariance']/(df['PrevStdVolume']**3)\n",
    "    df['SkewVariance'] = (df['SkewNumerator'] + (df['Volume'] - df['MeanVolume'])**3)/df['GroupNum']\n",
    "    df['SkewVolume'] = (df['SkewVariance']/(df['StdVolume']**3)).round(2)\n",
    "    df['CVVolume'] = (df['StdVolume']/df['MeanVolume']).round(2)\n",
    "    \n",
    "    df['KurtosisNumerator'] = ((df['PrevVolume'] - df['PrevMeanVolume'])**4).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['KurtosisNumerator'] = df['KurtosisNumerator'].ffill()\n",
    "    df['PrevKurtosisVariance'] = df['KurtosisNumerator']/(df['GroupNum']-1)\n",
    "    df['PrevKurtosisVolume'] = df['PrevKurtosisVariance']/(df['PrevStdVolume']**4)-3\n",
    "    df['KurtosisVariance'] = (df['KurtosisNumerator'] + (df['Volume'] - df['MeanVolume'])**4)/df['GroupNum']\n",
    "    df['KurtosisVolume'] = (df['KurtosisVariance']/(df['StdVolume']**4)-3).round(2)\n",
    "    \n",
    "    df['PrevAutoCorVolumeNumerator'] = ((df['PrevVolume'] - df['PrevMeanVolume']) * (df['PrevPrevVolume'] - df['PrevMeanVolume'])).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['PrevAutoCorVolumeDenominator'] = np.square(df['PrevVolume'] - df['PrevMeanVolume']).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['AutoCorrelationVolume'] = (df['PrevAutoCorVolumeNumerator']/df['PrevAutoCorVolumeDenominator']).round(2)\n",
    "    df['AutoCorrelationVolume'] = df['AutoCorrelationVolume'].ffill()\n",
    "    \n",
    "    df['VolumeOutlierSum1'] = df['PrevVolume'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreVolume'].shift(1) > 1)).cumsum()\n",
    "    df['VolumeOutlierSum1'] = df['VolumeOutlierSum1'].ffill()\n",
    "    df['VolumeOutlierSum2'] = df['PrevVolume'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreVolume'].shift(1) > 2)).cumsum()\n",
    "    df['VolumeOutlierSum2'] = df['VolumeOutlierSum2'].ffill()\n",
    "    df['VolumeOutlierSum3'] = df['PrevVolume'].where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['ZScoreVolume'].shift(1) > 3)).cumsum()\n",
    "    df['VolumeOutlierSum3'] = df['VolumeOutlierSum3'].ffill()\n",
    "    df['VolumeOutlierImpact1'] = (df['VolumeOutlierSum1']/df['SumPrevVolume']).round(2)\n",
    "    df['VolumeOutlierImpact2'] = (df['VolumeOutlierSum2']/df['SumPrevVolume']).round(2)\n",
    "    df['VolumeOutlierImpact3'] = (df['VolumeOutlierSum3']/df['SumPrevVolume']).round(2)\n",
    "\n",
    "    df['Range'] = df['High'] - df['Low']\n",
    "    range_dict = df['Range'].to_dict()\n",
    "    df['PrevRange'] = df['PrevBar'].map(lambda x: range_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['SumPrevRange'] = df['PrevRange'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SumPrevRange'] = df['SumPrevRange'].ffill()\n",
    "    df['PrevMeanRange'] = (df['SumPrevRange']/(df['GroupNum']-1)).round(2)\n",
    "    df['MeanRange'] = ((df['SumPrevRange'] + df['Range'])/df['GroupNum']).round(2)\n",
    "    df['ResidualsRange'] = np.square(df['PrevRange'] - df['PrevMeanRange']).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['ResidualsRange'] = df['ResidualsRange'].ffill()\n",
    "    df['PrevStdRange'] = np.sqrt(df['ResidualsRange']/df['GroupNum']-1)\n",
    "    df['ResidualsRange'] = df['ResidualsRange'] + np.square(df['Range'] - df['MeanRange'])\n",
    "    df['StdRange'] = (np.sqrt(df['ResidualsRange']/df['GroupNum'])).round(2)\n",
    "    df['PrevZScoreRange'] = ((df['PrevRange']-df['PrevMeanRange'])/df['PrevStdRange']).round(2)\n",
    "    df['ZScoreRange'] = ((df['Range']-df['MeanRange'])/df['StdRange']).round(2)\n",
    "    \n",
    "    df['SkewNumerator'] = ((df['PrevRange'] - df['PrevMeanRange'])**3).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['SkewNumerator'] = df['SkewNumerator'].ffill()\n",
    "    df['PrevSkewVariance'] = df['SkewNumerator']/(df['GroupNum']-1)\n",
    "    df['PrevSkewRange'] = df['PrevSkewVariance']/(df['PrevStdRange']**3)\n",
    "    df['SkewVariance'] = (df['SkewNumerator'] + (df['Range'] - df['MeanRange'])**3)/df['GroupNum']\n",
    "    df['SkewRange'] = (df['SkewVariance']/(df['StdRange']**3)).round(2)\n",
    "    df['CVRange'] = (df['StdRange']/df['MeanRange']).round(2)\n",
    "    \n",
    "    df['KurtosisNumerator'] = ((df['PrevRange'] - df['PrevMeanRange'])**4).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['KurtosisNumerator'] = df['KurtosisNumerator'].ffill()\n",
    "    df['PrevKurtosisVariance'] = df['KurtosisNumerator']/(df['GroupNum']-1)\n",
    "    df['PrevKurtosisRange'] = df['PrevKurtosisVariance']/(df['PrevStdRange']**4)-3\n",
    "    df['KurtosisVariance'] = (df['KurtosisNumerator'] + (df['Range'] - df['MeanRange'])**4)/df['GroupNum']\n",
    "    df['KurtosisRange'] = (df['KurtosisVariance']/(df['StdRange']**4)-3).round(2)\n",
    "    \n",
    "    df['PrevPrevRange'] = df['PrevPrevBar'].map(lambda x: range_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['PrevAutoCorRangeNumerator'] = ((df['PrevRange'] - df['PrevMeanRange']) * (df['PrevPrevRange'] - df['PrevMeanRange'])).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['PrevAutoCorRangeDenominator'] = np.square(df['PrevRange'] - df['PrevMeanRange']).where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum()\n",
    "    df['AutoCorrelationRange'] = (df['PrevAutoCorRangeNumerator']/df['PrevAutoCorRangeDenominator']).round(2)\n",
    "    \n",
    "    df['ShiftedRange'] = df['Range'].shift(1).where(df['GroupNum'] != df['GroupNum'].shift(1))\n",
    "    df['MinRange'] = df['ShiftedRange'].cummin().ffill()\n",
    "    df['MaxRange'] = df['Range'].where(df['ZScoreRange'] <= 3).cummax().ffill()\n",
    "    df['NormalizedRange'] = np.minimum(((df['Range'] - df['MinRange'])/(df['MaxRange'] - df['MinRange'])).round(2), 1)\n",
    "\n",
    "    df['Lows'] = (df['Low'] - df['PrevLow']).round(2)\n",
    "    df['Lows'] = np.where(df['SessionBar'] == True, 0, df['Lows'])\n",
    "    df['Lows'] = np.where(df['Lows'] < 0, 0, df['Lows'])\n",
    "    df['LastLows'] = np.where(df['GroupNum'] != df['GroupNum'].shift(1), df['Lows'].shift(1), np.nan)\n",
    "    df['LastLows'] = df['LastLows'].shift(1).ffill()\n",
    "    df['LastLows'] = np.where(df['LastLows'] < 0, 0, df['LastLows'])\n",
    "    lows_dict = df['Lows'].to_dict() \n",
    "    df['PrevLows'] = df['PrevBar'].map(lambda x: lows_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['NumberOfLows'] = ((df['PrevLows'] > 0) & (df['GroupNum'] != df['GroupNum'].shift(1))).cumsum()\n",
    "    df['CurrentNoOfLows'] = np.where(df['Lows'] > 0, df['NumberOfLows']+1, df['NumberOfLows'])\n",
    "    df['SumPrevLows'] = df['PrevLows'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum().ffill()\n",
    "    df['PrevMeanLows'] = (df['SumPrevLows']/df['NumberOfLows']).round(2)\n",
    "    df['MeanLows'] = ((df['SumPrevLows'] + df['Lows'])/df['CurrentNoOfLows']).round(2)\n",
    "    df['ResidualsLows'] = (np.square(df['PrevLows'] - df['PrevMeanLows']).where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['PrevLows'] > 0)).cumsum()).round(2).ffill()\n",
    "    df['PrevStdLows'] = np.sqrt(df['ResidualsLows']/df['NumberOfLows'])\n",
    "    df['ResidualsLows'] = df['ResidualsLows'] + np.square(df['Lows'] - df['MeanLows'])\n",
    "    df['StdLows'] = (np.sqrt(df['ResidualsLows']/df['CurrentNoOfLows'])).round(2)\n",
    "    df['PrevZScoreLows'] = ((df['PrevLows']-df['PrevMeanLows'])/df['PrevStdLows']).round(2)\n",
    "    df['ZScoreLows'] = ((df['Lows']-df['MeanLows'])/df['StdLows']).round(2)\n",
    "    \n",
    "    # Simplified Skew/Kurtosis for Lows/Highs to save space - Core stats above are critical\n",
    "    df['SkewLows'] = 0.0 # Placeholder if deep calc fails/skips\n",
    "    df['CVLows'] = (df['StdLows']/df['MeanLows']).round(2)\n",
    "    df['KurtosisLows'] = 0.0\n",
    "\n",
    "    df['Highs'] = (df['PrevHigh'] - df['High']).round(2)\n",
    "    df['Highs'] = np.where(df['SessionBar'] == True, 0, df['Highs'])\n",
    "    df['Highs'] = np.where(df['Highs'] < 0, 0, df['Highs'])\n",
    "    df['LastHighs'] = np.where(df['GroupNum'] != df['GroupNum'].shift(1), df['Highs'].shift(1), np.nan)\n",
    "    df['LastHighs'] = df['LastHighs'].shift(1).ffill()\n",
    "    df['LastHighs'] = np.where(df['LastHighs'] < 0, 0, df['LastHighs'])\n",
    "    highs_dict = df['Highs'].to_dict()\n",
    "    df['PrevHighs'] = df['PrevBar'].map(lambda x: highs_dict.get(int(x), np.nan) if not np.isnan(x) else np.nan)\n",
    "    df['NumberOfHighs'] = ((df['PrevHighs'] > 0) & (df['GroupNum'] != df['GroupNum'].shift(1))).cumsum()\n",
    "    df['CurrentNoOfHighs'] = np.where(df['Lows'] > 0, df['NumberOfHighs']+1, df['NumberOfHighs'])\n",
    "    df['SumPrevHighs'] = df['PrevHighs'].where(df['GroupNum'] != df['GroupNum'].shift(1)).cumsum().ffill()\n",
    "    df['PrevMeanHighs'] = (df['SumPrevHighs']/df['NumberOfHighs']).round(2)\n",
    "    df['MeanHighs'] = ((df['SumPrevHighs'] + df['Highs'])/df['CurrentNoOfHighs']).round(2)\n",
    "    df['ResidualsHighs'] = (np.square(df['PrevHighs'] - df['PrevMeanHighs']).where((df['GroupNum'] != df['GroupNum'].shift(1)) & (df['PrevHighs'] > 0)).cumsum()).round(2).ffill()\n",
    "    df['PrevStdHighs'] = np.sqrt(df['ResidualsHighs']/df['NumberOfHighs'])\n",
    "    df['ResidualsHighs'] = df['ResidualsHighs'] + np.square(df['Highs'] - df['MeanHighs'])\n",
    "    df['StdHighs'] = (np.sqrt(df['ResidualsHighs']/df['CurrentNoOfHighs'])).round(2)\n",
    "    df['ZScoreHighs'] = ((df['Highs']-df['MeanHighs'])/df['StdHighs']).round(2)\n",
    "    \n",
    "    df['SkewHighs'] = 0.0\n",
    "    df['CVHighs'] = (df['StdHighs']/df['MeanHighs']).round(2)\n",
    "    df['KurtosisHighs'] = 0.0\n",
    "\n",
    "    # Predictions & Technicals\n",
    "    df['le_next_low_pred'] = 2 * df['Low'] - df['Low'].shift(1)\n",
    "    df['le_next_high_pred'] = 2 * df['High'] - df['High'].shift(1) \n",
    "    df['atr'] = df['Range'].rolling(window=14, min_periods=1).mean()  \n",
    "    df['atr_next_low_pred'] = df['Low'] - 0.5 * df['atr']\n",
    "    df['atr_next_high_pred'] = df['High'] + 0.5 * df['atr'] \n",
    "    \n",
    "    # Reg Forecast\n",
    "    df['reg_next_low_pred'] = df['Low'].rolling(7, min_periods=2).apply(forecast_next, raw=False)\n",
    "    df['reg_next_high_pred'] = df['High'].rolling(7, min_periods=2).apply(forecast_next, raw=False)\n",
    "    \n",
    "    df['ema_low'] = df['Low'].ewm(span=10, adjust=False).mean()\n",
    "    df['ema_high'] = df['High'].ewm(span=10, adjust=False).mean()\n",
    "    df['low_std'] = df['Low'].rolling(window=10, min_periods=1).std()\n",
    "    df['high_std'] = df['High'].rolling(window=10, min_periods=1).std()\n",
    "    df['ema_next_low_pred'] = df['ema_low'] - df['low_std']\n",
    "    df['ema_next_high_pred'] = df['ema_high'] + df['high_std']\n",
    "    \n",
    "    df['predicted_next_low'] = (df['le_next_low_pred'] + df['atr_next_low_pred'] + df['reg_next_low_pred']) / 3\n",
    "    df['predicted_next_high'] = (df['le_next_high_pred'] + df['atr_next_high_pred'] + df['reg_next_high_pred']) / 3\n",
    "    \n",
    "    df['feature_imbalance'] = (df['BidSize'] - df['AskSize']) / (df['BidSize'] + df['AskSize'] + 1e-9)\n",
    "    df['realtive_spread'] = df['Spread'] / ((df['BestBid'] + df['BestAsk']) / 2)\n",
    "    df['quote_stuffing'] = df['BestBid'].diff().abs() + df['BestAsk'].diff().abs()\n",
    "    df['depth_slope'] = (df['BidSize'] - df['AskSize']) / (df['BidSize'] + df['AskSize'] + 1e-9)\n",
    "    \n",
    "    df['price_momentum'] = df['Close'].diff()\n",
    "    df['price_momentum_ratio'] = df['Close'] / df['Close'].shift(1)\n",
    "    \n",
    "    # Hilbert & MVR\n",
    "    df['hilbert_phase'] = df['Close'].rolling(50).apply(rolling_hilbert, raw=True).shift(1)\n",
    "    df['large_order_fraction'] = (df['BidSize'] + df['AskSize']) / (df['Volume'] + 1e-9)\n",
    "    df['mvr'] = df['Close'].pct_change().rolling(5).std().shift(1) / df['Close'].pct_change().rolling(30).std().shift(1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def resampled_ohlc(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # --- FIXED COLUMN MAPPING ---\n",
    "    rename_map = {\n",
    "        'BestBuyPrice': 'BuyPrice', 'BestSellPrice': 'SellPrice',\n",
    "        'BestBuyQty': 'BuyQty', 'BestSellQty': 'SellQty',\n",
    "        'BestBid': 'BuyPrice', 'BestAsk': 'SellPrice', # Added aliases\n",
    "        'BidSize': 'BuyQty', 'AskSize': 'SellQty',\n",
    "        'LastTradedQty': 'LTQ', 'LastTradedPrice': 'LTP'\n",
    "    }\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "    \n",
    "    df['Time'] = df['Time'].astype(str).apply(clean_time_format)\n",
    "    df['datetime'] = pd.to_datetime(df['Date'].astype(str) + ' ' + df['Time'].astype(str), errors='coerce')\n",
    "    df.set_index('datetime', inplace=True)\n",
    "\n",
    "    # 15s Resampling\n",
    "    interval_start = df.index.floor('15s')\n",
    "    df['group'] = interval_start\n",
    "    df['GroupTime'] = interval_start.time\n",
    "    df['TimePassed'] = ((pd.to_datetime(df['Time'], format='%H:%M:%S.%f') - pd.to_datetime(df['GroupTime'], format='%H:%M:%S')).dt.total_seconds())\n",
    "\n",
    "    df['Open'] = df.groupby('group')['LTP'].transform('first')\n",
    "    df['High'] = df.groupby('group')['LTP'].transform('cummax')\n",
    "    df['Low'] = df.groupby('group')['LTP'].transform('cummin')\n",
    "    df['Close'] = df['LTP']\n",
    "    \n",
    "    # Safe access after rename\n",
    "    df['BestBid'] = df['BuyPrice'] if 'BuyPrice' in df else np.nan\n",
    "    df['BestAsk'] = df['SellPrice'] if 'SellPrice' in df else np.nan\n",
    "    df['BidSize'] = df['BuyQty'] if 'BuyQty' in df else np.nan\n",
    "    df['AskSize'] = df['SellQty'] if 'SellQty' in df else np.nan\n",
    "    \n",
    "    df['Volume'] = df.groupby('group')['LTQ'].transform('cumsum')\n",
    "    df['GroupNum'] = (df['group'] != df['group'].shift(1)).cumsum()\n",
    "    \n",
    "    # Helper for Time features\n",
    "    df['GroupTimeOnly'] = df['group'].dt.time\n",
    "    intervals_list_sorted = sorted(df['GroupTimeOnly'].unique())\n",
    "    time_map = {t: i + 1 for i, t in enumerate(intervals_list_sorted)}\n",
    "    df['GroupNumTime'] = df['GroupTimeOnly'].map(time_map)\n",
    "\n",
    "    ohlc = df.copy()\n",
    "    ohlc.reset_index(inplace=True)\n",
    "    ohlc['Date'] = ohlc['datetime'].dt.strftime('%d/%m/%Y')\n",
    "    ohlc.drop(columns=['datetime'], inplace=True)\n",
    "    \n",
    "    mask = ohlc['GroupNum'] != ohlc['GroupNum'].shift(1)\n",
    "    ohlc['PrevBar'] = np.where(mask, ohlc.index - 1, np.nan)\n",
    "    ohlc['PrevBar'] = ohlc['PrevBar'].ffill()\n",
    "    ohlc['PrevPrevBar'] = np.where(mask, ohlc['PrevBar'].shift(1), np.nan)\n",
    "    ohlc['PrevPrevBar'] = ohlc['PrevPrevBar'].ffill()\n",
    "\n",
    "    ohlc.rename(columns={'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'group': 'Group'}, inplace=True)\n",
    "    ohlc.dropna(subset=['Open'], inplace=True)\n",
    "    ohlc = additional_columns(ohlc)\n",
    "    ohlc['SessionBar'] = ohlc.groupby('Group')['Session'].transform(lambda x: (x == 0).any())\n",
    "    \n",
    "    return ohlc.reset_index(drop=True)\n",
    "\n",
    "# ==========================================\n",
    "# 2. PIPELINE: DATA & PREP\n",
    "# ==========================================\n",
    "\n",
    "def get_prepared_data_for_day(year, month, day):\n",
    "    # Download\n",
    "    key = f\"year={year}/month={month:02d}/day={day:02d}/Futures/{SYMBOL}/{FUT_TS}.parquet\"\n",
    "    local_path = \"test_data.parquet\"\n",
    "    \n",
    "    s3 = boto3.client(\"s3\")\n",
    "    try:\n",
    "        s3.download_file(BUCKET, key, local_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {key}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # Load Raw Tick Data\n",
    "    df_tick = pd.read_parquet(local_path)\n",
    "    df_tick[\"DateTime\"] = pd.to_datetime(\n",
    "        df_tick[\"Date\"].astype(str) + \" \" + df_tick[\"Time\"].astype(str), \n",
    "        dayfirst=True, errors=\"coerce\"\n",
    "    )\n",
    "    df_tick[\"LTP\"] = pd.to_numeric(df_tick[\"LastTradedPrice\"] if \"LastTradedPrice\" in df_tick else df_tick[\"LTP\"], errors='coerce')\n",
    "    df_tick = df_tick.dropna(subset=[\"DateTime\", \"LTP\"]).sort_values(\"DateTime\").reset_index(drop=True)\n",
    "    \n",
    "    # Process 15s Bars\n",
    "    print(\"Generating Features (Resampling 15s)...\")\n",
    "    df_prep = df_tick.copy()\n",
    "    df_bars = resampled_ohlc(df_prep)\n",
    "    \n",
    "    # Aggregation\n",
    "    df_bars['DateTime'] = pd.to_datetime(df_bars['Date'] + ' ' + df_bars['Time'], format=\"%d/%m/%Y %H:%M:%S.%f\", errors='coerce')\n",
    "    df_bars.set_index('DateTime', inplace=True)\n",
    "    \n",
    "    # --- FIX 1: Removed 'PrevVolume' (doesn't exist yet) ---\n",
    "    # --- FIX 2: Added ALL structural columns required by feature gen ---\n",
    "    df_agg = df_bars.resample('15s').agg({\n",
    "        'Date': 'last', 'Time': 'last', 'Open': 'first', 'High':'max', 'Low':'min', 'Close':'last',\n",
    "        'BestBid':'last', 'BestAsk':'last', 'BidSize':'last', 'AskSize':'last', 'Volume':'sum',\n",
    "        'GroupNum':'last',\n",
    "        'Group':'last', 'GroupNumTime':'last', 'TimePassed':'last', \n",
    "        'PrevBar':'last', 'PrevPrevBar':'last', 'Session':'last', 'SessionBar':'last' \n",
    "    }).reset_index()\n",
    "    \n",
    "    # Generate Vars\n",
    "    df_feats = generate_basic_variables(df_agg)\n",
    "    \n",
    "    # Add Complex Vars (Numba)\n",
    "    df_feats['Close'] = pd.to_numeric(df_feats['Close'], errors='coerce')\n",
    "    for w in [200, 500, 1000, 1500]:\n",
    "        df_feats[f'Lyapunov_{w}'] = df_feats['Close'].rolling(w).apply(lambda x: lyapunov_exponent_numba(x), raw=True)\n",
    "        df_feats[f'Hurst_{w}'] = df_feats['Close'].rolling(w).apply(lambda x: hurst_exponent_numba(x), raw=True)\n",
    "        df_feats[f'FDI_{w}'] = df_feats['Close'].rolling(w).apply(lambda x: fractal_dimension_numba(x), raw=True)\n",
    "        \n",
    "    return df_tick, df_feats\n",
    "\n",
    "# ==========================================\n",
    "# 3. TEST: QUANT KINETIC LOGIC WITH TRADE MANAGEMENT\n",
    "# ==========================================\n",
    "\n",
    "class KineticBrain:\n",
    "    def __init__(self, threshold=37500):\n",
    "        self.threshold = threshold\n",
    "        self.buf = deque(maxlen=50)\n",
    "        self.last_score = 0\n",
    "        self.last_vwap = 0.0\n",
    "    \n",
    "    def tick(self, ltp, vol):\n",
    "        self.buf.append((ltp, vol))\n",
    "        if len(self.buf) < 50: return 0\n",
    "        \n",
    "        arr = np.array(self.buf)\n",
    "        p, v = arr[:,0], arr[:,1]\n",
    "        v_diff = np.diff(v)\n",
    "        v_diff = np.where(v_diff < 0, 0, v_diff)\n",
    "        \n",
    "        score = np.sum(v_diff) / (abs(p[-1]-p[0]) + 0.05)\n",
    "        self.last_score = score\n",
    "        \n",
    "        # Calculate Tick VWAP (Micro-Trend)\n",
    "        self.last_vwap = np.average(p, weights=v) if np.sum(v) > 0 else np.mean(p)\n",
    "        \n",
    "        return score\n",
    "\n",
    "def run_kinetic_trading_test(df_tick, df_bars):\n",
    "    print(\"\\n--- TEST: QUANT KINETIC TRADING (Managed) ---\")\n",
    "    \n",
    "    df_bars_lookup = df_bars.set_index('DateTime').sort_index()\n",
    "    brain = KineticBrain(threshold=KINETIC_THRESHOLD)\n",
    "    trades = []\n",
    "    \n",
    "    # Trade State\n",
    "    in_trade = False\n",
    "    entry_time = None\n",
    "    entry_price = 0.0\n",
    "    trade_type = None\n",
    "    \n",
    "    for i, row in df_tick.iterrows():\n",
    "        ts = row['DateTime']\n",
    "        ltp = row['LTP']\n",
    "        vol = row['Volume'] if 'Volume' in row else 0 \n",
    "        \n",
    "        # --- TRADE MANAGEMENT (Check Every Tick) ---\n",
    "        if in_trade:\n",
    "            duration = (ts - entry_time).total_seconds()\n",
    "            pnl_pts = 0\n",
    "            \n",
    "            if trade_type == 'LONG':\n",
    "                pnl_pts = ltp - entry_price\n",
    "            else:\n",
    "                pnl_pts = entry_price - ltp\n",
    "                \n",
    "            # Exit Logic\n",
    "            exit_reason = None\n",
    "            if pnl_pts <= -STOP_LOSS: exit_reason = 'SL'\n",
    "            elif pnl_pts >= TAKE_PROFIT: exit_reason = 'TP'\n",
    "            elif duration >= MAX_HOLD_SEC: exit_reason = 'TIME'\n",
    "            \n",
    "            if exit_reason:\n",
    "                trades.append({\n",
    "                    'Entry_Time': entry_time, 'Exit_Time': ts, 'Type': trade_type, \n",
    "                    'Entry_Price': entry_price, 'Exit_Price': ltp, \n",
    "                    'PnL_Pts': pnl_pts - COST_PER_TRADE, 'Reason': exit_reason\n",
    "                })\n",
    "                in_trade = False\n",
    "                continue\n",
    "\n",
    "        # --- SIGNAL LOGIC ---\n",
    "        k_score = brain.tick(ltp, vol)\n",
    "        \n",
    "        if not in_trade and k_score > KINETIC_THRESHOLD:\n",
    "            try:\n",
    "                latest_bar_idx = df_bars_lookup.index.asof(ts)\n",
    "                if pd.isna(latest_bar_idx): continue\n",
    "                \n",
    "                bar_data = df_bars_lookup.loc[latest_bar_idx]\n",
    "                imbalance = bar_data.get('feature_imbalance', 0)\n",
    "                zscore = bar_data.get('ZScoreLTQ', 0)\n",
    "                \n",
    "                # Check 1: Statistical Significance\n",
    "                if zscore < ZSCORE_THRESH: continue\n",
    "                \n",
    "                # Check 2: Imbalance + Micro-Trend Alignment (VWAP)\n",
    "                micro_trend_up = ltp > brain.last_vwap\n",
    "                micro_trend_down = ltp < brain.last_vwap\n",
    "                \n",
    "                if imbalance > IMBALANCE_THRESH and micro_trend_up:\n",
    "                    trade_type = 'LONG'\n",
    "                    in_trade = True\n",
    "                    entry_time = ts; entry_price = ltp\n",
    "                    \n",
    "                elif imbalance < -IMBALANCE_THRESH and micro_trend_down:\n",
    "                    trade_type = 'SHORT'\n",
    "                    in_trade = True\n",
    "                    entry_time = ts; entry_price = ltp\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    # Results\n",
    "    res = pd.DataFrame(trades)\n",
    "    if not res.empty:\n",
    "        print(f\"Total Trades: {len(res)}\")\n",
    "        print(f\"Total PnL: {res['PnL_Pts'].sum():.2f} pts\")\n",
    "        print(f\"Win Rate: {(res['PnL_Pts']>0).mean()*100:.2f}%\")\n",
    "        print(\"\\n--- Trade Diagnostics ---\")\n",
    "        print(res[['Entry_Time', 'Type', 'Reason', 'PnL_Pts']].head(10))\n",
    "    else:\n",
    "        print(\"No trades generated.\")\n",
    "\n",
    "# ==========================================\n",
    "# MAIN\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(f\"Loading Data for {TEST_DAY}-{TEST_MONTH}-{TEST_YEAR}...\")\n",
    "    df_tick, df_bars = get_prepared_data_for_day(TEST_YEAR, TEST_MONTH, TEST_DAY)\n",
    "    if df_tick is None: return\n",
    "    \n",
    "    run_kinetic_trading_test(df_tick, df_bars)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e46a8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running KINETIC HUNTER from 2025-09-01 to 2025-11-26\n",
      "Trap: 09:15 - 10:00:00 | Trigger: Kinetic > 37500\n",
      "-----------------------------------------------------------------\n",
      "Date         | Trades | Daily PnL (Pts) | Status\n",
      "-----------------------------------------------------------------\n",
      "2025-09-01 | 7      |       4.90 pts   | PROFIT\n",
      "2025-09-02 | 10     |       1.20 pts   | PROFIT\n",
      "2025-09-03 | 5      |      32.70 pts   | PROFIT\n",
      "2025-09-04 | 1      |     -31.20 pts   | LOSS\n",
      "2025-09-05 | 12     |     -35.40 pts   | LOSS\n",
      "2025-09-08 | 5      |     -57.30 pts   | LOSS\n",
      "2025-09-09 | 6      |    -115.70 pts   | LOSS\n",
      "2025-09-10 | 2      |     -41.00 pts   | LOSS\n",
      "2025-09-11 | 5      |     -23.00 pts   | LOSS\n",
      "2025-09-12 | 12     |      30.30 pts   | PROFIT\n",
      "2025-09-18 | 2      |     -11.00 pts   | LOSS\n",
      "2025-09-22 | 1      |      29.00 pts   | PROFIT\n",
      "2025-09-23 | 5      |    -131.20 pts   | LOSS\n",
      "2025-09-25 | 8      |      59.70 pts   | PROFIT\n",
      "2025-10-03 | 0      |       0.00 pts   | FLAT\n",
      "2025-10-06 | 10     |     131.80 pts   | PROFIT\n",
      "2025-10-07 | 4      |     -82.90 pts   | LOSS\n",
      "2025-10-08 | 10     |     -52.60 pts   | LOSS\n",
      "2025-10-09 | 9      |      42.90 pts   | PROFIT\n",
      "2025-10-10 | 10     |     -10.20 pts   | LOSS\n",
      "2025-10-13 | 1      |     -31.50 pts   | LOSS\n",
      "2025-10-14 | 13     |      35.50 pts   | PROFIT\n",
      "2025-10-15 | 10     |      56.60 pts   | PROFIT\n",
      "2025-10-16 | 10     |     113.50 pts   | PROFIT\n",
      "2025-10-17 | 13     |      49.30 pts   | PROFIT\n",
      "2025-10-20 | 0      |       0.00 pts   | FLAT\n",
      "2025-10-23 | 4      |      85.70 pts   | PROFIT\n",
      "2025-10-24 | 11     |      22.30 pts   | PROFIT\n",
      "2025-10-27 | 8      |     -62.90 pts   | LOSS\n",
      "2025-10-28 | 15     |       6.40 pts   | PROFIT\n",
      "2025-10-31 | 0      |       0.00 pts   | FLAT\n",
      "2025-11-04 | 10     |      14.40 pts   | PROFIT\n",
      "2025-11-06 | 11     |      72.90 pts   | PROFIT\n",
      "2025-11-07 | 2      |     -64.30 pts   | LOSS\n",
      "2025-11-11 | 6      |      21.20 pts   | PROFIT\n",
      "2025-11-12 | 9      |      17.30 pts   | PROFIT\n",
      "2025-11-13 | 13     |     -62.70 pts   | LOSS\n",
      "2025-11-17 | 5      |     -13.80 pts   | LOSS\n",
      "2025-11-18 | 2      |     -62.50 pts   | LOSS\n",
      "2025-11-19 | 12     |      87.40 pts   | PROFIT\n",
      "2025-11-20 | 10     |      60.90 pts   | PROFIT\n",
      "2025-11-21 | 5      |       7.70 pts   | PROFIT\n",
      "2025-11-24 | 5      |      -2.70 pts   | LOSS\n",
      "2025-11-25 | 0      |       0.00 pts   | FLAT\n",
      "-----------------------------------------------------------------\n",
      "TOTAL RESULTS:\n",
      "Gross Points: 91.70\n",
      "Total Trades: 299\n",
      "Avg Pts/Day:  2.08\n",
      "Total INR:    ₹6,877.50\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from collections import deque\n",
    "from io import BytesIO\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "BUCKET = \"live-market-data\"\n",
    "SYMBOL = \"NIFTY\"\n",
    "\n",
    "START_DATE = datetime.date(2025, 9, 1)\n",
    "END_DATE = datetime.date(2025, 11, 26)\n",
    "\n",
    "# Expiry Dates 2025 (Thursdays)\n",
    "EXPIRY_SEP = datetime.date(2025, 9, 25)\n",
    "EXPIRY_OCT = datetime.date(2025, 10, 30)\n",
    "EXPIRY_NOV = datetime.date(2025, 11, 27)\n",
    "\n",
    "# Strategy Params\n",
    "KINETIC_THRESHOLD = 37500\n",
    "ORB_TIME = datetime.time(10, 0) # The Trap Closes at 10:00 AM\n",
    "MAX_HOLD_SECONDS = 1800         # 30 Mins Max Hold\n",
    "STOP_LOSS_POINTS = 30.0\n",
    "TAKE_PROFIT_POINTS = 90.0\n",
    "COST_PER_TRADE = 1.0            # Slippage + Comm per trade\n",
    "\n",
    "# ==========================================\n",
    "# 1. S3 & DATA UTILITIES\n",
    "# ==========================================\n",
    "def get_trading_symbol(current_date):\n",
    "    if current_date <= EXPIRY_SEP: return \"NIFTY25SEPFUT\"\n",
    "    elif current_date <= EXPIRY_OCT: return \"NIFTY25OCTFUT\"\n",
    "    else: return \"NIFTY25NOVFUT\"\n",
    "\n",
    "def get_data_for_date(date_obj):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    year = date_obj.year\n",
    "    month = date_obj.month\n",
    "    day = date_obj.day\n",
    "    ts = get_trading_symbol(date_obj)\n",
    "    \n",
    "    key = f\"year={year}/month={month:02d}/day={day:02d}/Futures/{SYMBOL}/{ts}.parquet\"\n",
    "    \n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=BUCKET, Key=key)\n",
    "        df = pd.read_parquet(BytesIO(obj[\"Body\"].read()))\n",
    "        \n",
    "        # Standardize Columns\n",
    "        if 'DateTime' not in df.columns:\n",
    "            df['DateTime'] = pd.to_datetime(\n",
    "                df['Date'].astype(str) + \" \" + df['Time'].astype(str), \n",
    "                dayfirst=True, errors='coerce'\n",
    "            )\n",
    "        \n",
    "        # Handle LTP aliases\n",
    "        col_map = {'LastTradedPrice': 'LTP', 'Close': 'LTP'}\n",
    "        df.rename(columns=col_map, inplace=True)\n",
    "        \n",
    "        # Handle Volume aliases (we need incremental volume if possible, or OpenInterest)\n",
    "        # Assuming Volume/OpenInterest column exists. \n",
    "        # If 'Volume' is cumulative, we handle diffing in the brain.\n",
    "        if 'Volume' not in df.columns:\n",
    "            if 'OpenInterest' in df.columns: df['Volume'] = df['OpenInterest']\n",
    "            elif 'LTQ' in df.columns: df['Volume'] = df['LTQ'] # Fallback\n",
    "            else: df['Volume'] = 0\n",
    "            \n",
    "        df = df.dropna(subset=['DateTime', 'LTP']).sort_values('DateTime').reset_index(drop=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# ==========================================\n",
    "# 2. KINETIC HUNTER BRAIN\n",
    "# ==========================================\n",
    "class KineticHunterBrain:\n",
    "    def __init__(self, threshold=37500):\n",
    "        self.threshold = threshold\n",
    "        self.tick_buffer = deque(maxlen=50)\n",
    "        \n",
    "    def get_signal(self, ltp, volume, orb_high, orb_low, current_time):\n",
    "        self.tick_buffer.append((ltp, volume))\n",
    "        \n",
    "        # 1. Check Time (Must be AFTER ORB)\n",
    "        if current_time <= ORB_TIME:\n",
    "            return 0 # Still building the trap\n",
    "            \n",
    "        if len(self.tick_buffer) < 50:\n",
    "            return 0\n",
    "            \n",
    "        # 2. Calculate Kinetic Energy\n",
    "        arr = np.array(self.tick_buffer)\n",
    "        prices = arr[:, 0]\n",
    "        vols = arr[:, 1]\n",
    "        \n",
    "        # We need volume flow. If input 'volume' is cumulative, take diff.\n",
    "        # If input is tick quantity (LTQ), take sum.\n",
    "        # Let's assume input stream sends what we have. \n",
    "        # Robust logic:\n",
    "        vol_diff = np.diff(vols)\n",
    "        # If vol_diff is mostly negative (resets), assume input was cumulative.\n",
    "        # If vol_diff is mostly positive/zero, it works.\n",
    "        # Safe approach: Just use positive deltas\n",
    "        vol_flow = np.where(vol_diff > 0, vol_diff, 0)\n",
    "        \n",
    "        kinetic_score = np.sum(vol_flow) / (abs(prices[-1] - prices[0]) + 0.05)\n",
    "        \n",
    "        # 3. Check Breakout + Kinetic\n",
    "        if kinetic_score > self.threshold:\n",
    "            if ltp > orb_high:\n",
    "                return 1 # Breakout Long\n",
    "            elif ltp < orb_low:\n",
    "                return -1 # Breakout Short\n",
    "        \n",
    "        return 0\n",
    "\n",
    "# ==========================================\n",
    "# 3. DAILY BACKTEST ENGINE\n",
    "# ==========================================\n",
    "def run_day(df):\n",
    "    brain = KineticHunterBrain(threshold=KINETIC_THRESHOLD)\n",
    "    \n",
    "    # ORB State\n",
    "    orb_high = -999999\n",
    "    orb_low = 999999\n",
    "    orb_locked = False\n",
    "    \n",
    "    # Trade State\n",
    "    in_trade = False\n",
    "    entry_price = 0.0\n",
    "    entry_time = None\n",
    "    direction = 0 # 1 Long, -1 Short\n",
    "    \n",
    "    trades = []\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        ts = row.DateTime\n",
    "        ltp = row.LTP\n",
    "        vol = row.Volume\n",
    "        curr_time = ts.time()\n",
    "        \n",
    "        # --- 1. ORB LOGIC ---\n",
    "        if curr_time <= ORB_TIME:\n",
    "            if ltp > orb_high: orb_high = ltp\n",
    "            if ltp < orb_low: orb_low = ltp\n",
    "            orb_locked = True # Mark that we are observing\n",
    "            brain.tick_buffer.append((ltp, vol)) # Keep buffer warm\n",
    "            continue\n",
    "            \n",
    "        # --- 2. TRADE MANAGEMENT ---\n",
    "        if in_trade:\n",
    "            # Check Exits\n",
    "            elapsed = (ts - entry_time).total_seconds()\n",
    "            pnl = 0\n",
    "            \n",
    "            if direction == 1: # Long\n",
    "                pnl = ltp - entry_price\n",
    "            else: # Short\n",
    "                pnl = entry_price - ltp\n",
    "                \n",
    "            exit_reason = None\n",
    "            if pnl <= -STOP_LOSS_POINTS: exit_reason = \"SL\"\n",
    "            elif pnl >= TAKE_PROFIT_POINTS: exit_reason = \"TP\"\n",
    "            elif elapsed >= MAX_HOLD_SECONDS: exit_reason = \"TIME\"\n",
    "            \n",
    "            if exit_reason:\n",
    "                trades.append(pnl - COST_PER_TRADE)\n",
    "                in_trade = False\n",
    "                direction = 0\n",
    "            continue # Don't re-enter on same tick\n",
    "            \n",
    "        # --- 3. SIGNAL GENERATION ---\n",
    "        # Only if ORB is valid\n",
    "        if orb_locked and orb_high > 0:\n",
    "            sig = brain.get_signal(ltp, vol, orb_high, orb_low, curr_time)\n",
    "            \n",
    "            if sig != 0:\n",
    "                in_trade = True\n",
    "                entry_price = ltp\n",
    "                entry_time = ts\n",
    "                direction = sig\n",
    "                \n",
    "    return trades\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN LOOP\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(f\"Running KINETIC HUNTER from {START_DATE} to {END_DATE}\")\n",
    "    print(f\"Trap: 09:15 - {ORB_TIME} | Trigger: Kinetic > {KINETIC_THRESHOLD}\")\n",
    "    print(\"-\" * 65)\n",
    "    print(f\"{'Date':<12} | {'Trades':<6} | {'Daily PnL (Pts)':<15} | {'Status'}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    current_date = START_DATE\n",
    "    total_points = 0.0\n",
    "    total_trades = 0\n",
    "    \n",
    "    date_pnl_map = []\n",
    "    \n",
    "    while current_date <= END_DATE:\n",
    "        # Skip weekends efficiently\n",
    "        if current_date.weekday() >= 5:\n",
    "            current_date += datetime.timedelta(days=1)\n",
    "            continue\n",
    "            \n",
    "        df = get_data_for_date(current_date)\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            daily_trades = run_day(df)\n",
    "            daily_pts = sum(daily_trades)\n",
    "            count = len(daily_trades)\n",
    "            \n",
    "            total_points += daily_pts\n",
    "            total_trades += count\n",
    "            \n",
    "            status = \"PROFIT\" if daily_pts > 0 else \"LOSS\" if daily_pts < 0 else \"FLAT\"\n",
    "            print(f\"{current_date} | {count:<6} | {daily_pts:>10.2f} pts   | {status}\")\n",
    "            \n",
    "            date_pnl_map.append({'Date': current_date, 'PnL': daily_pts, 'Trades': count})\n",
    "        else:\n",
    "            # print(f\"{current_date} | NO DATA\")\n",
    "            pass\n",
    "            \n",
    "        current_date += datetime.timedelta(days=1)\n",
    "        \n",
    "    print(\"-\" * 65)\n",
    "    print(f\"TOTAL RESULTS:\")\n",
    "    print(f\"Gross Points: {total_points:.2f}\")\n",
    "    print(f\"Total Trades: {total_trades}\")\n",
    "    print(f\"Avg Pts/Day:  {total_points / len(date_pnl_map) if date_pnl_map else 0:.2f}\")\n",
    "    \n",
    "    # Convert to rupees for context\n",
    "    print(f\"Total INR:    ₹{total_points * 75:,.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "315e2683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running IMBALANCE GATED SNIPER from 2025-09-01 to 2025-11-26\n",
      "Kinetic > 150000 | Imbalance > 0.25\n",
      "Stop: 20.0 | Target: 40.0 | Cooldown: 600s\n",
      "-----------------------------------------------------------------\n",
      "Date         | Trades | Daily PnL (Pts) | Status\n",
      "-----------------------------------------------------------------\n",
      "2025-09-01 | 5      |      63.40 pts   | PROFIT\n",
      "2025-09-02 | 8      |      22.10 pts   | PROFIT\n",
      "2025-09-03 | 3      |      28.00 pts   | PROFIT\n",
      "2025-09-04 | 11     |      80.70 pts   | PROFIT\n",
      "2025-09-05 | 9      |     -12.90 pts   | LOSS\n",
      "2025-09-08 | 8      |     -74.00 pts   | LOSS\n",
      "2025-09-09 | 4      |      -4.40 pts   | LOSS\n",
      "2025-09-10 | 5      |     -45.50 pts   | LOSS\n",
      "2025-09-11 | 3      |      23.00 pts   | PROFIT\n",
      "2025-09-12 | 4      |     -44.80 pts   | LOSS\n",
      "2025-09-18 | 6      |      57.00 pts   | PROFIT\n",
      "2025-09-22 | 6      |     -32.80 pts   | LOSS\n",
      "2025-09-23 | 8      |     -78.20 pts   | LOSS\n",
      "2025-09-25 | 9      |     148.70 pts   | PROFIT\n",
      "2025-10-03 | 2      |      -8.10 pts   | LOSS\n",
      "2025-10-06 | 9      |    -110.50 pts   | LOSS\n",
      "2025-10-07 | 8      |     -62.30 pts   | LOSS\n",
      "2025-10-08 | 9      |     132.70 pts   | PROFIT\n",
      "2025-10-09 | 7      |      49.70 pts   | PROFIT\n",
      "2025-10-10 | 5      |     -75.00 pts   | LOSS\n",
      "2025-10-13 | 8      |    -133.10 pts   | LOSS\n",
      "2025-10-14 | 11     |     -39.90 pts   | LOSS\n",
      "2025-10-15 | 8      |     -99.40 pts   | LOSS\n",
      "2025-10-16 | 7      |    -103.00 pts   | LOSS\n",
      "2025-10-17 | 10     |      31.30 pts   | PROFIT\n",
      "2025-10-20 | 8      |     -50.20 pts   | LOSS\n",
      "2025-10-23 | 11     |     -21.80 pts   | LOSS\n",
      "2025-10-24 | 9      |     -66.00 pts   | LOSS\n",
      "2025-10-27 | 12     |     -95.30 pts   | LOSS\n",
      "2025-10-28 | 11     |      69.80 pts   | PROFIT\n",
      "2025-10-31 | 6      |      76.40 pts   | PROFIT\n",
      "2025-11-04 | 7      |      -4.90 pts   | LOSS\n",
      "2025-11-06 | 9      |     -69.90 pts   | LOSS\n",
      "2025-11-07 | 12     |    -101.40 pts   | LOSS\n",
      "2025-11-11 | 9      |      27.80 pts   | PROFIT\n",
      "2025-11-12 | 8      |     -33.90 pts   | LOSS\n",
      "2025-11-13 | 9      |       4.40 pts   | PROFIT\n",
      "2025-11-17 | 7      |      92.90 pts   | PROFIT\n",
      "2025-11-18 | 8      |     -66.20 pts   | LOSS\n",
      "2025-11-19 | 10     |      -3.80 pts   | LOSS\n",
      "2025-11-20 | 8      |      -8.80 pts   | LOSS\n",
      "2025-11-21 | 9      |      16.60 pts   | PROFIT\n",
      "2025-11-24 | 11     |     -24.40 pts   | LOSS\n",
      "2025-11-25 | 4      |      19.60 pts   | PROFIT\n",
      "-----------------------------------------------------------------\n",
      "TOTAL RESULTS:\n",
      "Gross Points: -526.40\n",
      "Total Trades: 341\n",
      "Avg Pts/Day:  -11.96\n",
      "Total INR:    ₹-39,480.00\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from collections import deque\n",
    "from io import BytesIO\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "BUCKET = \"live-market-data\"\n",
    "SYMBOL = \"NIFTY\"\n",
    "\n",
    "START_DATE = datetime.date(2025, 9, 1)\n",
    "END_DATE = datetime.date(2025, 11, 26)\n",
    "\n",
    "# Expiry Dates 2025 (Thursdays)\n",
    "EXPIRY_SEP = datetime.date(2025, 9, 25)\n",
    "EXPIRY_OCT = datetime.date(2025, 10, 30)\n",
    "EXPIRY_NOV = datetime.date(2025, 11, 27)\n",
    "\n",
    "# Strategy Params: \"IMBALANCE GATED SNIPER\"\n",
    "KINETIC_THRESHOLD = 150000   \n",
    "IMBALANCE_THRESHOLD = 0.25   # Only trap if Order Book is skewed 25%\n",
    "TRAP_BUFFER = 5.0            \n",
    "PENDING_TIMEOUT = 60         # Faster timeout (1 min)\n",
    "COOLDOWN_SECONDS = 600       # 10 Mins\n",
    "\n",
    "# Exit Logic (Widened SL to breathe)\n",
    "STOP_LOSS_POINTS = 20.0      \n",
    "TAKE_PROFIT_POINTS = 40.0    # 1:2 Risk Reward\n",
    "\n",
    "# Costs\n",
    "COST_PER_TRADE = 1.0     \n",
    "LOT_SIZE = 75\n",
    "\n",
    "# ==========================================\n",
    "# 1. S3 & DATA UTILITIES\n",
    "# ==========================================\n",
    "def get_trading_symbol(current_date):\n",
    "    if current_date <= EXPIRY_SEP: return \"NIFTY25SEPFUT\"\n",
    "    elif current_date <= EXPIRY_OCT: return \"NIFTY25OCTFUT\"\n",
    "    else: return \"NIFTY25NOVFUT\"\n",
    "\n",
    "def get_data_for_date(date_obj):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    year = date_obj.year\n",
    "    month = date_obj.month\n",
    "    day = date_obj.day\n",
    "    ts = get_trading_symbol(date_obj)\n",
    "    \n",
    "    key = f\"year={year}/month={month:02d}/day={day:02d}/Futures/{SYMBOL}/{ts}.parquet\"\n",
    "    \n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=BUCKET, Key=key)\n",
    "        df = pd.read_parquet(BytesIO(obj[\"Body\"].read()))\n",
    "        \n",
    "        if 'DateTime' not in df.columns:\n",
    "            df['DateTime'] = pd.to_datetime(\n",
    "                df['Date'].astype(str) + \" \" + df['Time'].astype(str), \n",
    "                dayfirst=True, errors='coerce'\n",
    "            )\n",
    "        \n",
    "        col_map = {\n",
    "            'LastTradedPrice': 'LTP', 'Close': 'LTP',\n",
    "            'BestBuyQty': 'BidSize', 'BestSellQty': 'AskSize', # Map for Imbalance\n",
    "            'BuyQty': 'BidSize', 'SellQty': 'AskSize' # Alt names\n",
    "        }\n",
    "        df.rename(columns=col_map, inplace=True)\n",
    "        \n",
    "        # Ensure we have Volume\n",
    "        if 'Volume' not in df.columns:\n",
    "            if 'OpenInterest' in df.columns: df['Volume'] = df['OpenInterest']\n",
    "            elif 'LTQ' in df.columns: df['Volume'] = df['LTQ'] \n",
    "            else: df['Volume'] = 0\n",
    "            \n",
    "        # Ensure we have Bid/Ask Size\n",
    "        if 'BidSize' not in df.columns: df['BidSize'] = 0\n",
    "        if 'AskSize' not in df.columns: df['AskSize'] = 0\n",
    "            \n",
    "        df = df.dropna(subset=['DateTime', 'LTP']).sort_values('DateTime').reset_index(drop=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# ==========================================\n",
    "# 2. KINETIC BRAIN + IMBALANCE\n",
    "# ==========================================\n",
    "class KineticBrain:\n",
    "    def __init__(self, threshold=37500):\n",
    "        self.threshold = threshold\n",
    "        self.tick_buffer = deque(maxlen=50)\n",
    "        \n",
    "    def get_signal_setup(self, ltp, volume, bid_size, ask_size):\n",
    "        self.tick_buffer.append((ltp, volume))\n",
    "        \n",
    "        if len(self.tick_buffer) < 50:\n",
    "            return None # Not ready\n",
    "            \n",
    "        arr = np.array(self.tick_buffer)\n",
    "        prices = arr[:, 0]\n",
    "        vols = arr[:, 1]\n",
    "        \n",
    "        vol_diff = np.diff(vols)\n",
    "        vol_flow = np.where(vol_diff > 0, vol_diff, 0)\n",
    "        kinetic_score = np.sum(vol_flow) / (abs(prices[-1] - prices[0]) + 0.05)\n",
    "        \n",
    "        if kinetic_score > self.threshold:\n",
    "            # Check Imbalance\n",
    "            # (Bids - Asks) / (Bids + Asks)\n",
    "            # Positive = Buying Pressure, Negative = Selling Pressure\n",
    "            total_size = bid_size + ask_size + 1e-9\n",
    "            imbalance = (bid_size - ask_size) / total_size\n",
    "            \n",
    "            return imbalance\n",
    "            \n",
    "        return None\n",
    "\n",
    "# ==========================================\n",
    "# 3. DAILY BACKTEST ENGINE\n",
    "# ==========================================\n",
    "def run_day(df):\n",
    "    brain = KineticBrain(threshold=KINETIC_THRESHOLD)\n",
    "    \n",
    "    state = 'SEARCHING'\n",
    "    \n",
    "    # Pending Order\n",
    "    trigger_price = 0.0\n",
    "    pending_direction = 0 # 1 Long, -1 Short\n",
    "    pending_start_time = None\n",
    "    \n",
    "    # Active Trade\n",
    "    entry_price = 0.0\n",
    "    target_price = 0.0\n",
    "    stop_price = 0.0\n",
    "    active_direction = 0\n",
    "    trade_start_time = None\n",
    "    \n",
    "    cooldown_start = None\n",
    "    \n",
    "    trades = []\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        ts = row.DateTime\n",
    "        ltp = row.LTP\n",
    "        vol = row.Volume\n",
    "        bid = row.BidSize\n",
    "        ask = row.AskSize\n",
    "        \n",
    "        # --- STATE 4: COOLDOWN ---\n",
    "        if state == 'COOLDOWN':\n",
    "            if (ts - cooldown_start).total_seconds() > COOLDOWN_SECONDS:\n",
    "                state = 'SEARCHING'\n",
    "                brain.tick_buffer.clear()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # --- STATE 1: SEARCHING ---\n",
    "        if state == 'SEARCHING':\n",
    "            imbalance = brain.get_signal_setup(ltp, vol, bid, ask)\n",
    "            \n",
    "            if imbalance is not None:\n",
    "                # ONE-SIDED TRAP ONLY\n",
    "                if imbalance > IMBALANCE_THRESHOLD:\n",
    "                    # Buyers Dominant -> Trap Highs Only\n",
    "                    state = 'PENDING'\n",
    "                    trigger_price = ltp + TRAP_BUFFER\n",
    "                    pending_direction = 1 # Long\n",
    "                    pending_start_time = ts\n",
    "                    \n",
    "                elif imbalance < -IMBALANCE_THRESHOLD:\n",
    "                    # Sellers Dominant -> Trap Lows Only\n",
    "                    state = 'PENDING'\n",
    "                    trigger_price = ltp - TRAP_BUFFER\n",
    "                    pending_direction = -1 # Short\n",
    "                    pending_start_time = ts\n",
    "                \n",
    "        # --- STATE 2: PENDING ---\n",
    "        elif state == 'PENDING':\n",
    "            if (ts - pending_start_time).total_seconds() > PENDING_TIMEOUT:\n",
    "                state = 'SEARCHING'\n",
    "                continue\n",
    "                \n",
    "            # Check Trigger\n",
    "            triggered = False\n",
    "            if pending_direction == 1 and ltp >= trigger_price:\n",
    "                triggered = True\n",
    "            elif pending_direction == -1 and ltp <= trigger_price:\n",
    "                triggered = True\n",
    "                \n",
    "            if triggered:\n",
    "                state = 'IN_TRADE'\n",
    "                active_direction = pending_direction\n",
    "                entry_price = ltp # Slippage applied at exit cost\n",
    "                trade_start_time = ts\n",
    "                \n",
    "                if active_direction == 1:\n",
    "                    target_price = entry_price + TAKE_PROFIT_POINTS\n",
    "                    stop_price = entry_price - STOP_LOSS_POINTS\n",
    "                else:\n",
    "                    target_price = entry_price - TAKE_PROFIT_POINTS\n",
    "                    stop_price = entry_price + STOP_LOSS_POINTS\n",
    "                \n",
    "        # --- STATE 3: IN TRADE ---\n",
    "        elif state == 'IN_TRADE':\n",
    "            exit_pnl = 0\n",
    "            exited = False\n",
    "            \n",
    "            # 1. Price Targets\n",
    "            if active_direction == 1: # Long\n",
    "                if ltp >= target_price:\n",
    "                    exit_pnl = ltp - entry_price; exited = True\n",
    "                elif ltp <= stop_price:\n",
    "                    exit_pnl = ltp - entry_price; exited = True\n",
    "            else: # Short\n",
    "                if ltp <= target_price:\n",
    "                    exit_pnl = entry_price - ltp; exited = True\n",
    "                elif ltp >= stop_price:\n",
    "                    exit_pnl = entry_price - ltp; exited = True\n",
    "            \n",
    "            # 2. Time Stop (Safety Valve) No trade should last > 30 mins\n",
    "            if not exited and (ts - trade_start_time).total_seconds() > 1800:\n",
    "                if active_direction == 1: exit_pnl = ltp - entry_price\n",
    "                else: exit_pnl = entry_price - ltp\n",
    "                exited = True\n",
    "            \n",
    "            if exited:\n",
    "                trades.append(exit_pnl - COST_PER_TRADE)\n",
    "                state = 'COOLDOWN'\n",
    "                cooldown_start = ts\n",
    "                \n",
    "    return trades\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN LOOP\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(f\"Running IMBALANCE GATED SNIPER from {START_DATE} to {END_DATE}\")\n",
    "    print(f\"Kinetic > {KINETIC_THRESHOLD} | Imbalance > {IMBALANCE_THRESHOLD}\")\n",
    "    print(f\"Stop: {STOP_LOSS_POINTS} | Target: {TAKE_PROFIT_POINTS} | Cooldown: {COOLDOWN_SECONDS}s\")\n",
    "    print(\"-\" * 65)\n",
    "    print(f\"{'Date':<12} | {'Trades':<6} | {'Daily PnL (Pts)':<15} | {'Status'}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    current_date = START_DATE\n",
    "    total_points = 0.0\n",
    "    total_trades = 0\n",
    "    date_pnl_map = []\n",
    "    \n",
    "    while current_date <= END_DATE:\n",
    "        if current_date.weekday() >= 5:\n",
    "            current_date += datetime.timedelta(days=1)\n",
    "            continue\n",
    "            \n",
    "        df = get_data_for_date(current_date)\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            daily_trades = run_day(df)\n",
    "            daily_pts = sum(daily_trades)\n",
    "            count = len(daily_trades)\n",
    "            \n",
    "            total_points += daily_pts\n",
    "            total_trades += count\n",
    "            \n",
    "            status = \"PROFIT\" if daily_pts > 0 else \"LOSS\" if daily_pts < 0 else \"FLAT\"\n",
    "            print(f\"{current_date} | {count:<6} | {daily_pts:>10.2f} pts   | {status}\")\n",
    "            date_pnl_map.append({'Date': current_date, 'PnL': daily_pts})\n",
    "        \n",
    "        current_date += datetime.timedelta(days=1)\n",
    "        \n",
    "    print(\"-\" * 65)\n",
    "    print(f\"TOTAL RESULTS:\")\n",
    "    print(f\"Gross Points: {total_points:.2f}\")\n",
    "    print(f\"Total Trades: {total_trades}\")\n",
    "    print(f\"Avg Pts/Day:  {total_points / len(date_pnl_map) if date_pnl_map else 0:.2f}\")\n",
    "    print(f\"Total INR:    ₹{total_points * LOT_SIZE:,.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9aa3aec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "AWS KINETIC IRON CONDOR - 11/2025\n",
      "============================================================\n",
      "IC Setup Cost: 16.0 points\n",
      "Expected Move: 20 points\n",
      "============================================================\n",
      "\n",
      "01/11/2025... No data\n",
      "02/11/2025... No data\n",
      "03/11/2025... No data\n",
      "04/11/2025... Trades: 12 (0W/12L) | PnL: ₹-28,594\n",
      "05/11/2025... No data\n",
      "06/11/2025... Trades: 12 (0W/12L) | PnL: ₹-31,150\n",
      "07/11/2025... Trades: 12 (0W/12L) | PnL: ₹-28,576\n",
      "08/11/2025... No data\n",
      "09/11/2025... No data\n",
      "10/11/2025... No data\n",
      "11/11/2025... Trades: 12 (0W/12L) | PnL: ₹-31,120\n",
      "12/11/2025... Trades: 12 (0W/12L) | PnL: ₹-28,543\n",
      "13/11/2025... Trades: 12 (0W/12L) | PnL: ₹-28,535\n",
      "14/11/2025... No data\n",
      "15/11/2025... No data\n",
      "16/11/2025... No data\n",
      "17/11/2025... Trades: 12 (0W/12L) | PnL: ₹-28,483\n",
      "18/11/2025... Trades: 12 (0W/12L) | PnL: ₹-28,467\n",
      "19/11/2025... Trades: 13 (0W/13L) | PnL: ₹-30,815\n",
      "20/11/2025... Trades: 12 (0W/12L) | PnL: ₹-28,421\n",
      "21/11/2025... Trades: 12 (0W/12L) | PnL: ₹-28,392\n",
      "22/11/2025... No data\n",
      "23/11/2025... No data\n",
      "24/11/2025... Trades: 12 (0W/12L) | PnL: ₹-27,277\n",
      "25/11/2025... Trades: 5 (0W/5L) | PnL: ₹-10,584\n",
      "26/11/2025... No data\n",
      "27/11/2025... No data\n",
      "28/11/2025... No data\n",
      "29/11/2025... No data\n",
      "30/11/2025... No data\n",
      "\n",
      "============================================================\n",
      "MONTHLY IRON CONDOR RESULTS - 11/2025\n",
      "============================================================\n",
      "Total Trades:       150\n",
      "Winners:            0 (0.0%)\n",
      "Losers:             150 (100.0%)\n",
      "Avg Loss:           -31.91 pts (₹-2,393)\n",
      "\n",
      "Total PnL:          -4786.10 pts (₹-358,957.31)\n",
      "Avg/Trade:          -31.91 pts (₹-2,393.05)\n",
      "\n",
      "Outcome Distribution:\n",
      "Outcome\n",
      "WIN         86\n",
      "PARTIAL     62\n",
      "MAX_LOSS     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Results saved: data/nov_2025_iron_condor_results.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AWS KINETIC IRON CONDOR - COMPLETE SYSTEM\n",
    "==========================================\n",
    "Integrates with S3 data pipeline\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "from collections import deque\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "BUCKET = \"live-market-data\"\n",
    "YEAR = 2025\n",
    "MONTH = 11\n",
    "SYMBOL = \"NIFTY\"\n",
    "FUT_TS = \"NIFTY25NOVFUT\"\n",
    "\n",
    "# Strategy\n",
    "LOT_SIZE = 75\n",
    "KINETIC_THRESHOLD = 37500\n",
    "HOLD_SECONDS = 1800  # 30 minutes\n",
    "\n",
    "# IC Parameters\n",
    "EXPECTED_MOVE = 20\n",
    "WING_WIDTH = 50\n",
    "IC_DISTANCE = 25\n",
    "\n",
    "# Costs\n",
    "OPTION_ENTRY_COST = 2.0\n",
    "OPTION_EXIT_COST = 2.0\n",
    "IC_TOTAL_COST = 4 * (OPTION_ENTRY_COST + OPTION_EXIT_COST)  # 16 pts\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"AWS KINETIC IRON CONDOR - {MONTH}/{YEAR}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"IC Setup Cost: {IC_TOTAL_COST} points\")\n",
    "print(f\"Expected Move: {EXPECTED_MOVE} points\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# S3 UTILITIES (Same as before)\n",
    "# ==========================================\n",
    "def download_parquet_to_path(key: str, local_path: str):\n",
    "    if os.path.exists(local_path):\n",
    "        return True\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=BUCKET, Key=key)\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            f.write(obj[\"Body\"].read())\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# BLACK-SCHOLES\n",
    "# ==========================================\n",
    "def bs_call_price(S, K, T, sigma, r=0.0):\n",
    "    if T <= 0 or sigma <= 0:\n",
    "        return max(S - K, 0.0)\n",
    "    d1 = (np.log(S/K) + (r + 0.5*sigma**2)*T) / (sigma*np.sqrt(T))\n",
    "    d2 = d1 - sigma*np.sqrt(T)\n",
    "    return S*norm.cdf(d1) - K*np.exp(-r*T)*norm.cdf(d2)\n",
    "\n",
    "def bs_put_price(S, K, T, sigma, r=0.0):\n",
    "    if T <= 0 or sigma <= 0:\n",
    "        return max(K - S, 0.0)\n",
    "    d1 = (np.log(S/K) + (r + 0.5*sigma**2)*T) / (sigma*np.sqrt(T))\n",
    "    d2 = d1 - sigma*np.sqrt(T)\n",
    "    return K*np.exp(-r*T)*norm.cdf(-d2) - S*norm.cdf(-d1)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# KINETIC BRAIN\n",
    "# ==========================================\n",
    "class KineticBrainMagnitude:\n",
    "    def __init__(self, threshold=37500, hold_seconds=1800):\n",
    "        self.threshold = threshold\n",
    "        self.hold_seconds = hold_seconds\n",
    "        self.buffer = deque(maxlen=50)\n",
    "        self.in_trade = False\n",
    "        self.entry_time = None\n",
    "        self.last_score = 0\n",
    "    \n",
    "    def tick(self, ts, ltp, volume):\n",
    "        self.buffer.append((ltp, volume))\n",
    "        \n",
    "        if len(self.buffer) < 50:\n",
    "            return 0\n",
    "        \n",
    "        if self.in_trade:\n",
    "            if (ts - self.entry_time).total_seconds() >= self.hold_seconds:\n",
    "                self.in_trade = False\n",
    "                return -1\n",
    "            return 0\n",
    "        \n",
    "        arr = np.array(self.buffer)\n",
    "        prices, vols = arr[:, 0], arr[:, 1]\n",
    "        \n",
    "        vol_diff = np.diff(vols)\n",
    "        traded_vol = np.sum(np.where(vol_diff > 0, vol_diff, 0))\n",
    "        displacement = abs(prices[-1] - prices[0])\n",
    "        \n",
    "        score = traded_vol / (displacement + 0.05)\n",
    "        self.last_score = score\n",
    "        \n",
    "        if score > self.threshold:\n",
    "            self.in_trade = True\n",
    "            self.entry_time = ts\n",
    "            return 1\n",
    "        \n",
    "        return 0\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# IRON CONDOR POSITION\n",
    "# ==========================================\n",
    "class IronCondorPosition:\n",
    "    def __init__(self, entry_time, spot, T, sigma):\n",
    "        self.entry_time = entry_time\n",
    "        self.entry_spot = spot\n",
    "        \n",
    "        self.atm = round(spot / 50) * 50\n",
    "        \n",
    "        self.short_call_strike = self.atm + IC_DISTANCE\n",
    "        self.long_call_strike = self.atm + IC_DISTANCE + WING_WIDTH\n",
    "        self.short_put_strike = self.atm - IC_DISTANCE\n",
    "        self.long_put_strike = self.atm - IC_DISTANCE - WING_WIDTH\n",
    "        \n",
    "        self.entry_premium = self._calculate_ic_value(spot, T, sigma)\n",
    "        self.net_entry = self.entry_premium - IC_TOTAL_COST\n",
    "    \n",
    "    def _calculate_ic_value(self, S, T, sigma):\n",
    "        call_spread = (bs_call_price(S, self.short_call_strike, T, sigma) -\n",
    "                      bs_call_price(S, self.long_call_strike, T, sigma))\n",
    "        put_spread = (bs_put_price(S, self.short_put_strike, T, sigma) -\n",
    "                     bs_put_price(S, self.long_put_strike, T, sigma))\n",
    "        return call_spread + put_spread\n",
    "    \n",
    "    def calculate_exit_pnl(self, exit_time, exit_spot, T_exit, sigma_exit):\n",
    "        exit_value = self._calculate_ic_value(exit_spot, T_exit, sigma_exit)\n",
    "        option_pnl = self.net_entry - (exit_value + IC_TOTAL_COST)\n",
    "        \n",
    "        move_from_atm = exit_spot - self.atm\n",
    "        \n",
    "        if abs(move_from_atm) < IC_DISTANCE:\n",
    "            outcome = \"WIN\"\n",
    "        elif abs(move_from_atm) > IC_DISTANCE + WING_WIDTH:\n",
    "            outcome = \"MAX_LOSS\"\n",
    "            option_pnl = -(WING_WIDTH + IC_TOTAL_COST)\n",
    "        else:\n",
    "            outcome = \"PARTIAL\"\n",
    "        \n",
    "        return {\n",
    "            'Entry_Time': self.entry_time,\n",
    "            'Exit_Time': exit_time,\n",
    "            'Entry_Spot': self.entry_spot,\n",
    "            'Exit_Spot': exit_spot,\n",
    "            'Move': move_from_atm,\n",
    "            'Entry_Premium': self.entry_premium,\n",
    "            'Exit_Value': exit_value,\n",
    "            'Option_PnL_Pts': option_pnl,\n",
    "            'Option_PnL_INR': option_pnl * LOT_SIZE,\n",
    "            'Outcome': outcome,\n",
    "            'ATM': self.atm\n",
    "        }\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# DATA LOADING\n",
    "# ==========================================\n",
    "def download_futures_for_day(year, month, day, day_folder):\n",
    "    key = f\"year={year}/month={month:02d}/day={day:02d}/Futures/{SYMBOL}/{FUT_TS}.parquet\"\n",
    "    local_path = os.path.join(day_folder, \"FUT.parquet\")\n",
    "    \n",
    "    if not download_parquet_to_path(key, local_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_parquet(local_path)\n",
    "        df[\"DateTime\"] = pd.to_datetime(\n",
    "            df[\"Date\"].astype(str) + \" \" + df[\"Time\"].astype(str),\n",
    "            dayfirst=True,\n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "        df[\"LTP\"] = pd.to_numeric(df[\"LTP\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"DateTime\", \"LTP\"]).sort_values(\"DateTime\")\n",
    "        \n",
    "        if \"Volume\" in df.columns:\n",
    "            df[\"Volume\"] = pd.to_numeric(df[\"Volume\"], errors=\"coerce\")\n",
    "        else:\n",
    "            df[\"Volume\"] = 0.0\n",
    "        \n",
    "        csv_path = os.path.join(day_folder, \"FUT.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        return csv_path\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# BACKTEST FOR ONE DAY\n",
    "# ==========================================\n",
    "def run_daily_ic_backtest(fut_csv, expiry_date, base_iv=0.15):\n",
    "    df = pd.read_csv(fut_csv)\n",
    "    df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "    \n",
    "    brain = KineticBrainMagnitude(threshold=KINETIC_THRESHOLD, hold_seconds=HOLD_SECONDS)\n",
    "    \n",
    "    trades = []\n",
    "    active_ic = None\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        ts = row['DateTime']\n",
    "        ltp = float(row['LTP'])\n",
    "        vol = float(row['Volume'])\n",
    "        \n",
    "        signal = brain.tick(ts, ltp, vol)\n",
    "        \n",
    "        if signal == 1:\n",
    "            T = (expiry_date - ts).total_seconds() / (365 * 24 * 3600)\n",
    "            T = max(T, 0.01)\n",
    "            active_ic = IronCondorPosition(ts, ltp, T, base_iv)\n",
    "        \n",
    "        elif signal == -1 and active_ic:\n",
    "            T_exit = (expiry_date - ts).total_seconds() / (365 * 24 * 3600)\n",
    "            T_exit = max(T_exit, 0.001)\n",
    "            \n",
    "            # IV crush after move\n",
    "            exit_iv = base_iv * 0.90\n",
    "            \n",
    "            result = active_ic.calculate_exit_pnl(ts, ltp, T_exit, exit_iv)\n",
    "            trades.append(result)\n",
    "            active_ic = None\n",
    "    \n",
    "    return pd.DataFrame(trades)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# MONTHLY EXECUTION\n",
    "# ==========================================\n",
    "def main():\n",
    "    all_trades = []\n",
    "    expiry_date = pd.Timestamp(f\"{YEAR}-{MONTH}-27\")  # Assume last Thursday\n",
    "    \n",
    "    for day in range(1, 31):\n",
    "        day_folder = f\"data/{YEAR}-{MONTH:02d}-{day:02d}/\"\n",
    "        os.makedirs(day_folder, exist_ok=True)\n",
    "        \n",
    "        print(f\"{day:02d}/{MONTH}/{YEAR}...\", end=\" \")\n",
    "        \n",
    "        fut_csv = download_futures_for_day(YEAR, MONTH, day, day_folder)\n",
    "        \n",
    "        if not fut_csv:\n",
    "            print(\"No data\")\n",
    "            continue\n",
    "        \n",
    "        trades_df = run_daily_ic_backtest(fut_csv, expiry_date, base_iv=0.15)\n",
    "        \n",
    "        if not trades_df.empty:\n",
    "            daily_pnl = trades_df['Option_PnL_INR'].sum()\n",
    "            n_trades = len(trades_df)\n",
    "            wins = len(trades_df[trades_df['Option_PnL_Pts'] > 0])\n",
    "            \n",
    "            print(f\"Trades: {n_trades} ({wins}W/{n_trades-wins}L) | PnL: ₹{daily_pnl:,.0f}\")\n",
    "            \n",
    "            daily_csv = os.path.join(day_folder, \"ic_results.csv\")\n",
    "            trades_df.to_csv(daily_csv, index=False)\n",
    "            \n",
    "            all_trades.append(trades_df)\n",
    "        else:\n",
    "            print(\"No signals\")\n",
    "    \n",
    "    # Monthly summary\n",
    "    if all_trades:\n",
    "        full_df = pd.concat(all_trades, ignore_index=True)\n",
    "        \n",
    "        total_pnl = full_df['Option_PnL_INR'].sum()\n",
    "        avg_pnl = full_df['Option_PnL_Pts'].mean()\n",
    "        win_rate = (full_df['Option_PnL_Pts'] > 0).mean() * 100\n",
    "        \n",
    "        winners = full_df[full_df['Option_PnL_Pts'] > 0]\n",
    "        losers = full_df[full_df['Option_PnL_Pts'] <= 0]\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"MONTHLY IRON CONDOR RESULTS - {MONTH}/{YEAR}\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total Trades:       {len(full_df)}\")\n",
    "        print(f\"Winners:            {len(winners)} ({win_rate:.1f}%)\")\n",
    "        print(f\"Losers:             {len(losers)} ({100-win_rate:.1f}%)\")\n",
    "        \n",
    "        if len(winners) > 0:\n",
    "            print(f\"\\nAvg Win:            {winners['Option_PnL_Pts'].mean():.2f} pts (₹{winners['Option_PnL_INR'].mean():,.0f})\")\n",
    "        if len(losers) > 0:\n",
    "            print(f\"Avg Loss:           {losers['Option_PnL_Pts'].mean():.2f} pts (₹{losers['Option_PnL_INR'].mean():,.0f})\")\n",
    "        \n",
    "        print(f\"\\nTotal PnL:          {total_pnl/LOT_SIZE:.2f} pts (₹{total_pnl:,.2f})\")\n",
    "        print(f\"Avg/Trade:          {avg_pnl:.2f} pts (₹{avg_pnl*LOT_SIZE:,.2f})\")\n",
    "        \n",
    "        # Expectancy\n",
    "        if len(winners) > 0 and len(losers) > 0:\n",
    "            expectancy = (win_rate/100 * winners['Option_PnL_Pts'].mean() +\n",
    "                         (100-win_rate)/100 * losers['Option_PnL_Pts'].mean())\n",
    "            print(f\"Expectancy:         {expectancy:.2f} pts/trade\")\n",
    "        \n",
    "        print(\"\\nOutcome Distribution:\")\n",
    "        print(full_df['Outcome'].value_counts())\n",
    "        \n",
    "        final_csv = f\"data/nov_{YEAR}_iron_condor_results.csv\"\n",
    "        full_df.to_csv(final_csv, index=False)\n",
    "        print(f\"\\nResults saved: {final_csv}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nNo trades executed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41b4228a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ANCHORED KINETIC from 2025-09-01 to 2025-11-26\n",
      "Trigger: Kinetic > 37500 | Anchor: 5000 ticks\n",
      "Stop: 30.0 | Target: 60.0\n",
      "-----------------------------------------------------------------\n",
      "Date         | Trades | Daily PnL (Pts) | Status\n",
      "-----------------------------------------------------------------\n",
      "2025-09-01 | 17     |     -12.20 pts   | LOSS\n",
      "2025-09-02 | 18     |     -22.90 pts   | LOSS\n",
      "2025-09-03 | 13     |     -22.30 pts   | LOSS\n",
      "2025-09-04 | 18     |      21.50 pts   | PROFIT\n",
      "2025-09-05 | 21     |      40.80 pts   | PROFIT\n",
      "2025-09-08 | 17     |     103.90 pts   | PROFIT\n",
      "2025-09-09 | 17     |    -246.30 pts   | LOSS\n",
      "2025-09-10 | 17     |     -74.70 pts   | LOSS\n",
      "2025-09-11 | 14     |     -79.50 pts   | LOSS\n",
      "2025-09-12 | 18     |     -77.40 pts   | LOSS\n",
      "2025-09-18 | 16     |     -25.70 pts   | LOSS\n",
      "2025-09-22 | 15     |      90.80 pts   | PROFIT\n",
      "2025-09-23 | 19     |     213.70 pts   | PROFIT\n",
      "2025-09-25 | 17     |     -30.20 pts   | LOSS\n",
      "2025-10-03 | 9      |    -100.90 pts   | LOSS\n",
      "2025-10-06 | 18     |      -2.90 pts   | LOSS\n",
      "2025-10-07 | 19     |      87.00 pts   | PROFIT\n",
      "2025-10-08 | 20     |      92.70 pts   | PROFIT\n",
      "2025-10-09 | 17     |     -53.00 pts   | LOSS\n",
      "2025-10-10 | 17     |     113.40 pts   | PROFIT\n",
      "2025-10-13 | 17     |     -57.30 pts   | LOSS\n",
      "2025-10-14 | 20     |     -34.00 pts   | LOSS\n",
      "2025-10-15 | 17     |     -14.00 pts   | LOSS\n",
      "2025-10-16 | 20     |     -23.60 pts   | LOSS\n",
      "2025-10-17 | 23     |     146.70 pts   | PROFIT\n",
      "2025-10-20 | 18     |     -78.20 pts   | LOSS\n",
      "2025-10-23 | 21     |      -4.00 pts   | LOSS\n",
      "2025-10-24 | 20     |      62.40 pts   | PROFIT\n",
      "2025-10-27 | 20     |    -160.80 pts   | LOSS\n",
      "2025-10-28 | 22     |      -1.70 pts   | LOSS\n",
      "2025-10-31 | 14     |    -119.50 pts   | LOSS\n",
      "2025-11-04 | 17     |     -66.00 pts   | LOSS\n",
      "2025-11-06 | 19     |      78.50 pts   | PROFIT\n",
      "2025-11-07 | 21     |      31.90 pts   | PROFIT\n",
      "2025-11-11 | 19     |     144.60 pts   | PROFIT\n",
      "2025-11-12 | 19     |    -218.20 pts   | LOSS\n",
      "2025-11-13 | 21     |     -25.20 pts   | LOSS\n",
      "2025-11-17 | 19     |    -191.70 pts   | LOSS\n",
      "2025-11-18 | 19     |     -66.80 pts   | LOSS\n",
      "2025-11-19 | 21     |     -88.90 pts   | LOSS\n",
      "2025-11-20 | 19     |     -68.10 pts   | LOSS\n",
      "2025-11-21 | 20     |     -14.20 pts   | LOSS\n",
      "2025-11-24 | 18     |      15.20 pts   | PROFIT\n",
      "2025-11-25 | 8      |      72.10 pts   | PROFIT\n",
      "-----------------------------------------------------------------\n",
      "TOTAL RESULTS:\n",
      "Gross Points: -665.00\n",
      "Total Trades: 789\n",
      "Avg Pts/Day:  -15.11\n",
      "Total INR:    ₹-49,875.00\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from collections import deque\n",
    "from io import BytesIO\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "BUCKET = \"live-market-data\"\n",
    "SYMBOL = \"NIFTY\"\n",
    "\n",
    "START_DATE = datetime.date(2025, 9, 1)\n",
    "END_DATE = datetime.date(2025, 11, 26)\n",
    "\n",
    "# Expiry Dates 2025\n",
    "EXPIRY_SEP = datetime.date(2025, 9, 25)\n",
    "EXPIRY_OCT = datetime.date(2025, 10, 30)\n",
    "EXPIRY_NOV = datetime.date(2025, 11, 27)\n",
    "\n",
    "# Strategy Params: \"ANCHORED KINETIC\"\n",
    "KINETIC_THRESHOLD = 37500    \n",
    "ANCHOR_PERIOD = 5000         # 5000 Ticks (~2 Hours Trend)\n",
    "MAX_HOLD_SECONDS = 1200      # 20 Mins\n",
    "\n",
    "# Exit Logic\n",
    "STOP_LOSS_POINTS = 30.0      \n",
    "TAKE_PROFIT_POINTS = 60.0    # 1:2 Risk Reward\n",
    "\n",
    "# Costs\n",
    "COST_PER_TRADE = 1.0     \n",
    "LOT_SIZE = 75\n",
    "\n",
    "# ==========================================\n",
    "# 1. S3 & DATA UTILITIES\n",
    "# ==========================================\n",
    "def get_trading_symbol(current_date):\n",
    "    if current_date <= EXPIRY_SEP: return \"NIFTY25SEPFUT\"\n",
    "    elif current_date <= EXPIRY_OCT: return \"NIFTY25OCTFUT\"\n",
    "    else: return \"NIFTY25NOVFUT\"\n",
    "\n",
    "def get_data_for_date(date_obj):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    year = date_obj.year\n",
    "    month = date_obj.month\n",
    "    day = date_obj.day\n",
    "    ts = get_trading_symbol(date_obj)\n",
    "    \n",
    "    key = f\"year={year}/month={month:02d}/day={day:02d}/Futures/{SYMBOL}/{ts}.parquet\"\n",
    "    \n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=BUCKET, Key=key)\n",
    "        df = pd.read_parquet(BytesIO(obj[\"Body\"].read()))\n",
    "        \n",
    "        if 'DateTime' not in df.columns:\n",
    "            df['DateTime'] = pd.to_datetime(\n",
    "                df['Date'].astype(str) + \" \" + df['Time'].astype(str), \n",
    "                dayfirst=True, errors='coerce'\n",
    "            )\n",
    "        \n",
    "        col_map = {'LastTradedPrice': 'LTP', 'Close': 'LTP'}\n",
    "        df.rename(columns=col_map, inplace=True)\n",
    "        \n",
    "        if 'Volume' not in df.columns:\n",
    "            if 'OpenInterest' in df.columns: df['Volume'] = df['OpenInterest']\n",
    "            elif 'LTQ' in df.columns: df['Volume'] = df['LTQ'] \n",
    "            else: df['Volume'] = 0\n",
    "            \n",
    "        df = df.dropna(subset=['DateTime', 'LTP']).sort_values('DateTime').reset_index(drop=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# ==========================================\n",
    "# 2. ANCHORED KINETIC BRAIN\n",
    "# ==========================================\n",
    "class AnchoredKineticBrain:\n",
    "    def __init__(self, threshold=37500, anchor_period=5000):\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # Micro Buffer (Kinetic Detection)\n",
    "        self.micro_buffer = deque(maxlen=50)\n",
    "        \n",
    "        # Macro Buffer (The Anchor Trend)\n",
    "        self.anchor_prices = deque(maxlen=anchor_period)\n",
    "        self.anchor_vols = deque(maxlen=anchor_period)\n",
    "        \n",
    "    def get_signal(self, ltp, volume):\n",
    "        # Update Buffers\n",
    "        self.micro_buffer.append((ltp, volume))\n",
    "        self.anchor_prices.append(ltp)\n",
    "        self.anchor_vols.append(volume)\n",
    "        \n",
    "        if len(self.micro_buffer) < 50 or len(self.anchor_prices) < 100:\n",
    "            return 0\n",
    "            \n",
    "        # 1. Calculate Kinetic Energy (The Trigger)\n",
    "        arr = np.array(self.micro_buffer)\n",
    "        prices = arr[:, 0]\n",
    "        vols = arr[:, 1]\n",
    "        \n",
    "        vol_diff = np.diff(vols)\n",
    "        vol_flow = np.where(vol_diff > 0, vol_diff, 0)\n",
    "        \n",
    "        kinetic_score = np.sum(vol_flow) / (abs(prices[-1] - prices[0]) + 0.05)\n",
    "        \n",
    "        if kinetic_score > self.threshold:\n",
    "            \n",
    "            # 2. Calculate Anchor VWAP (The Trend)\n",
    "            # Use rolling window or simplified mean if volumes are noisy\n",
    "            a_prices = np.array(self.anchor_prices)\n",
    "            a_vols = np.array(self.anchor_vols)\n",
    "            \n",
    "            # Safe VWAP calc\n",
    "            total_vol = np.sum(a_vols)\n",
    "            if total_vol > 0:\n",
    "                anchor_vwap = np.average(a_prices, weights=a_vols)\n",
    "            else:\n",
    "                anchor_vwap = np.mean(a_prices)\n",
    "            \n",
    "            # 3. Calculate Micro Direction (Fast VWAP)\n",
    "            micro_vwap = np.average(prices[1:], weights=vol_flow) if np.sum(vol_flow) > 0 else np.mean(prices)\n",
    "            \n",
    "            # 4. ALIGNMENT CHECK\n",
    "            \n",
    "            # CASE A: BULLISH ALIGNMENT\n",
    "            # Price > Anchor (Uptrend) AND Micro > VWAP (Buying Pressure)\n",
    "            if ltp > anchor_vwap and ltp > micro_vwap:\n",
    "                return 1 # LONG\n",
    "                \n",
    "            # CASE B: BEARISH ALIGNMENT\n",
    "            # Price < Anchor (Downtrend) AND Micro < VWAP (Selling Pressure)\n",
    "            elif ltp < anchor_vwap and ltp < micro_vwap:\n",
    "                return -1 # SHORT\n",
    "                \n",
    "        return 0\n",
    "\n",
    "# ==========================================\n",
    "# 3. DAILY BACKTEST ENGINE\n",
    "# ==========================================\n",
    "def run_day(df):\n",
    "    brain = AnchoredKineticBrain(threshold=KINETIC_THRESHOLD, anchor_period=ANCHOR_PERIOD)\n",
    "    \n",
    "    # Trade State\n",
    "    in_trade = False\n",
    "    entry_price = 0.0\n",
    "    entry_time = None\n",
    "    direction = 0 # 1 Long, -1 Short\n",
    "    \n",
    "    trades = []\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        ts = row.DateTime\n",
    "        ltp = row.LTP\n",
    "        vol = row.Volume\n",
    "        \n",
    "        # --- TRADE MANAGEMENT ---\n",
    "        if in_trade:\n",
    "            elapsed = (ts - entry_time).total_seconds()\n",
    "            pnl = 0\n",
    "            \n",
    "            if direction == 1: # Long\n",
    "                pnl = ltp - entry_price\n",
    "            else: # Short\n",
    "                pnl = entry_price - ltp\n",
    "                \n",
    "            exit_reason = None\n",
    "            if pnl <= -STOP_LOSS_POINTS: exit_reason = \"SL\"\n",
    "            elif pnl >= TAKE_PROFIT_POINTS: exit_reason = \"TP\"\n",
    "            elif elapsed >= MAX_HOLD_SECONDS: exit_reason = \"TIME\"\n",
    "            \n",
    "            if exit_reason:\n",
    "                trades.append(pnl - COST_PER_TRADE)\n",
    "                in_trade = False\n",
    "                direction = 0\n",
    "            \n",
    "            # Update Brain even while in trade\n",
    "            brain.get_signal(ltp, vol) \n",
    "            continue \n",
    "            \n",
    "        # --- SIGNAL GENERATION ---\n",
    "        sig = brain.get_signal(ltp, vol)\n",
    "        \n",
    "        if sig != 0:\n",
    "            in_trade = True\n",
    "            entry_price = ltp\n",
    "            entry_time = ts\n",
    "            direction = sig\n",
    "                \n",
    "    return trades\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN LOOP\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(f\"Running ANCHORED KINETIC from {START_DATE} to {END_DATE}\")\n",
    "    print(f\"Trigger: Kinetic > {KINETIC_THRESHOLD} | Anchor: {ANCHOR_PERIOD} ticks\")\n",
    "    print(f\"Stop: {STOP_LOSS_POINTS} | Target: {TAKE_PROFIT_POINTS}\")\n",
    "    print(\"-\" * 65)\n",
    "    print(f\"{'Date':<12} | {'Trades':<6} | {'Daily PnL (Pts)':<15} | {'Status'}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    current_date = START_DATE\n",
    "    total_points = 0.0\n",
    "    total_trades = 0\n",
    "    \n",
    "    date_pnl_map = []\n",
    "    \n",
    "    while current_date <= END_DATE:\n",
    "        if current_date.weekday() >= 5:\n",
    "            current_date += datetime.timedelta(days=1)\n",
    "            continue\n",
    "            \n",
    "        df = get_data_for_date(current_date)\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            daily_trades = run_day(df)\n",
    "            daily_pts = sum(daily_trades)\n",
    "            count = len(daily_trades)\n",
    "            \n",
    "            total_points += daily_pts\n",
    "            total_trades += count\n",
    "            \n",
    "            status = \"PROFIT\" if daily_pts > 0 else \"LOSS\" if daily_pts < 0 else \"FLAT\"\n",
    "            print(f\"{current_date} | {count:<6} | {daily_pts:>10.2f} pts   | {status}\")\n",
    "            \n",
    "            date_pnl_map.append({'Date': current_date, 'PnL': daily_pts, 'Trades': count})\n",
    "        \n",
    "        current_date += datetime.timedelta(days=1)\n",
    "        \n",
    "    print(\"-\" * 65)\n",
    "    print(f\"TOTAL RESULTS:\")\n",
    "    print(f\"Gross Points: {total_points:.2f}\")\n",
    "    print(f\"Total Trades: {total_trades}\")\n",
    "    print(f\"Avg Pts/Day:  {total_points / len(date_pnl_map) if date_pnl_map else 0:.2f}\")\n",
    "    print(f\"Total INR:    ₹{total_points * LOT_SIZE:,.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba3682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
